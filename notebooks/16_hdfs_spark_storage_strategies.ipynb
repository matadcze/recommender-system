{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 16 - HDFS + Spark Storage Strategies\n",
    "\n",
    "Optymalizacja przechowywania danych na HDFS dla Spark.\n",
    "\n",
    "**Tematy:**\n",
    "- Formaty plików: Parquet vs ORC vs Avro vs CSV (benchmark)\n",
    "- Kompresja: Snappy vs GZIP vs LZ4 vs ZSTD\n",
    "- Partycjonowanie danych na HDFS\n",
    "- Small files problem i compaction\n",
    "- Block size vs partition size\n",
    "- Bucketing - optymalizacja joinów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"16_Storage_Strategies\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "HDFS_URL = \"hdfs://namenode:9000\"\n",
    "HDFS_BASE = f\"{HDFS_URL}/data/movielens/benchmarks\"\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "jdbc_props = {\"user\": \"recommender\", \"password\": \"recommender\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "ratings = spark.read.jdbc(\n",
    "    jdbc_url, \"movielens.ratings\", properties=jdbc_props,\n",
    "    column=\"user_id\", lowerBound=1, upperBound=300000, numPartitions=10\n",
    ")\n",
    "ratings.cache()\n",
    "print(f\"Ratings: {ratings.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Benchmark formatów plików\n",
    "\n",
    "| Format | Typ | Kompresja | Column pruning | Predicate pushdown | Use case |\n",
    "|--------|-----|-----------|----------------|-------------------|----------|\n",
    "| **Parquet** | Kolumnowy | Snappy/GZIP/ZSTD | Tak | Tak | Analityka (domyślny Spark) |\n",
    "| **ORC** | Kolumnowy | ZLIB/Snappy/LZO | Tak | Tak | Hive ecosystem |\n",
    "| **Avro** | Wierszowy | Snappy/Deflate | Nie | Nie | Streaming, schema evolution |\n",
    "| **CSV** | Tekst | GZIP/BZip2 | Nie | Nie | Interop, małe dane |\n",
    "| **JSON** | Tekst | GZIP | Nie | Nie | API, czytelność |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdfs_size(path):\n",
    "    \"\"\"Rozmiar pliku/katalogu na HDFS w MB.\"\"\"\n",
    "    try:\n",
    "        fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "            spark.sparkContext._jvm.java.net.URI(HDFS_URL),\n",
    "            spark.sparkContext._jsc.hadoopConfiguration()\n",
    "        )\n",
    "        p = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(path)\n",
    "        return fs.getContentSummary(p).getLength() / 1024 / 1024\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Benchmark: zapis w różnych formatach\n",
    "formats = [\"parquet\", \"orc\", \"csv\", \"json\"]\n",
    "write_results = []\n",
    "\n",
    "for fmt in formats:\n",
    "    path = f\"{HDFS_BASE}/format_{fmt}\"\n",
    "    start = time.time()\n",
    "    if fmt == \"csv\":\n",
    "        ratings.write.mode(\"overwrite\").option(\"header\", True).csv(path)\n",
    "    else:\n",
    "        ratings.write.mode(\"overwrite\").format(fmt).save(path)\n",
    "    write_time = time.time() - start\n",
    "    size = get_hdfs_size(path)\n",
    "    write_results.append((fmt, write_time, size))\n",
    "    print(f\"{fmt:<10} write={write_time:.1f}s  size={size:.0f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: odczyt + analiza w różnych formatach\n",
    "print(\"\\n=== Full scan (count) ===\")\n",
    "for fmt in formats:\n",
    "    path = f\"{HDFS_BASE}/format_{fmt}\"\n",
    "    start = time.time()\n",
    "    if fmt == \"csv\":\n",
    "        spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(path).count()\n",
    "    else:\n",
    "        spark.read.format(fmt).load(path).count()\n",
    "    t = time.time() - start\n",
    "    print(f\"{fmt:<10} {t:.2f}s\")\n",
    "\n",
    "print(\"\\n=== Column pruning (select 2 columns) ===\")\n",
    "for fmt in formats:\n",
    "    path = f\"{HDFS_BASE}/format_{fmt}\"\n",
    "    start = time.time()\n",
    "    if fmt == \"csv\":\n",
    "        spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(path) \\\n",
    "            .select(\"user_id\", \"rating\").count()\n",
    "    else:\n",
    "        spark.read.format(fmt).load(path).select(\"user_id\", \"rating\").count()\n",
    "    t = time.time() - start\n",
    "    print(f\"{fmt:<10} {t:.2f}s\")\n",
    "\n",
    "print(\"\\n=== Predicate pushdown (filter + aggregate) ===\")\n",
    "for fmt in formats:\n",
    "    path = f\"{HDFS_BASE}/format_{fmt}\"\n",
    "    start = time.time()\n",
    "    if fmt == \"csv\":\n",
    "        df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(path)\n",
    "    else:\n",
    "        df = spark.read.format(fmt).load(path)\n",
    "    df.filter(col(\"rating\") >= 4.5).groupBy(\"movie_id\").count().count()\n",
    "    t = time.time() - start\n",
    "    print(f\"{fmt:<10} {t:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. Kompresja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark kompresji dla Parquet\n",
    "compressions = [\"snappy\", \"gzip\", \"lz4\", \"zstd\", \"none\"]\n",
    "\n",
    "print(\"=== Parquet compression benchmark ===\")\n",
    "for comp in compressions:\n",
    "    path = f\"{HDFS_BASE}/compression_{comp}\"\n",
    "    start = time.time()\n",
    "    ratings.write.mode(\"overwrite\") \\\n",
    "        .option(\"compression\", comp) \\\n",
    "        .parquet(path)\n",
    "    write_time = time.time() - start\n",
    "    size = get_hdfs_size(path)\n",
    "    \n",
    "    # Read benchmark\n",
    "    start = time.time()\n",
    "    spark.read.parquet(path).filter(col(\"rating\") >= 4.0).count()\n",
    "    read_time = time.time() - start\n",
    "    \n",
    "    print(f\"{comp:<10} size={size:>6.0f}MB  write={write_time:.1f}s  read={read_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000002",
   "metadata": {},
   "source": [
    "### Rekomendacje kompresji:\n",
    "- **Snappy** (domyślna): najszybsza, umiarkowana kompresja. Hot data.\n",
    "- **ZSTD**: najlepsza kompresja z rozsądną szybkością. Warm data.\n",
    "- **GZIP**: dobra kompresja, wolny zapis. Cold data / archiwum.\n",
    "- **LZ4**: bardzo szybka, słaba kompresja. Streaming.\n",
    "- **none**: bez kompresji. Tylko gdy CPU jest bottleneckiem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Partycjonowanie na HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partycjonowanie po roku - tworzy podkatalogi\n",
    "ratings_with_year = ratings.withColumn(\"year\", year(col(\"rating_timestamp\")))\n",
    "\n",
    "ratings_with_year.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .parquet(f\"{HDFS_BASE}/partitioned_by_year\")\n",
    "\n",
    "# Partition pruning - czyta TYLKO potrzebny rok\n",
    "print(\"=== Partition pruning benchmark ===\")\n",
    "\n",
    "# Bez partycjonowania (full scan)\n",
    "start = time.time()\n",
    "ratings_with_year.filter(col(\"year\") == 2015).count()\n",
    "no_partition = time.time() - start\n",
    "\n",
    "# Z partycjonowaniem (partition pruning)\n",
    "start = time.time()\n",
    "spark.read.parquet(f\"{HDFS_BASE}/partitioned_by_year\") \\\n",
    "    .filter(col(\"year\") == 2015).count()\n",
    "with_partition = time.time() - start\n",
    "\n",
    "print(f\"Bez partycjonowania: {no_partition:.2f}s\")\n",
    "print(f\"Z partycjonowaniem:  {with_partition:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Small Files Problem\n",
    "\n",
    "Każdy plik na HDFS = ~150 bajtów metadanych w NameNode (RAM!).\n",
    "1 milion małych plików = 150MB RAM NameNode.\n",
    "\n",
    "**Problem:** `repartition(1000).partitionBy(\"year\")` z 15 lat = 15000 plików!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symulacja problemu małych plików\n",
    "# ZŁE: dużo małych plików\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "ratings_with_year.repartition(200) \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .parquet(f\"{HDFS_BASE}/small_files_bad\")\n",
    "\n",
    "# DOBRE: kontrolowana liczba plików\n",
    "ratings_with_year.repartition(\"year\") \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .parquet(f\"{HDFS_BASE}/small_files_good\")\n",
    "\n",
    "# Policz pliki\n",
    "def count_files(path):\n",
    "    fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark.sparkContext._jvm.java.net.URI(HDFS_URL),\n",
    "        spark.sparkContext._jsc.hadoopConfiguration())\n",
    "    p = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(path)\n",
    "    iterator = fs.listFiles(p, True)\n",
    "    count = 0\n",
    "    while iterator.hasNext():\n",
    "        f = iterator.next()\n",
    "        if not f.getPath().getName().startswith(\"_\"):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "bad_count = count_files(f\"{HDFS_BASE}/small_files_bad\")\n",
    "good_count = count_files(f\"{HDFS_BASE}/small_files_good\")\n",
    "print(f\"Bez optymalizacji: {bad_count} plików\")\n",
    "print(f\"Z repartition(year): {good_count} plików\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compaction - łączenie małych plików w większe\n",
    "# Odczytaj i zapisz ponownie z mniejszą liczbą partycji\n",
    "\n",
    "spark.read.parquet(f\"{HDFS_BASE}/small_files_bad\") \\\n",
    "    .coalesce(10) \\\n",
    "    .write.mode(\"overwrite\") \\\n",
    "    .parquet(f\"{HDFS_BASE}/compacted\")\n",
    "\n",
    "compacted_count = count_files(f\"{HDFS_BASE}/compacted\")\n",
    "print(f\"Po compaction: {compacted_count} plików\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Bucketing - optymalizacja joinów\n",
    "\n",
    "Bucketing pre-sortuje dane po kluczu i zapisuje do stałej liczby plików.\n",
    "Przy join po tym samym kluczu - **zero shuffle!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz ratings z bucketing po movie_id\n",
    "ratings.write.mode(\"overwrite\") \\\n",
    "    .bucketBy(16, \"movie_id\") \\\n",
    "    .sortBy(\"movie_id\") \\\n",
    "    .saveAsTable(\"ratings_bucketed\")\n",
    "\n",
    "# Join bucketed tables - sprawdź plan (brak Exchange/Shuffle!)\n",
    "bucketed = spark.table(\"ratings_bucketed\")\n",
    "bucketed.join(spark.table(\"ratings_bucketed\").groupBy(\"movie_id\").count(), \"movie_id\") \\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## Zadanie końcowe\n",
    "\n",
    "Zoptymalizuj storage dla systemu rekomendacji na HDFS:\n",
    "\n",
    "1. Wybierz optymalny format i kompresję dla każdej warstwy:\n",
    "   - Bronze: archiwum (priorytet: rozmiar)\n",
    "   - Silver: częsty odczyt (priorytet: szybkość odczytu)\n",
    "   - Gold: małe agregaty (priorytet: szybkość query)\n",
    "2. Zaprojektuj partycjonowanie (po jakim kluczu?)\n",
    "3. Zmierz: rozmiar na HDFS, czas zapisu, czas typowego query\n",
    "4. Porównaj z nieoptymalną wersją (CSV, bez partycji, bez kompresji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.unpersist()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}