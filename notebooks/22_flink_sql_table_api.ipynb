{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 22 - Flink SQL & Table API\n",
    "\n",
    "Nauka Flink SQL i Table API - przetwarzanie danych strumieniowych i batchowych w jednym modelu.\n",
    "\n",
    "**Tematy:**\n",
    "- Flink SQL vs Spark SQL - porownanie skladni i mozliwosci\n",
    "- PyFlink Table Environment setup\n",
    "- Tworzenie tabel: DDL, connectors (JDBC, filesystem)\n",
    "- Flink SQL: SELECT, JOIN, GROUP BY, window functions\n",
    "- Continuous queries vs batch queries\n",
    "- Temporal joins (versioned tables)\n",
    "- CDC (Change Data Capture): Flink CDC connectors\n",
    "- Pattern matching: MATCH_RECOGNIZE (CEP w SQL)\n",
    "- Katalogi: Flink + Hive Metastore\n",
    "- Porownanie wydajnosci: Flink SQL vs Spark SQL na MovieLens\n",
    "- Zadanie koncowe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Flink SQL vs Spark SQL - porownanie\n",
    "\n",
    "| Cecha | Flink SQL | Spark SQL |\n",
    "|-------|-----------|----------|\n",
    "| **Model** | Stream-first (batch = bounded stream) | Batch-first (streaming dodany pozniej) |\n",
    "| **Latencja** | Milisekundy (true streaming) | Sekundy-minuty (micro-batch) |\n",
    "| **Semantyka czasu** | Event time + watermarki natywnie | Event time przez okna |\n",
    "| **Continuous queries** | Tak - zapytanie dziala ciagle | Nie - micro-batch lub trigger |\n",
    "| **CDC** | Natywne wsparcie (changelog stream) | Wymaga Delta Lake / Hudi |\n",
    "| **MATCH_RECOGNIZE** | Tak - CEP w SQL | Brak |\n",
    "| **Temporal joins** | Natywne | Brak (trzeba recznie) |\n",
    "| **Catalog** | Hive Metastore, JDBC Catalog | Hive Metastore |\n",
    "| **UDF** | Java, Scala, Python (PyFlink) | Python, Scala, Java |\n",
    "\n",
    "### Kiedy Flink SQL?\n",
    "- Przetwarzanie strumieni danych w czasie rzeczywistym (Kafka, CDC)\n",
    "- Continuous queries - zapytania dzialajace non-stop\n",
    "- Complex Event Processing (CEP) - wykrywanie wzorcow w strumieniach\n",
    "- Niskie latencje (ms vs sekundy w Spark)\n",
    "\n",
    "### Kiedy Spark SQL?\n",
    "- Duze batch ETL na danych historycznych\n",
    "- Machine Learning pipeline (MLlib)\n",
    "- Ad-hoc analityka na duzych zbiorach danych\n",
    "- Integracja z ekosystemem Python (pandas UDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. PyFlink Table Environment - setup\n",
    "\n",
    "W Flink sa dwa glowne srodowiska:\n",
    "- `StreamTableEnvironment` - dla streaming + batch (zalecane)\n",
    "- `TableEnvironment` - tylko batch\n",
    "\n",
    "Flink Table API uzywa koncepcji **Table** - odpowiednik DataFrame w Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
    "from pyflink.table.expressions import col, lit, call\n",
    "from pyflink.common import Row\n",
    "import pandas as pd\n",
    "\n",
    "# Streaming mode - obsluguje zarowno stream jak i batch\n",
    "env_settings = EnvironmentSettings.in_streaming_mode()\n",
    "t_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "# Konfiguracja Flink - JobManager jest na flink-jobmanager:8081\n",
    "t_env.get_config().set(\"parallelism.default\", \"2\")\n",
    "t_env.get_config().set(\"table.exec.resource.default-parallelism\", \"2\")\n",
    "\n",
    "# Dla batch queries w streaming mode:\n",
    "t_env.get_config().set(\"execution.runtime-mode\", \"batch\")\n",
    "\n",
    "print(\"Flink Table Environment created!\")\n",
    "print(f\"Planner: {t_env.get_config().get('table.planner', 'blink')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. Tworzenie tabel - DDL i connectors\n",
    "\n",
    "Flink uzywa instrukcji DDL do tworzenia tabel polaczonych ze zrodlami danych.\n",
    "Kazda tabela ma zdefiniowany **connector** (JDBC, filesystem, Kafka itp.).\n",
    "\n",
    "### Architektura connectorow:\n",
    "```\n",
    "Flink SQL Query\n",
    "      |\n",
    "      v\n",
    "Table (DDL)  -->  Connector  -->  Zewnetrzny system\n",
    "                  - jdbc          - PostgreSQL\n",
    "                  - filesystem    - HDFS / local\n",
    "                  - kafka         - Kafka topics\n",
    "                  - upsert-kafka  - Kafka compacted\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela JDBC - polaczenie z PostgreSQL\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS ratings (\n",
    "        user_id     INT,\n",
    "        movie_id    INT,\n",
    "        rating      DOUBLE,\n",
    "        rating_timestamp BIGINT\n",
    "    ) WITH (\n",
    "        'connector' = 'jdbc',\n",
    "        'url' = 'jdbc:postgresql://postgres:5432/recommender',\n",
    "        'table-name' = 'movielens.ratings',\n",
    "        'username' = 'recommender',\n",
    "        'password' = 'recommender',\n",
    "        'scan.partition.column' = 'user_id',\n",
    "        'scan.partition.num' = '4',\n",
    "        'scan.partition.lower-bound' = '1',\n",
    "        'scan.partition.upper-bound' = '300000'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS movies (\n",
    "        movie_id    INT,\n",
    "        title       STRING,\n",
    "        genres      STRING\n",
    "    ) WITH (\n",
    "        'connector' = 'jdbc',\n",
    "        'url' = 'jdbc:postgresql://postgres:5432/recommender',\n",
    "        'table-name' = 'movielens.movies',\n",
    "        'username' = 'recommender',\n",
    "        'password' = 'recommender'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tabele JDBC utworzone.\")\n",
    "\n",
    "# Tabela filesystem - odczyt z HDFS (Parquet)\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS ratings_hdfs (\n",
    "        user_id     INT,\n",
    "        movie_id    INT,\n",
    "        rating      DOUBLE,\n",
    "        rating_timestamp BIGINT\n",
    "    ) WITH (\n",
    "        'connector' = 'filesystem',\n",
    "        'path' = 'hdfs://namenode:9000/user/spark/movielens/ratings',\n",
    "        'format' = 'parquet'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tabela HDFS (filesystem) utworzona.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdz dostepne tabele\n",
    "t_env.execute_sql(\"SHOW TABLES\").print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Flink SQL - podstawowe zapytania\n",
    "\n",
    "Skladnia Flink SQL jest bardzo zblizona do ANSI SQL i Spark SQL.\n",
    "Glowna roznica: Flink traktuje wyniki jako **changelog stream** (insert/update/delete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT z WHERE - tak samo jak w Spark SQL\n",
    "t_env.execute_sql(\"\"\"\n",
    "    SELECT user_id, movie_id, rating\n",
    "    FROM ratings\n",
    "    WHERE rating >= 4.5 AND user_id <= 100\n",
    "    ORDER BY rating DESC\n",
    "    LIMIT 10\n",
    "\"\"\").print()\n",
    "\n",
    "# GROUP BY z HAVING\n",
    "t_env.execute_sql(\"\"\"\n",
    "    SELECT user_id,\n",
    "           COUNT(*) AS rating_count,\n",
    "           ROUND(AVG(rating), 2) AS avg_rating,\n",
    "           MIN(rating) AS min_rating,\n",
    "           MAX(rating) AS max_rating\n",
    "    FROM ratings\n",
    "    GROUP BY user_id\n",
    "    HAVING COUNT(*) > 1000\n",
    "    ORDER BY rating_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN - identyczny SQL jak w Spark\n",
    "t_env.execute_sql(\"\"\"\n",
    "    SELECT m.title,\n",
    "           COUNT(*) AS num_ratings,\n",
    "           ROUND(AVG(r.rating), 2) AS avg_rating\n",
    "    FROM ratings r\n",
    "    JOIN movies m ON r.movie_id = m.movie_id\n",
    "    GROUP BY m.title\n",
    "    HAVING COUNT(*) > 5000\n",
    "    ORDER BY avg_rating DESC\n",
    "    LIMIT 15\n",
    "\"\"\").print()\n",
    "\n",
    "# CTE (Common Table Expressions) - identycznie jak w Spark SQL\n",
    "t_env.execute_sql(\"\"\"\n",
    "    WITH user_stats AS (\n",
    "        SELECT user_id,\n",
    "               COUNT(*) AS num_ratings,\n",
    "               ROUND(AVG(rating), 2) AS avg_rating\n",
    "        FROM ratings\n",
    "        GROUP BY user_id\n",
    "    ),\n",
    "    user_categories AS (\n",
    "        SELECT *,\n",
    "               CASE\n",
    "                   WHEN num_ratings >= 1000 THEN 'power_user'\n",
    "                   WHEN num_ratings >= 100 THEN 'active'\n",
    "                   WHEN num_ratings >= 20 THEN 'casual'\n",
    "                   ELSE 'rare'\n",
    "               END AS user_category\n",
    "        FROM user_stats\n",
    "    )\n",
    "    SELECT user_category,\n",
    "           COUNT(*) AS num_users,\n",
    "           ROUND(AVG(avg_rating), 2) AS mean_avg_rating,\n",
    "           CAST(ROUND(AVG(num_ratings), 0) AS INT) AS mean_num_ratings\n",
    "    FROM user_categories\n",
    "    GROUP BY user_category\n",
    "    ORDER BY mean_num_ratings DESC\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Table API - programistyczny odpowiednik SQL\n",
    "\n",
    "Table API to programistyczny interfejs do Flink - odpowiednik DataFrame API w Spark.\n",
    "Kazde zapytanie SQL mozna wyrazic jako lancuch operacji Table API i odwrotnie.\n",
    "\n",
    "| Spark DataFrame API | Flink Table API |\n",
    "|--------------------|-----------------|\n",
    "| `df.select(...)` | `table.select(...)` |\n",
    "| `df.filter(...)` | `table.where(...)` |\n",
    "| `df.groupBy(...).agg(...)` | `table.group_by(...).select(...)` |\n",
    "| `df.join(...)` | `table.join(...)` |\n",
    "| `df.orderBy(...)` | `table.order_by(...)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.table.expressions import col, lit, call\n",
    "\n",
    "# Pobierz tabele jako obiekty Table\n",
    "ratings_table = t_env.from_path(\"ratings\")\n",
    "movies_table = t_env.from_path(\"movies\")\n",
    "\n",
    "# SELECT z WHERE - Table API\n",
    "result = ratings_table \\\n",
    "    .where(col(\"rating\") >= 4.5) \\\n",
    "    .where(col(\"user_id\") <= 100) \\\n",
    "    .select(col(\"user_id\"), col(\"movie_id\"), col(\"rating\")) \\\n",
    "    .order_by(col(\"rating\").desc) \\\n",
    "    .fetch(10)\n",
    "\n",
    "result.print()\n",
    "\n",
    "# GROUP BY - Table API\n",
    "user_stats = ratings_table \\\n",
    "    .group_by(col(\"user_id\")) \\\n",
    "    .select(\n",
    "        col(\"user_id\"),\n",
    "        col(\"rating\").count.alias(\"rating_count\"),\n",
    "        col(\"rating\").avg.alias(\"avg_rating\")\n",
    "    ) \\\n",
    "    .where(col(\"rating_count\") > 500) \\\n",
    "    .order_by(col(\"rating_count\").desc) \\\n",
    "    .fetch(10)\n",
    "\n",
    "user_stats.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN - Table API\n",
    "joined = ratings_table \\\n",
    "    .join(movies_table) \\\n",
    "    .where(col(\"ratings.movie_id\") == col(\"movies.movie_id\")) \\\n",
    "    .group_by(col(\"title\")) \\\n",
    "    .select(\n",
    "        col(\"title\"),\n",
    "        col(\"rating\").count.alias(\"num_ratings\"),\n",
    "        col(\"rating\").avg.alias(\"avg_rating\")\n",
    "    ) \\\n",
    "    .where(col(\"num_ratings\") > 5000) \\\n",
    "    .order_by(col(\"avg_rating\").desc) \\\n",
    "    .fetch(10)\n",
    "\n",
    "joined.print()\n",
    "\n",
    "# Mieszanie Table API z SQL - identycznie jak w Spark\n",
    "temp_table = ratings_table \\\n",
    "    .group_by(col(\"movie_id\")) \\\n",
    "    .select(\n",
    "        col(\"movie_id\"),\n",
    "        col(\"rating\").count.alias(\"cnt\"),\n",
    "        col(\"rating\").avg.alias(\"avg_r\")\n",
    "    )\n",
    "\n",
    "t_env.create_temporary_view(\"movie_stats\", temp_table)\n",
    "\n",
    "# Kontynuuj w SQL\n",
    "t_env.execute_sql(\"\"\"\n",
    "    SELECT m.title, s.cnt, ROUND(s.avg_r, 2) AS avg_rating\n",
    "    FROM movie_stats s\n",
    "    JOIN movies m ON s.movie_id = m.movie_id\n",
    "    WHERE s.cnt > 10000\n",
    "    ORDER BY s.avg_r DESC\n",
    "    LIMIT 10\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Continuous queries vs batch queries\n",
    "\n",
    "To jest **kluczowa roznica** miedzy Flink a Spark:\n",
    "\n",
    "- **Batch query**: przetwarza skonczone dane i konczy sie (jak Spark SQL)\n",
    "- **Continuous query**: dziala w nieskonczonosc, aktualizujac wyniki na biezaco\n",
    "\n",
    "W Flink SQL ten sam SQL moze dzialac w obu trybach!\n",
    "\n",
    "```\n",
    "Batch:      [dane] ---> [zapytanie] ---> [wynik] (koniec)\n",
    "Continuous: [strumien] ---> [zapytanie] ---> [changelog stream] (ciagle)\n",
    "```\n",
    "\n",
    "### Changelog stream\n",
    "Wynik continuous query to **changelog**:\n",
    "- `+I` = INSERT (nowy wiersz)\n",
    "- `-U` = UPDATE BEFORE (stara wartosc przed aktualizacja)\n",
    "- `+U` = UPDATE AFTER (nowa wartosc po aktualizacji)\n",
    "- `-D` = DELETE\n",
    "\n",
    "Przyklad: `SELECT genre, COUNT(*) FROM ratings GROUP BY genre`\n",
    "```\n",
    "+I[Action, 1]        -- pierwszy rating Action\n",
    "+I[Comedy, 1]        -- pierwszy rating Comedy\n",
    "-U[Action, 1]        -- aktualizacja Action (stara wartosc)\n",
    "+U[Action, 2]        -- aktualizacja Action (nowa wartosc)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch mode - przetwarza wszystkie dane i konczy sie\n",
    "t_env.get_config().set(\"execution.runtime-mode\", \"batch\")\n",
    "\n",
    "t_env.execute_sql(\"\"\"\n",
    "    SELECT movie_id,\n",
    "           COUNT(*) AS num_ratings,\n",
    "           ROUND(AVG(rating), 2) AS avg_rating\n",
    "    FROM ratings\n",
    "    GROUP BY movie_id\n",
    "    HAVING COUNT(*) > 10000\n",
    "    ORDER BY avg_rating DESC\n",
    "    LIMIT 10\n",
    "\"\"\").print()\n",
    "\n",
    "# Streaming mode - ten sam SQL, ale jako continuous query\n",
    "# W trybie streaming GROUP BY emituje changelog (+I, -U, +U)\n",
    "# Wlaczamy streaming mode aby zobaczyc roznice\n",
    "t_env.get_config().set(\"execution.runtime-mode\", \"streaming\")\n",
    "\n",
    "# Definicja tabeli sink do zapisu wynikow (print connector)\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS rating_counts_sink (\n",
    "        movie_id INT,\n",
    "        num_ratings BIGINT,\n",
    "        avg_rating DOUBLE,\n",
    "        PRIMARY KEY (movie_id) NOT ENFORCED\n",
    "    ) WITH (\n",
    "        'connector' = 'print'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# INSERT INTO ... SELECT - uruchamia continuous query\n",
    "# W srodowisku produkcyjnym to dzialalaby w nieskonczonosc\n",
    "# Tutaj zakonczy sie po przetworzeniu wszystkich danych z JDBC (bounded source)\n",
    "t_env.execute_sql(\"\"\"\n",
    "    INSERT INTO rating_counts_sink\n",
    "    SELECT movie_id,\n",
    "           COUNT(*) AS num_ratings,\n",
    "           ROUND(AVG(rating), 2) AS avg_rating\n",
    "    FROM ratings\n",
    "    GROUP BY movie_id\n",
    "\"\"\").wait()\n",
    "\n",
    "print(\"Continuous query zakonczony (bounded source).\")\n",
    "\n",
    "# Powrot do batch mode\n",
    "t_env.get_config().set(\"execution.runtime-mode\", \"batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. Window functions i Flink SQL specifics\n",
    "\n",
    "Flink SQL obsluguje standardowe window functions jak Spark, ale dodaje tez **okna czasowe** specyficzne dla streamingu:\n",
    "\n",
    "| Typ okna | Opis | Spark equivalent |\n",
    "|----------|------|------------------|\n",
    "| `TUMBLE` | Okna nienakladajace sie o stalym rozmiarze | `window(col, \"1 hour\")` |\n",
    "| `HOP` | Okna nakladajace sie (sliding) | `window(col, \"1 hour\", \"15 min\")` |\n",
    "| `CUMULATE` | Okna kumulatywne (rosnace) | Brak odpowiednika |\n",
    "| `SESSION` | Okna sesyjne (gap-based) | Brak w SQL (jest w Structured Streaming) |\n",
    "| `OVER` | Standardowe window functions | `Window.partitionBy().orderBy()` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardowe window functions - dzialaja identycznie jak w Spark SQL\n",
    "t_env.execute_sql(\"\"\"\n",
    "    SELECT user_id, movie_id, rating, rn\n",
    "    FROM (\n",
    "        SELECT user_id, movie_id, rating,\n",
    "               ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY rating DESC) AS rn\n",
    "        FROM ratings\n",
    "        WHERE user_id <= 5\n",
    "    )\n",
    "    WHERE rn <= 3\n",
    "    ORDER BY user_id, rn\n",
    "\"\"\").print()\n",
    "\n",
    "# TUMBLE window - okna czasowe (specyficzne dla Flink)\n",
    "# Wymaga kolumny czasowej zadeklarowanej jako TIMESTAMP\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS ratings_with_ts (\n",
    "        user_id     INT,\n",
    "        movie_id    INT,\n",
    "        rating      DOUBLE,\n",
    "        rating_timestamp BIGINT,\n",
    "        ts AS TO_TIMESTAMP_LTZ(rating_timestamp, 0),\n",
    "        WATERMARK FOR ts AS ts - INTERVAL '5' SECOND\n",
    "    ) WITH (\n",
    "        'connector' = 'jdbc',\n",
    "        'url' = 'jdbc:postgresql://postgres:5432/recommender',\n",
    "        'table-name' = 'movielens.ratings',\n",
    "        'username' = 'recommender',\n",
    "        'password' = 'recommender'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Agregacja w oknach czasowych TUMBLE (np. per rok)\n",
    "t_env.execute_sql(\"\"\"\n",
    "    SELECT\n",
    "        window_start,\n",
    "        window_end,\n",
    "        COUNT(*) AS num_ratings,\n",
    "        ROUND(AVG(rating), 2) AS avg_rating\n",
    "    FROM TABLE(\n",
    "        TUMBLE(TABLE ratings_with_ts, DESCRIPTOR(ts), INTERVAL '365' DAY)\n",
    "    )\n",
    "    GROUP BY window_start, window_end\n",
    "    ORDER BY window_start\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Temporal joins i CDC (Change Data Capture)\n",
    "\n",
    "### Temporal joins\n",
    "Temporal join laczy strumien zdarzen z **wersjonowana tabela** - dla kazdego zdarzenia\n",
    "uzywa wersji tabeli obowiazujacej w momencie zdarzenia.\n",
    "\n",
    "Przyklad: rating z momentu T laczymy z cenami filmow obowiazujacymi w momencie T.\n",
    "\n",
    "```sql\n",
    "-- Temporal join w Flink SQL\n",
    "SELECT r.user_id, r.movie_id, r.rating, p.price\n",
    "FROM ratings r\n",
    "JOIN movie_prices FOR SYSTEM_TIME AS OF r.event_time AS p\n",
    "  ON r.movie_id = p.movie_id\n",
    "```\n",
    "\n",
    "### CDC - Change Data Capture\n",
    "Flink moze czytac **changelog** bazy danych jako strumien:\n",
    "- `mysql-cdc` - czyta MySQL binlog\n",
    "- `postgres-cdc` - czyta PostgreSQL WAL (Write-Ahead Log)\n",
    "- `mongodb-cdc` - czyta MongoDB oplog\n",
    "\n",
    "```sql\n",
    "-- Tabela CDC - Flink automatycznie czyta zmiany z PostgreSQL\n",
    "CREATE TABLE movies_cdc (\n",
    "    movie_id INT,\n",
    "    title STRING,\n",
    "    genres STRING,\n",
    "    PRIMARY KEY (movie_id) NOT ENFORCED\n",
    ") WITH (\n",
    "    'connector' = 'postgres-cdc',\n",
    "    'hostname' = 'postgres',\n",
    "    'port' = '5432',\n",
    "    'username' = 'recommender',\n",
    "    'password' = 'recommender',\n",
    "    'database-name' = 'recommender',\n",
    "    'schema-name' = 'movielens',\n",
    "    'table-name' = 'movies',\n",
    "    'slot.name' = 'flink_movies_slot'\n",
    ");\n",
    "```\n",
    "\n",
    "Gdy ktos zrobi UPDATE/INSERT/DELETE w PostgreSQL, Flink natychmiast zobaczy zmiane!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symulacja temporal join - uzycie lookup join (bounded version)\n",
    "# Lookup join: dla kazdego wiersza ze strumienia odpytuje tabele wymiarowa\n",
    "\n",
    "# Tabela wymiarowa (movies) z wlaczonym lookup cache\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS movies_lookup (\n",
    "        movie_id    INT,\n",
    "        title       STRING,\n",
    "        genres      STRING,\n",
    "        PRIMARY KEY (movie_id) NOT ENFORCED\n",
    "    ) WITH (\n",
    "        'connector' = 'jdbc',\n",
    "        'url' = 'jdbc:postgresql://postgres:5432/recommender',\n",
    "        'table-name' = 'movielens.movies',\n",
    "        'username' = 'recommender',\n",
    "        'password' = 'recommender',\n",
    "        'lookup.cache.max-rows' = '10000',\n",
    "        'lookup.cache.ttl' = '1h'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Lookup join: strumien ratings laczy sie z tabela movies\n",
    "# FOR SYSTEM_TIME AS OF - kluczowa skladnia temporal join\n",
    "# W batch mode dziala jak zwykly join z cache\n",
    "t_env.execute_sql(\"\"\"\n",
    "    SELECT r.user_id,\n",
    "           r.movie_id,\n",
    "           r.rating,\n",
    "           m.title,\n",
    "           m.genres\n",
    "    FROM ratings_with_ts r\n",
    "    JOIN movies_lookup FOR SYSTEM_TIME AS OF r.ts AS m\n",
    "      ON r.movie_id = m.movie_id\n",
    "    WHERE r.user_id <= 5 AND r.rating = 5.0\n",
    "    LIMIT 15\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## 9. MATCH_RECOGNIZE - Pattern matching (CEP w SQL)\n",
    "\n",
    "MATCH_RECOGNIZE to unikalna cecha Flink SQL - pozwala wykrywac **wzorce w sekwencjach zdarzen**\n",
    "bezposrednio w SQL. To odpowiednik Complex Event Processing (CEP).\n",
    "\n",
    "Spark SQL **nie ma** tej funkcjonalnosci.\n",
    "\n",
    "### Skladnia:\n",
    "```sql\n",
    "SELECT *\n",
    "FROM tabela\n",
    "MATCH_RECOGNIZE (\n",
    "    PARTITION BY klucz\n",
    "    ORDER BY czas\n",
    "    MEASURES\n",
    "        -- co chcemy wyciagnac z dopasowania\n",
    "    ONE ROW PER MATCH          -- lub ALL ROWS PER MATCH\n",
    "    PATTERN (wzorzec regex)\n",
    "    DEFINE\n",
    "        -- definicje symboli we wzorcu\n",
    ")\n",
    "```\n",
    "\n",
    "### Przyklad: wykryj uzytkownikow z rosnacymi ocenami\n",
    "Wzorzec: 3 kolejne oceny, kazda wyzsza od poprzedniej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATCH_RECOGNIZE - wykryj wzorzec rosnacych ocen\n",
    "# Szukamy uzytkownikow, ktorzy dali 3+ kolejnych rosnacych ocen\n",
    "t_env.execute_sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM ratings_with_ts\n",
    "    MATCH_RECOGNIZE (\n",
    "        PARTITION BY user_id\n",
    "        ORDER BY ts\n",
    "        MEASURES\n",
    "            FIRST(A.rating) AS first_rating,\n",
    "            LAST(A.rating) AS last_rating,\n",
    "            COUNT(A.rating) AS streak_length,\n",
    "            FIRST(A.ts) AS streak_start,\n",
    "            LAST(A.ts) AS streak_end\n",
    "        ONE ROW PER MATCH\n",
    "        AFTER MATCH SKIP PAST LAST ROW\n",
    "        PATTERN (A{3,})\n",
    "        DEFINE\n",
    "            A AS A.rating > LAST(A.rating, 1) OR FIRST(A.rating) = A.rating\n",
    "    )\n",
    "    WHERE user_id <= 100\n",
    "    ORDER BY streak_length DESC\n",
    "    LIMIT 15\n",
    "\"\"\").print()\n",
    "\n",
    "# MATCH_RECOGNIZE - wykryj nagly spadek ocen (V-shape)\n",
    "# Wzorzec: wysoka ocena -> niska ocena -> wysoka ocena\n",
    "t_env.execute_sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM ratings_with_ts\n",
    "    MATCH_RECOGNIZE (\n",
    "        PARTITION BY user_id\n",
    "        ORDER BY ts\n",
    "        MEASURES\n",
    "            A.rating AS high_before,\n",
    "            B.rating AS low_point,\n",
    "            C.rating AS high_after,\n",
    "            B.movie_id AS disliked_movie\n",
    "        ONE ROW PER MATCH\n",
    "        AFTER MATCH SKIP PAST LAST ROW\n",
    "        PATTERN (A B C)\n",
    "        DEFINE\n",
    "            A AS A.rating >= 4.0,\n",
    "            B AS B.rating <= 2.0,\n",
    "            C AS C.rating >= 4.0\n",
    "    )\n",
    "    WHERE user_id <= 50\n",
    "    LIMIT 15\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10000001",
   "metadata": {},
   "source": [
    "## 10. Katalogi - Flink + Hive Metastore\n",
    "\n",
    "Flink moze uzyc **Hive Metastore** jako katalogu, dzieki czemu:\n",
    "- Tabele sa wspoldzielone miedzy Flink, Spark i Trino\n",
    "- Metadane przetrwaja restart Flink (persistent catalog)\n",
    "- Mozna odpytywac tabele Hive/Parquet na HDFS\n",
    "\n",
    "```sql\n",
    "-- Rejestracja Hive Catalog\n",
    "CREATE CATALOG hive_catalog WITH (\n",
    "    'type' = 'hive',\n",
    "    'hive-conf-dir' = '/opt/hive/conf'\n",
    ");\n",
    "\n",
    "USE CATALOG hive_catalog;\n",
    "SHOW DATABASES;\n",
    "SHOW TABLES;\n",
    "\n",
    "-- Teraz mozesz odpytywac tabele Hive!\n",
    "SELECT * FROM movielens.ratings LIMIT 10;\n",
    "```\n",
    "\n",
    "### Hierarchia obiektow:\n",
    "```\n",
    "Catalog (np. hive_catalog, default_catalog)\n",
    "  └── Database (np. movielens, default)\n",
    "       └── Table (np. ratings, movies)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domyslny katalog (in-memory)\n",
    "t_env.execute_sql(\"SHOW CATALOGS\").print()\n",
    "t_env.execute_sql(\"SHOW DATABASES\").print()\n",
    "\n",
    "# Hive Catalog - wspoldzielone metadane z innymi silnikami\n",
    "# Wymaga hive-conf-dir z plikiem hive-site.xml\n",
    "try:\n",
    "    t_env.execute_sql(\"\"\"\n",
    "        CREATE CATALOG hive_catalog WITH (\n",
    "            'type' = 'hive',\n",
    "            'hive-conf-dir' = '/opt/hive/conf'\n",
    "        )\n",
    "    \"\"\")\n",
    "    t_env.execute_sql(\"USE CATALOG hive_catalog\")\n",
    "    t_env.execute_sql(\"SHOW DATABASES\").print()\n",
    "    t_env.execute_sql(\"USE CATALOG default_catalog\")\n",
    "except Exception as e:\n",
    "    print(f\"Hive Catalog niedostepny (wymagany Hive Metastore): {e}\")\n",
    "    print(\"W produkcji Hive Catalog pozwala wspoldzielic tabele z Spark i Trino.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11000001",
   "metadata": {},
   "source": [
    "## 11. Porownanie wydajnosci: Flink SQL vs Spark SQL na MovieLens\n",
    "\n",
    "Uruchomimy te same zapytania w Flink SQL i porownamy z wynikami ze Spark SQL (notebook 05).\n",
    "\n",
    "**Uwaga:** Flink w batch mode na bounded data nie jest typowym use case - Flink swieci\n",
    "w streaming. Ten benchmark sluzy celom edukacyjnym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t_env.get_config().set(\"execution.runtime-mode\", \"batch\")\n",
    "\n",
    "queries = {\n",
    "    \"simple_count\": \"\"\"\n",
    "        SELECT COUNT(*) AS total FROM ratings\n",
    "    \"\"\",\n",
    "    \"group_by\": \"\"\"\n",
    "        SELECT movie_id, COUNT(*) AS cnt, ROUND(AVG(rating), 2) AS avg_r\n",
    "        FROM ratings\n",
    "        GROUP BY movie_id\n",
    "        ORDER BY cnt DESC\n",
    "        LIMIT 10\n",
    "    \"\"\",\n",
    "    \"join_agg\": \"\"\"\n",
    "        SELECT m.title, COUNT(*) AS cnt, ROUND(AVG(r.rating), 2) AS avg_r\n",
    "        FROM ratings r\n",
    "        JOIN movies m ON r.movie_id = m.movie_id\n",
    "        GROUP BY m.title\n",
    "        HAVING COUNT(*) > 5000\n",
    "        ORDER BY avg_r DESC\n",
    "        LIMIT 10\n",
    "    \"\"\",\n",
    "    \"window_function\": \"\"\"\n",
    "        SELECT user_id, movie_id, rating, rn FROM (\n",
    "            SELECT user_id, movie_id, rating,\n",
    "                   ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY rating DESC) AS rn\n",
    "            FROM ratings WHERE user_id <= 100\n",
    "        ) WHERE rn <= 3\n",
    "        ORDER BY user_id, rn\n",
    "        LIMIT 20\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(f\"{'Query':<20} {'Flink Time':>12}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for name, sql in queries.items():\n",
    "    start = time.time()\n",
    "    result = t_env.execute_sql(sql)\n",
    "    # Consume all results\n",
    "    with result.collect() as results:\n",
    "        rows = list(results)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"{name:<20} {elapsed:>10.2f}s ({len(rows)} rows)\")\n",
    "\n",
    "print()\n",
    "print(\"Porownaj z wynikami Spark SQL z notebooka 05.\")\n",
    "print(\"Flink batch jest zazwyczaj porownwyalny ze Spark na malych danych.\")\n",
    "print(\"Przewaga Flink pojawia sie w streaming (ms latency vs Spark micro-batch).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12000001",
   "metadata": {},
   "source": [
    "## Zadanie koncowe\n",
    "\n",
    "Napisz w Flink SQL kompletna analize MovieLens laczaca poznane techniki:\n",
    "\n",
    "### Czesc 1: Analiza SQL (batch)\n",
    "1. Znajdz **top 10 najlepiej ocenianych filmow** (min. 1000 ocen) - uzyj JOIN z tabela `movies`\n",
    "2. Dla kazdego filmu pokaz: tytul, gatunek, liczbe ocen, srednia ocene\n",
    "3. Dodaj **ranking** filmow za pomoca `ROW_NUMBER()` OVER\n",
    "\n",
    "### Czesc 2: Segmentacja uzytkownikow\n",
    "1. Podziel uzytkownikow na segmenty: power (>1000 ocen), active (100-1000), casual (<100)\n",
    "2. Dla kazdego segmentu oblicz: liczbe uzytkownikow, srednia ocene, srednia liczbe ocen\n",
    "\n",
    "### Czesc 3: MATCH_RECOGNIZE (CEP)\n",
    "1. Dla uzytkownikow 1-200 znajdz wzorzec: **3 kolejne oceny 5.0**\n",
    "2. Pokaz user_id, movie_id pierwszego i ostatniego filmu w serii, dlugosc serii\n",
    "\n",
    "### Czesc 4 (dodatkowe): Porownanie zrodel\n",
    "1. Porownaj COUNT(*) i AVG(rating) z tabeli `ratings` (JDBC) i `ratings_hdfs` (filesystem)\n",
    "2. Czy wyniki sa identyczne? Dlaczego moga sie roznic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiazanie - Czesc 1: Top 10 filmow z rankingiem\n",
    "t_env.execute_sql(\"\"\"\n",
    "\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiazanie - Czesc 2: Segmentacja uzytkownikow\n",
    "t_env.execute_sql(\"\"\"\n",
    "\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiazanie - Czesc 3: MATCH_RECOGNIZE\n",
    "t_env.execute_sql(\"\"\"\n",
    "\n",
    "\"\"\").print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiazanie - Czesc 4: Porownanie zrodel\n",
    "t_env.execute_sql(\"\"\"\n",
    "\n",
    "\"\"\").print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
