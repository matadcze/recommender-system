{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 24 - NiFi + Kafka + Spark Integration\n",
    "\n",
    "Integracja Apache NiFi z Kafka i Spark Structured Streaming w kontekscie systemu rekomendacji MovieLens.\n",
    "\n",
    "**Tematy:**\n",
    "- NiFi jako producent do Kafki (PublishKafka processor)\n",
    "- Spark Structured Streaming konsumujacy z Kafki\n",
    "- Full pipeline: Source -> NiFi -> Kafka -> Spark -> HDFS/PostgreSQL\n",
    "- NiFi Site-to-Site protocol\n",
    "- NiFi Registry - wersjonowanie flow\n",
    "- Error handling i retry w NiFi\n",
    "- Monitoring end-to-end pipeline\n",
    "- Zadanie koncowe: end-to-end pipeline dla nowych ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Architektura end-to-end pipeline\n",
    "\n",
    "```\n",
    "  ┌────────────┐     ┌───────────────────────┐     ┌─────────┐     ┌──────────────────┐\n",
    "  │   Source    │     │       Apache NiFi      │     │  Kafka  │     │  Spark Streaming │\n",
    "  │            │     │                         │     │         │     │                  │\n",
    "  │ CSV files  │────>│ GetFile -> Transform    │────>│  topic: │────>│ readStream       │\n",
    "  │ REST API   │     │         -> PublishKafka  │     │ ratings │     │   .format(kafka) │\n",
    "  │ Database   │     │                         │     │         │     │   .groupBy()     │\n",
    "  └────────────┘     └───────────────────────┘     └─────────┘     │   .writeStream   │\n",
    "                                                                     └────────┬─────────┘\n",
    "                                                                              │\n",
    "                                                              ┌───────────────┼───────────────┐\n",
    "                                                              ▼               ▼               ▼\n",
    "                                                        ┌──────────┐  ┌────────────┐  ┌───────────┐\n",
    "                                                        │   HDFS   │  │ PostgreSQL │  │  Console  │\n",
    "                                                        │ (parquet)│  │ (aggregaty)│  │ (debug)   │\n",
    "                                                        └──────────┘  └────────────┘  └───────────┘\n",
    "```\n",
    "\n",
    "### Dlaczego Kafka miedzy NiFi a Spark?\n",
    "- **Buforowanie** - Kafka trzyma dane nawet gdy Spark jest niedostepny\n",
    "- **Decoupling** - NiFi i Spark moga skalowac sie niezaleznie\n",
    "- **Replay** - mozna ponownie przetworzyc dane z Kafki\n",
    "- **Multi-consumer** - wiele aplikacji moze czytac te same dane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# === NiFi API ===\n",
    "NIFI_API = \"https://nifi:8443/nifi-api\"\n",
    "nifi = requests.Session()\n",
    "nifi.verify = False\n",
    "\n",
    "def get_nifi_token(username=\"admin\", password=\"admin123456789\"):\n",
    "    try:\n",
    "        resp = nifi.post(\n",
    "            f\"{NIFI_API}/access/token\",\n",
    "            data={\"username\": username, \"password\": password},\n",
    "            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "        )\n",
    "        if resp.status_code == 201:\n",
    "            nifi.headers.update({\"Authorization\": f\"Bearer {resp.text}\"})\n",
    "            print(\"NiFi: token uzyskany\")\n",
    "    except Exception as e:\n",
    "        print(f\"NiFi: proba bez auth ({e})\")\n",
    "\n",
    "get_nifi_token()\n",
    "\n",
    "def nifi_get(path):\n",
    "    r = nifi.get(f\"{NIFI_API}{path}\"); r.raise_for_status(); return r.json()\n",
    "\n",
    "def nifi_post(path, data):\n",
    "    r = nifi.post(f\"{NIFI_API}{path}\", json=data, headers={\"Content-Type\": \"application/json\"})\n",
    "    r.raise_for_status(); return r.json()\n",
    "\n",
    "def nifi_put(path, data):\n",
    "    r = nifi.put(f\"{NIFI_API}{path}\", json=data, headers={\"Content-Type\": \"application/json\"})\n",
    "    r.raise_for_status(); return r.json()\n",
    "\n",
    "def nifi_delete(path, params=None):\n",
    "    r = nifi.delete(f\"{NIFI_API}{path}\", params=params); r.raise_for_status(); return r.json()\n",
    "\n",
    "root_flow = nifi_get(\"/flow/process-groups/root\")\n",
    "ROOT_PG_ID = root_flow[\"processGroupFlow\"][\"id\"]\n",
    "print(f\"NiFi Root PG: {ROOT_PG_ID}\")\n",
    "\n",
    "# Test polaczenia\n",
    "diag = nifi_get(\"/system-diagnostics\")\n",
    "print(f\"NiFi Heap: {diag['systemDiagnostics']['aggregateSnapshot']['heapUtilization']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"24_NiFi_Kafka_Spark\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.postgresql:postgresql:42.7.1,\"\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "HDFS_URL = \"hdfs://namenode:9000\"\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "jdbc_props = {\"user\": \"recommender\", \"password\": \"recommender\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(\"Spark + Kafka gotowy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. NiFi jako producent do Kafki\n",
    "\n",
    "Tworzymy flow w NiFi ktory:\n",
    "1. Czyta pliki CSV z danymi MovieLens\n",
    "2. Dzieli na linie (rekordy)\n",
    "3. Konwertuje na JSON\n",
    "4. Publikuje do tematu Kafka `movielens-ratings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper do tworzenia procesorow\n",
    "def create_processor(pg_id, proc_type, name, position, config=None, auto_terminate=None):\n",
    "    body = {\n",
    "        \"revision\": {\"version\": 0},\n",
    "        \"component\": {\n",
    "            \"type\": proc_type,\n",
    "            \"name\": name,\n",
    "            \"position\": position,\n",
    "            \"config\": {}\n",
    "        }\n",
    "    }\n",
    "    if config:\n",
    "        body[\"component\"][\"config\"][\"properties\"] = config\n",
    "    if auto_terminate:\n",
    "        body[\"component\"][\"config\"][\"autoTerminatedRelationships\"] = auto_terminate\n",
    "    result = nifi_post(f\"/process-groups/{pg_id}/processors\", body)\n",
    "    print(f\"  + {name} ({result['id'][:8]}...)\")\n",
    "    return result\n",
    "\n",
    "def create_connection(pg_id, src_id, dst_id, rels, name=\"\"):\n",
    "    body = {\n",
    "        \"revision\": {\"version\": 0},\n",
    "        \"component\": {\n",
    "            \"name\": name,\n",
    "            \"source\": {\"id\": src_id, \"type\": \"PROCESSOR\", \"groupId\": pg_id},\n",
    "            \"destination\": {\"id\": dst_id, \"type\": \"PROCESSOR\", \"groupId\": pg_id},\n",
    "            \"selectedRelationships\": rels,\n",
    "            \"backPressureObjectThreshold\": 10000,\n",
    "            \"backPressureDataSizeThreshold\": \"100 MB\"\n",
    "        }\n",
    "    }\n",
    "    return nifi_post(f\"/process-groups/{pg_id}/connections\", body)\n",
    "\n",
    "# === Tworzenie flow: CSV -> Kafka ===\n",
    "pg = nifi_post(f\"/process-groups/{ROOT_PG_ID}/process-groups\", {\n",
    "    \"revision\": {\"version\": 0},\n",
    "    \"component\": {\"name\": \"Ratings to Kafka\", \"position\": {\"x\": 100, \"y\": 100}}\n",
    "})\n",
    "KAFKA_PG_ID = pg[\"id\"]\n",
    "print(f\"Process Group: {KAFKA_PG_ID}\\n\")\n",
    "\n",
    "# 1. GetFile - czytaj CSV\n",
    "p_getfile = create_processor(\n",
    "    KAFKA_PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.GetFile\",\n",
    "    \"Read Ratings CSV\",\n",
    "    {\"x\": 100, \"y\": 100},\n",
    "    config={\"Input Directory\": \"/data/raw/movielens\", \"File Filter\": \"rating\\\\.csv\",\n",
    "            \"Keep Source File\": \"true\"}\n",
    ")\n",
    "\n",
    "# 2. SplitText - podziel CSV na linie (1 FlowFile = 1 rating)\n",
    "p_split = create_processor(\n",
    "    KAFKA_PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.SplitText\",\n",
    "    \"Split CSV Lines\",\n",
    "    {\"x\": 100, \"y\": 300},\n",
    "    config={\"Line Split Count\": \"1\", \"Header Line Count\": \"1\",\n",
    "            \"Remove Trailing Newlines\": \"true\"},\n",
    "    auto_terminate=[\"original\", \"failure\"]\n",
    ")\n",
    "\n",
    "# 3. ReplaceText - konwertuj CSV linie na JSON\n",
    "p_to_json = create_processor(\n",
    "    KAFKA_PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.ReplaceText\",\n",
    "    \"CSV to JSON\",\n",
    "    {\"x\": 100, \"y\": 500},\n",
    "    config={\n",
    "        \"Search Value\": \"^(.*?),(.*?),(.*?),(.*?)$\",\n",
    "        \"Replacement Value\": '{\"user_id\":\"$1\",\"movie_id\":\"$2\",\"rating\":\"$3\",\"timestamp\":\"$4\"}',\n",
    "        \"Replacement Strategy\": \"Regex Replace\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 4. UpdateAttribute - klucz Kafka = user_id\n",
    "p_key = create_processor(\n",
    "    KAFKA_PG_ID,\n",
    "    \"org.apache.nifi.processors.attributes.UpdateAttribute\",\n",
    "    \"Set Kafka Key\",\n",
    "    {\"x\": 100, \"y\": 700},\n",
    "    config={\"kafka.key\": \"${fragment.index}\"}\n",
    ")\n",
    "\n",
    "# 5. PublishKafka - wyslij do Kafka\n",
    "p_kafka = create_processor(\n",
    "    KAFKA_PG_ID,\n",
    "    \"org.apache.nifi.processors.kafka.pubsub.PublishKafka_2_6\",\n",
    "    \"Publish to Kafka\",\n",
    "    {\"x\": 100, \"y\": 900},\n",
    "    config={\n",
    "        \"bootstrap.servers\": \"kafka:9092\",\n",
    "        \"topic\": \"movielens-ratings\",\n",
    "        \"Delivery Guarantee\": \"Guarantee Replicated Delivery\",\n",
    "        \"Message Key Field\": \"kafka.key\",\n",
    "        \"Compression Type\": \"snappy\",\n",
    "        \"Max Request Size\": \"1 MB\"\n",
    "    },\n",
    "    auto_terminate=[\"success\", \"failure\"]\n",
    ")\n",
    "\n",
    "# Polaczenia\n",
    "create_connection(KAFKA_PG_ID, p_getfile[\"id\"], p_split[\"id\"], [\"success\"])\n",
    "create_connection(KAFKA_PG_ID, p_split[\"id\"], p_to_json[\"id\"], [\"splits\"])\n",
    "create_connection(KAFKA_PG_ID, p_to_json[\"id\"], p_key[\"id\"], [\"success\"])\n",
    "create_connection(KAFKA_PG_ID, p_key[\"id\"], p_kafka[\"id\"], [\"success\"])\n",
    "\n",
    "print(\"\\nFlow: GetFile -> SplitText -> ReplaceText -> UpdateAttribute -> PublishKafka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Spark Structured Streaming z Kafki\n",
    "\n",
    "Spark czyta dane z tematu Kafka `movielens-ratings` w trybie streaming.\n",
    "\n",
    "### Kafka message format:\n",
    "```\n",
    "key:   \"12345\"  (user_id)\n",
    "value: {\"user_id\":\"1\",\"movie_id\":\"31\",\"rating\":\"4.5\",\"timestamp\":\"1260759144\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemat JSON z Kafki\n",
    "rating_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"movie_id\", StringType(), True),\n",
    "    StructField(\"rating\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Czytaj stream z Kafki\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"movielens-ratings\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 10000) \\\n",
    "    .load()\n",
    "\n",
    "# Parsuj JSON z value\n",
    "parsed_stream = kafka_stream \\\n",
    "    .select(\n",
    "        col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
    "        from_json(col(\"value\").cast(\"string\"), rating_schema).alias(\"data\"),\n",
    "        col(\"topic\"),\n",
    "        col(\"partition\"),\n",
    "        col(\"offset\"),\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"kafka_key\",\n",
    "        col(\"data.user_id\").cast(\"integer\").alias(\"user_id\"),\n",
    "        col(\"data.movie_id\").cast(\"integer\").alias(\"movie_id\"),\n",
    "        col(\"data.rating\").cast(\"double\").alias(\"rating\"),\n",
    "        col(\"data.timestamp\").cast(\"long\").alias(\"rating_ts\"),\n",
    "        \"topic\", \"partition\", \"offset\", \"kafka_timestamp\"\n",
    "    )\n",
    "\n",
    "print(\"Schema streamu:\")\n",
    "parsed_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output 1: Console (debug) - sprawdz czy dane plyna\n",
    "console_query = parsed_stream \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .queryName(\"ratings_console\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream do konsoli uruchomiony.\")\n",
    "print(\"Uruchom flow NiFi aby wyslac dane do Kafki!\")\n",
    "print(f\"Query status: {console_query.status}\")\n",
    "\n",
    "# Poczekaj na kilka micro-batchy\n",
    "time.sleep(30)\n",
    "console_query.stop()\n",
    "print(\"Stream zatrzymany.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Full Pipeline: NiFi -> Kafka -> Spark -> HDFS + PostgreSQL\n",
    "\n",
    "Teraz budujemy kompletny pipeline z dwoma sinkami:\n",
    "- **HDFS**: surowe dane w formacie Parquet (archiwum)\n",
    "- **PostgreSQL**: agregaty w czasie rzeczywistym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sink 1: HDFS Parquet (append mode, partycjonowanie po dacie)\n",
    "hdfs_stream = parsed_stream \\\n",
    "    .withColumn(\"date\", to_date(col(\"kafka_timestamp\"))) \\\n",
    "    .select(\"user_id\", \"movie_id\", \"rating\", \"rating_ts\", \"date\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", f\"{HDFS_URL}/data/movielens/streaming/ratings\") \\\n",
    "    .option(\"checkpointLocation\", f\"{HDFS_URL}/checkpoints/ratings_hdfs\") \\\n",
    "    .partitionBy(\"date\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .queryName(\"ratings_to_hdfs\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream -> HDFS uruchomiony\")\n",
    "print(f\"Output: {HDFS_URL}/data/movielens/streaming/ratings/\")\n",
    "print(f\"Checkpoint: {HDFS_URL}/checkpoints/ratings_hdfs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sink 2: PostgreSQL (agregaty per film w micro-batch)\n",
    "\n",
    "def write_to_postgres(batch_df, batch_id):\n",
    "    \"\"\"Zapisz agregaty z micro-batcha do PostgreSQL.\"\"\"\n",
    "    if batch_df.isEmpty():\n",
    "        return\n",
    "    \n",
    "    # Oblicz agregaty per movie\n",
    "    movie_stats = batch_df.groupBy(\"movie_id\").agg(\n",
    "        count(\"*\").alias(\"batch_count\"),\n",
    "        avg(\"rating\").alias(\"batch_avg_rating\"),\n",
    "        max(\"rating\").alias(\"batch_max_rating\"),\n",
    "        min(\"rating\").alias(\"batch_min_rating\")\n",
    "    )\n",
    "    \n",
    "    # Zapisz do PostgreSQL (tabela streaming_movie_stats)\n",
    "    movie_stats.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(jdbc_url, \"movielens.streaming_movie_stats\", properties=jdbc_props)\n",
    "    \n",
    "    print(f\"  Batch {batch_id}: {batch_df.count()} ratings -> \"\n",
    "          f\"{movie_stats.count()} movie stats\")\n",
    "\n",
    "pg_stream = parsed_stream \\\n",
    "    .select(\"user_id\", \"movie_id\", \"rating\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .option(\"checkpointLocation\", f\"{HDFS_URL}/checkpoints/ratings_pg\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .queryName(\"ratings_to_postgres\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stream -> PostgreSQL uruchomiony\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring aktywnych streamow\n",
    "print(\"=== Aktywne streamy ===\")\n",
    "for q in spark.streams.active:\n",
    "    status = q.status\n",
    "    progress = q.lastProgress\n",
    "    print(f\"\\n  Query: {q.name}\")\n",
    "    print(f\"  Status: {'active' if q.isActive else 'stopped'}\")\n",
    "    print(f\"  Message: {status.get('message', 'n/a')}\")\n",
    "    if progress:\n",
    "        print(f\"  Rows/sec: {progress.get('processedRowsPerSecond', 0):.0f}\")\n",
    "        print(f\"  Batch ID: {progress.get('batchId', 'n/a')}\")\n",
    "        sources = progress.get('sources', [{}])\n",
    "        if sources:\n",
    "            s = sources[0]\n",
    "            print(f\"  Start offset: {s.get('startOffset', 'n/a')}\")\n",
    "            print(f\"  End offset: {s.get('endOffset', 'n/a')}\")\n",
    "\n",
    "# Poczekaj na przetworzenie\n",
    "print(\"\\nCzekam 60 sekund na przetworzenie danych...\")\n",
    "time.sleep(60)\n",
    "\n",
    "# Zatrzymaj streamy\n",
    "for q in spark.streams.active:\n",
    "    q.stop()\n",
    "    print(f\"Stream '{q.name}' zatrzymany.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. NiFi Site-to-Site Protocol\n",
    "\n",
    "Site-to-Site (S2S) to natywny protokol NiFi do przesylania danych miedzy instancjami NiFi lub z zewnetrznych aplikacji.\n",
    "\n",
    "### Zalety S2S vs Kafka:\n",
    "| Cecha | Site-to-Site | Kafka |\n",
    "|-------|-------------|-------|\n",
    "| Konfiguracja | Minimalna | Wymaga brokera |\n",
    "| Back pressure | Natywny | Konfigurowalny |\n",
    "| Provenance | Pelny lineage | Tylko offset |\n",
    "| Multi-consumer | Nie | Tak |\n",
    "| Replay | Nie | Tak (retention) |\n",
    "| Use case | NiFi-to-NiFi | Uniwersalny |\n",
    "\n",
    "### Konfiguracja S2S:\n",
    "```\n",
    "nifi.remote.input.http.enabled=true\n",
    "nifi.remote.input.socket.port=10000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdz konfiguracje Site-to-Site\n",
    "try:\n",
    "    s2s_info = nifi_get(\"/site-to-site\")\n",
    "    print(\"=== Site-to-Site Configuration ===\")\n",
    "    print(f\"  Controller:  {s2s_info.get('controller', {})}\")\n",
    "except Exception as e:\n",
    "    print(f\"S2S info: {e}\")\n",
    "\n",
    "# Tworzenie Remote Process Group (RPG) - polaczenie do innej instancji NiFi\n",
    "# RPG pozwala przesylac dane miedzy klasterami NiFi\n",
    "\n",
    "def create_remote_process_group(pg_id, target_uri, transport=\"HTTP\"):\n",
    "    \"\"\"Tworzy Remote Process Group do komunikacji S2S.\"\"\"\n",
    "    body = {\n",
    "        \"revision\": {\"version\": 0},\n",
    "        \"component\": {\n",
    "            \"targetUris\": target_uri,\n",
    "            \"communicationsTimeout\": \"30 sec\",\n",
    "            \"yieldDuration\": \"10 sec\",\n",
    "            \"transportProtocol\": transport,\n",
    "            \"position\": {\"x\": 400, \"y\": 100}\n",
    "        }\n",
    "    }\n",
    "    return nifi_post(f\"/process-groups/{pg_id}/remote-process-groups\", body)\n",
    "\n",
    "# Przyklad: polaczenie do innej instancji NiFi\n",
    "# rpg = create_remote_process_group(ROOT_PG_ID, \"https://nifi-remote:8443/nifi\")\n",
    "# print(f\"RPG created: {rpg['id']}\")\n",
    "\n",
    "print(\"\\nSite-to-Site jest przydatny gdy masz wiele instancji NiFi\")\n",
    "print(\"np. Edge NiFi (zbiera dane) -> Central NiFi (przetwarza)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. NiFi Registry - wersjonowanie flow\n",
    "\n",
    "NiFi Registry to osobny serwis do wersjonowania flow (jak Git dla NiFi).\n",
    "\n",
    "### Workflow:\n",
    "1. Utworz bucket w Registry\n",
    "2. Zapisz Process Group do Registry (commit)\n",
    "3. Edytuj flow w NiFi\n",
    "4. Zapisz nowa wersje (commit)\n",
    "5. Mozesz cofnac do poprzedniej wersji (revert)\n",
    "\n",
    "```\n",
    "NiFi Canvas  ───commit───>  NiFi Registry\n",
    "    │                           │\n",
    "    │<──────revert─────────────┘\n",
    "    │\n",
    "    │         ┌─ v1.0 (initial)\n",
    "    │         ├─ v1.1 (added transform)\n",
    "    │         ├─ v2.0 (added Kafka output)\n",
    "    │         └─ v2.1 (fixed error handling)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NiFi Registry API (osobny serwis, domyslnie port 18080)\n",
    "REGISTRY_API = \"http://nifi-registry:18080/nifi-registry-api\"\n",
    "\n",
    "def registry_get(path):\n",
    "    r = requests.get(f\"{REGISTRY_API}{path}\"); r.raise_for_status(); return r.json()\n",
    "\n",
    "def registry_post(path, data):\n",
    "    r = requests.post(f\"{REGISTRY_API}{path}\", json=data,\n",
    "                      headers={\"Content-Type\": \"application/json\"})\n",
    "    r.raise_for_status(); return r.json()\n",
    "\n",
    "# 1. Sprawdz istniejace buckety\n",
    "try:\n",
    "    buckets = registry_get(\"/buckets\")\n",
    "    print(f\"Istniejace buckety ({len(buckets)}):\")\n",
    "    for b in buckets:\n",
    "        print(f\"  {b['name']} (id: {b['identifier']})\")\n",
    "except Exception as e:\n",
    "    print(f\"Registry niedostepny: {e}\")\n",
    "    print(\"Upewnij sie ze kontener nifi-registry jest uruchomiony.\")\n",
    "\n",
    "# 2. Utworz bucket \"movielens-flows\"\n",
    "try:\n",
    "    bucket = registry_post(\"/buckets\", {\n",
    "        \"name\": \"movielens-flows\",\n",
    "        \"description\": \"Flow definitions for MovieLens recommender system\"\n",
    "    })\n",
    "    BUCKET_ID = bucket[\"identifier\"]\n",
    "    print(f\"\\nBucket utworzony: {BUCKET_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"Nie udalo sie utworzyc bucketu: {e}\")\n",
    "    BUCKET_ID = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Podlacz NiFi do Registry\n",
    "# Najpierw dodaj Registry Client w NiFi\n",
    "\n",
    "try:\n",
    "    # Sprawdz istniejacych klientow\n",
    "    clients = nifi_get(\"/controller/registry-clients\")\n",
    "    existing = clients.get(\"registries\", [])\n",
    "    print(f\"Istniejacy Registry Clients: {len(existing)}\")\n",
    "    \n",
    "    if not existing:\n",
    "        # Dodaj nowego klienta\n",
    "        client = nifi_post(\"/controller/registry-clients\", {\n",
    "            \"revision\": {\"version\": 0},\n",
    "            \"component\": {\n",
    "                \"name\": \"Local NiFi Registry\",\n",
    "                \"uri\": \"http://nifi-registry:18080\"\n",
    "            }\n",
    "        })\n",
    "        print(f\"Registry Client dodany: {client['id']}\")\n",
    "    else:\n",
    "        print(f\"Registry Client juz istnieje: {existing[0]['id']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Registry client: {e}\")\n",
    "\n",
    "# 4. Zapisz Process Group do Registry (start version control)\n",
    "if BUCKET_ID and KAFKA_PG_ID:\n",
    "    try:\n",
    "        vc = nifi_post(f\"/versions/process-groups/{KAFKA_PG_ID}\", {\n",
    "            \"processGroupRevision\": {\"version\": 0},\n",
    "            \"versionedFlow\": {\n",
    "                \"bucketId\": BUCKET_ID,\n",
    "                \"flowName\": \"ratings-to-kafka\",\n",
    "                \"description\": \"Pipeline: CSV ratings -> Kafka topic\",\n",
    "                \"comments\": \"Initial version\"\n",
    "            }\n",
    "        })\n",
    "        print(f\"\\nFlow zapisany w Registry!\")\n",
    "        print(f\"  Version: {vc.get('versionControlInformation', {}).get('version', '?')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Version control: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Error handling i retry w NiFi\n",
    "\n",
    "NiFi ma wbudowane mechanizmy obslugi bledow:\n",
    "\n",
    "### Strategie:\n",
    "1. **Failure relationship** - kazdy procesor ma relacje `failure` ktora mozna skierowac do:\n",
    "   - Retry (polacz failure z powrotem do tego samego procesora)\n",
    "   - Dead letter queue (osobny procesor PutFile/PutHDFS)\n",
    "   - Alert (LogMessage, PutEmail)\n",
    "\n",
    "2. **Penalty duration** - FlowFile ktory spowodowal blad jest \"ukarany\" (domyslnie 30s)\n",
    "\n",
    "3. **Yield duration** - procesor ktory napotkal blad czeka przed ponowna proba (domyslnie 1s)\n",
    "\n",
    "4. **Bulletin** - komunikat o bledzie widoczny w UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzymy flow z error handling\n",
    "err_pg = nifi_post(f\"/process-groups/{ROOT_PG_ID}/process-groups\", {\n",
    "    \"revision\": {\"version\": 0},\n",
    "    \"component\": {\"name\": \"Error Handling Example\", \"position\": {\"x\": 100, \"y\": 400}}\n",
    "})\n",
    "ERR_PG_ID = err_pg[\"id\"]\n",
    "print(f\"Error Handling PG: {ERR_PG_ID}\\n\")\n",
    "\n",
    "# Glowny procesor (moze failowac)\n",
    "p_invoke = create_processor(\n",
    "    ERR_PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.InvokeHTTP\",\n",
    "    \"Call Rating API\",\n",
    "    {\"x\": 100, \"y\": 100},\n",
    "    config={\n",
    "        \"HTTP Method\": \"POST\",\n",
    "        \"Remote URL\": \"http://api:8000/ratings\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Retry (UpdateAttribute z licznikiem prob)\n",
    "p_retry_count = create_processor(\n",
    "    ERR_PG_ID,\n",
    "    \"org.apache.nifi.processors.attributes.UpdateAttribute\",\n",
    "    \"Increment Retry Count\",\n",
    "    {\"x\": 400, \"y\": 100},\n",
    "    config={\n",
    "        \"retry.count\": \"${retry.count:replaceNull('0'):plus(1)}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Router: retry vs dead letter\n",
    "p_retry_router = create_processor(\n",
    "    ERR_PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.RouteOnAttribute\",\n",
    "    \"Check Retry Limit\",\n",
    "    {\"x\": 400, \"y\": 300},\n",
    "    config={\n",
    "        \"Routing Strategy\": \"Route to Property name\",\n",
    "        \"can_retry\": \"${retry.count:lt(3)}\",   # max 3 proby\n",
    "    },\n",
    "    auto_terminate=[\"unmatched\"]\n",
    ")\n",
    "\n",
    "# Dead letter queue\n",
    "p_dead_letter = create_processor(\n",
    "    ERR_PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.PutFile\",\n",
    "    \"Dead Letter Queue\",\n",
    "    {\"x\": 400, \"y\": 500},\n",
    "    config={\n",
    "        \"Directory\": \"/data/dead_letter/${now():format('yyyy-MM-dd')}\",\n",
    "        \"Conflict Resolution Strategy\": \"replace\"\n",
    "    },\n",
    "    auto_terminate=[\"success\", \"failure\"]\n",
    ")\n",
    "\n",
    "# Success sink\n",
    "p_log_success = create_processor(\n",
    "    ERR_PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.LogAttribute\",\n",
    "    \"Log Success\",\n",
    "    {\"x\": 100, \"y\": 300},\n",
    "    config={\"Log Level\": \"info\"},\n",
    "    auto_terminate=[\"success\"]\n",
    ")\n",
    "\n",
    "# Polaczenia\n",
    "# success -> log\n",
    "create_connection(ERR_PG_ID, p_invoke[\"id\"], p_log_success[\"id\"],\n",
    "                  [\"Response\"], \"success\")\n",
    "# failure -> retry counter\n",
    "create_connection(ERR_PG_ID, p_invoke[\"id\"], p_retry_count[\"id\"],\n",
    "                  [\"Failure\", \"No Retry\", \"Retry\"], \"on failure\")\n",
    "# retry counter -> router\n",
    "create_connection(ERR_PG_ID, p_retry_count[\"id\"], p_retry_router[\"id\"],\n",
    "                  [\"success\"], \"check retry\")\n",
    "# router -> retry (back to invoke)\n",
    "create_connection(ERR_PG_ID, p_retry_router[\"id\"], p_invoke[\"id\"],\n",
    "                  [\"can_retry\"], \"retry\")\n",
    "# router -> dead letter (unmatched = exceeded max retries)\n",
    "create_connection(ERR_PG_ID, p_retry_router[\"id\"], p_dead_letter[\"id\"],\n",
    "                  [\"unmatched\"], \"dead letter\")\n",
    "# auto-terminate Original relationship from InvokeHTTP\n",
    "\n",
    "print(\"\\nError handling flow:\")\n",
    "print(\"  InvokeHTTP -success-> LogAttribute\")\n",
    "print(\"  InvokeHTTP -failure-> UpdateAttribute(retry++) -> RouteOnAttribute\")\n",
    "print(\"    -> can_retry -> InvokeHTTP (loop)\")\n",
    "print(\"    -> exceeded  -> PutFile (dead letter queue)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## 9. Monitoring end-to-end pipeline\n",
    "\n",
    "Monitorujemy caly pipeline: NiFi -> Kafka -> Spark -> Sinks.\n",
    "Kazdy komponent ma swoje metryki - zbieramy je w jednym widoku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_pipeline():\n",
    "    \"\"\"Zbierz metryki z calego pipeline.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"  END-TO-END PIPELINE MONITORING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # --- NiFi ---\n",
    "    print(\"\\n[NiFi]\")\n",
    "    try:\n",
    "        root_status = nifi_get(f\"/flow/process-groups/root/status\")\n",
    "        rs = root_status[\"processGroupStatus\"][\"aggregateSnapshot\"]\n",
    "        print(f\"  Running processors:  {rs.get('runningCount', 0)}\")\n",
    "        print(f\"  Stopped processors:  {rs.get('stoppedCount', 0)}\")\n",
    "        print(f\"  Invalid processors:  {rs.get('invalidCount', 0)}\")\n",
    "        print(f\"  Queued FlowFiles:    {rs.get('flowFilesQueued', 0)}\")\n",
    "        print(f\"  Active threads:      {rs.get('activeThreadCount', 0)}\")\n",
    "        \n",
    "        # Bulletins\n",
    "        bulletins = nifi_get(\"/flow/bulletin-board\")\n",
    "        bl = bulletins.get(\"bulletinBoard\", {}).get(\"bulletins\", [])\n",
    "        errors = [b for b in bl if b.get(\"bulletin\", {}).get(\"level\") == \"ERROR\"]\n",
    "        print(f\"  Error bulletins:     {len(errors)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Blad: {e}\")\n",
    "    \n",
    "    # --- Kafka (via Spark) ---\n",
    "    print(\"\\n[Kafka]\")\n",
    "    try:\n",
    "        kafka_batch = spark.read \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "            .option(\"subscribe\", \"movielens-ratings\") \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .option(\"endingOffsets\", \"latest\") \\\n",
    "            .load()\n",
    "        msg_count = kafka_batch.count()\n",
    "        partitions = kafka_batch.select(\"partition\").distinct().count()\n",
    "        print(f\"  Topic: movielens-ratings\")\n",
    "        print(f\"  Messages:            {msg_count}\")\n",
    "        print(f\"  Partitions:          {partitions}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Blad: {e}\")\n",
    "    \n",
    "    # --- Spark Streaming ---\n",
    "    print(\"\\n[Spark Streaming]\")\n",
    "    active = spark.streams.active\n",
    "    print(f\"  Active streams:      {len(active)}\")\n",
    "    for q in active:\n",
    "        p = q.lastProgress\n",
    "        rows_sec = p.get(\"processedRowsPerSecond\", 0) if p else 0\n",
    "        batch_id = p.get(\"batchId\", \"?\") if p else \"?\"\n",
    "        print(f\"    {q.name}: batch={batch_id}, {rows_sec:.0f} rows/s\")\n",
    "    \n",
    "    # --- HDFS ---\n",
    "    print(\"\\n[HDFS Output]\")\n",
    "    try:\n",
    "        hdfs_data = spark.read.parquet(\n",
    "            f\"{HDFS_URL}/data/movielens/streaming/ratings\")\n",
    "        print(f\"  Ratings on HDFS:     {hdfs_data.count()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Brak danych: {e}\")\n",
    "    \n",
    "    # --- PostgreSQL ---\n",
    "    print(\"\\n[PostgreSQL Output]\")\n",
    "    try:\n",
    "        pg_data = spark.read.jdbc(\n",
    "            jdbc_url, \"movielens.streaming_movie_stats\", properties=jdbc_props)\n",
    "        print(f\"  Movie stats rows:    {pg_data.count()}\")\n",
    "        print(f\"  Unique movies:       {pg_data.select('movie_id').distinct().count()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Tabela nie istnieje: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "monitor_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10000001",
   "metadata": {},
   "source": [
    "## 10. Czyszczenie zasobow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zatrzymaj streamy Spark\n",
    "for q in spark.streams.active:\n",
    "    q.stop()\n",
    "    print(f\"Stream '{q.name}' zatrzymany.\")\n",
    "\n",
    "# Czyszczenie NiFi Process Groups (odkomentuj)\n",
    "def cleanup_pg(pg_id):\n",
    "    \"\"\"Zatrzymaj i usun Process Group.\"\"\"\n",
    "    try:\n",
    "        nifi_put(f\"/flow/process-groups/{pg_id}\", {\"id\": pg_id, \"state\": \"STOPPED\"})\n",
    "        time.sleep(2)\n",
    "        flow = nifi_get(f\"/flow/process-groups/{pg_id}\")\n",
    "        for c in flow[\"processGroupFlow\"][\"flow\"][\"connections\"]:\n",
    "            try:\n",
    "                nifi_post(f\"/flowfile-queues/{c['id']}/drop-requests\", {})\n",
    "                time.sleep(1)\n",
    "            except: pass\n",
    "            nifi_delete(f\"/connections/{c['id']}\", params={\"version\": c[\"revision\"][\"version\"]})\n",
    "        for p in flow[\"processGroupFlow\"][\"flow\"][\"processors\"]:\n",
    "            nifi_delete(f\"/processors/{p['id']}\", params={\"version\": p[\"revision\"][\"version\"]})\n",
    "        pg = nifi_get(f\"/process-groups/{pg_id}\")\n",
    "        nifi_delete(f\"/process-groups/{pg_id}\", params={\"version\": pg[\"revision\"][\"version\"]})\n",
    "        print(f\"PG {pg_id} usuniety.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Cleanup error: {e}\")\n",
    "\n",
    "# Odkomentuj aby posprzatac:\n",
    "# cleanup_pg(KAFKA_PG_ID)\n",
    "# cleanup_pg(ERR_PG_ID)\n",
    "print(\"Odkomentuj powyzsze linie aby usunac flow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark zatrzymany.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11000001",
   "metadata": {},
   "source": [
    "## Zadanie koncowe: End-to-end pipeline dla nowych ratings\n",
    "\n",
    "Zbuduj kompletny pipeline:\n",
    "\n",
    "1. **NiFi flow** (via API):\n",
    "   - `GenerateFlowFile` - symuluje nowe ratings (JSON)\n",
    "   - `UpdateAttribute` - dodaj metadane (timestamp, source)\n",
    "   - `PublishKafka` - wyslij do `movielens-new-ratings`\n",
    "   - Dodaj error handling z retry (max 3 proby) i dead letter queue\n",
    "\n",
    "2. **Spark Structured Streaming**:\n",
    "   - Czytaj z Kafki `movielens-new-ratings`\n",
    "   - Parsuj JSON, waliduj dane (rating 0.5-5.0, user_id > 0)\n",
    "   - Sink 1: HDFS Parquet (partycjonowanie po dacie)\n",
    "   - Sink 2: PostgreSQL (running average per movie)\n",
    "\n",
    "3. **Monitoring**:\n",
    "   - Uzyj funkcji `monitor_pipeline()` do sprawdzenia stanu\n",
    "   - Sprawdz provenance w NiFi\n",
    "   - Sprawdz Spark UI -> Streaming tab\n",
    "\n",
    "4. **Weryfikacja**:\n",
    "   - Ile wiadomosci jest w Kafce?\n",
    "   - Ile rekordow na HDFS?\n",
    "   - Ile filmow w tabeli agregatow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiazanie:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
