{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 09 - Structured Streaming\n",
    "\n",
    "Przetwarzanie danych w czasie rzeczywistym z Spark Structured Streaming.\n",
    "\n",
    "**Tematy:**\n",
    "- Koncepcja Structured Streaming - \"nieskończona tabela\"\n",
    "- Źródła danych: rate, socket, file, Kafka\n",
    "- Output modes: append, complete, update\n",
    "- Okna czasowe (tumbling, sliding)\n",
    "- Watermarki - obsługa spóźnionych danych\n",
    "- Streaming joins\n",
    "- Symulacja real-time ratingu filmów\n",
    "- foreachBatch - zapis wyników do PostgreSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"09_Structured_Streaming\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"recommender\",\n",
    "    \"password\": \"recommender\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Koncepcja Structured Streaming\n",
    "\n",
    "Structured Streaming traktuje strumień danych jako **nieskończoną tabelę**, do której ciągle dochodzą nowe wiersze.\n",
    "\n",
    "```\n",
    "Batch:     [dane] → przetworzenie → [wynik]\n",
    "Streaming: [dane...dane...dane...] → ciągłe przetwarzanie → [wynik aktualizowany]\n",
    "```\n",
    "\n",
    "**Kluczowa zaleta:** ten sam kod DataFrame/SQL działa w batch i streaming!\n",
    "\n",
    "### Output modes:\n",
    "- **append** - tylko nowe wiersze (domyślny, nie działa z agregacjami bez watermarki)\n",
    "- **complete** - cała tabela wyników (dla agregacji)\n",
    "- **update** - tylko zmienione wiersze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. Źródło `rate` - generator danych testowych\n",
    "\n",
    "Najprostrze źródło do nauki - generuje wiersze z autoinkrementowanym `value` i `timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Źródło rate - generuje N wierszy/sekundę\n",
    "rate_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .load()\n",
    "\n",
    "# To jest streaming DataFrame - wygląda jak zwykły ale jest \"leniwy\"\n",
    "rate_stream.printSchema()\n",
    "print(f\"Is streaming: {rate_stream.isStreaming}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przetwórz strumień - dodaj kolumny\n",
    "processed = rate_stream \\\n",
    "    .withColumn(\"value_squared\", col(\"value\") * col(\"value\")) \\\n",
    "    .withColumn(\"is_even\", col(\"value\") % 2 == 0)\n",
    "\n",
    "# Uruchom query z outputem do konsoli (pamięci)\n",
    "query = processed.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"rate_test\") \\\n",
    "    .start()\n",
    "\n",
    "# Poczekaj chwilę na dane\n",
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "# Odczytaj wyniki z pamięci\n",
    "spark.sql(\"SELECT * FROM rate_test ORDER BY timestamp DESC LIMIT 10\").show(truncate=False)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Symulacja strumienia ocen filmów\n",
    "\n",
    "Zapiszemy oceny jako pliki JSON, a Spark będzie je odczytywał jako strumień (file source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Katalog na symulowane dane streamingowe\n",
    "STREAM_DIR = \"/tmp/rating_stream\"\n",
    "CHECKPOINT_DIR = \"/tmp/rating_checkpoint\"\n",
    "\n",
    "os.makedirs(STREAM_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Wyczyść stare dane\n",
    "for f in os.listdir(STREAM_DIR):\n",
    "    os.remove(os.path.join(STREAM_DIR, f))\n",
    "\n",
    "def generate_rating_batch(batch_id, n=50):\n",
    "    \"\"\"Generuj plik JSON z losowymi ocenami.\"\"\"\n",
    "    ratings = []\n",
    "    base_time = datetime.now()\n",
    "    for i in range(n):\n",
    "        ratings.append({\n",
    "            \"user_id\": random.randint(1, 1000),\n",
    "            \"movie_id\": random.choice([1, 2, 50, 110, 260, 296, 318, 356, 480, 527, 589, 593, 2571, 4993, 58559]),\n",
    "            \"rating\": random.choice([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]),\n",
    "            \"timestamp\": (base_time + timedelta(seconds=random.randint(-30, 0))).isoformat()\n",
    "        })\n",
    "    \n",
    "    path = os.path.join(STREAM_DIR, f\"batch_{batch_id}.json\")\n",
    "    with open(path, 'w') as f:\n",
    "        for r in ratings:\n",
    "            f.write(json.dumps(r) + '\\n')\n",
    "    return path\n",
    "\n",
    "# Wygeneruj pierwszy batch\n",
    "generate_rating_batch(0)\n",
    "print(\"Pierwszy batch wygenerowany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemat strumienia - MUSI być zdefiniowany z góry (streaming nie może go wywnioskować)\n",
    "rating_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"movie_id\", IntegerType()),\n",
    "    StructField(\"rating\", DoubleType()),\n",
    "    StructField(\"timestamp\", StringType())\n",
    "])\n",
    "\n",
    "# Odczytaj strumień z katalogu\n",
    "rating_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(STREAM_DIR)\n",
    "\n",
    "# Dodaj event time jako timestamp\n",
    "rating_stream = rating_stream \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "print(f\"Is streaming: {rating_stream.isStreaming}\")\n",
    "rating_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Agregacje w strumieniu - output mode `complete`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Średnia ocena per film - aktualizowana w czasie rzeczywistym\n",
    "movie_avg_stream = rating_stream \\\n",
    "    .groupBy(\"movie_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_ratings\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"avg_rating\"),\n",
    "        max(\"event_time\").alias(\"last_rating_time\")\n",
    "    )\n",
    "\n",
    "# Uruchom z complete mode (cała tabela wyników)\n",
    "query_avg = movie_avg_stream.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"movie_averages\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/avg\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(3)\n",
    "spark.sql(\"SELECT * FROM movie_averages ORDER BY num_ratings DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodaj więcej danych - obserwuj jak wyniki się aktualizują\n",
    "for i in range(1, 4):\n",
    "    generate_rating_batch(i, n=100)\n",
    "    time.sleep(3)\n",
    "    print(f\"\\n=== Po batch {i} ===\")\n",
    "    spark.sql(\"SELECT * FROM movie_averages ORDER BY num_ratings DESC LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status query\n",
    "print(query_avg.status)\n",
    "print(f\"\\nLast progress:\")\n",
    "if query_avg.lastProgress:\n",
    "    progress = query_avg.lastProgress\n",
    "    print(f\"  Input rows: {progress.get('numInputRows', 'N/A')}\")\n",
    "    print(f\"  Processing time: {progress.get('batchDuration', 'N/A')}ms\")\n",
    "\n",
    "query_avg.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Okna czasowe (Window Operations)\n",
    "\n",
    "### Tumbling window\n",
    "Okno stałe, nieprzekrywające się: `[00:00-00:05), [00:05-00:10), ...`\n",
    "\n",
    "### Sliding window\n",
    "Okno przesuwane: slide co 1 min, okno 5 min: `[00:00-00:05), [00:01-00:06), ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyczyść checkpointy\n",
    "import shutil\n",
    "for d in os.listdir(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(os.path.join(CHECKPOINT_DIR, d), ignore_errors=True)\n",
    "\n",
    "# Odczytaj strumień od nowa\n",
    "rating_stream2 = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(STREAM_DIR) \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# Tumbling window - 10 sekundowe okna\n",
    "windowed_ratings = rating_stream2 \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"10 seconds\"),  # tumbling window\n",
    "        col(\"movie_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_ratings\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"avg_rating\")\n",
    "    )\n",
    "\n",
    "query_window = windowed_ratings.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"windowed_ratings\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/window\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(5)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT window.start, window.end, movie_id, num_ratings, avg_rating\n",
    "    FROM windowed_ratings\n",
    "    ORDER BY window.start DESC, num_ratings DESC\n",
    "    LIMIT 20\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "query_window.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyczyść checkpointy\n",
    "for d in os.listdir(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(os.path.join(CHECKPOINT_DIR, d), ignore_errors=True)\n",
    "\n",
    "# Odczytaj strumień od nowa\n",
    "rating_stream3 = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(STREAM_DIR) \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# Sliding window - okno 30s, slide co 10s\n",
    "sliding_ratings = rating_stream3 \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"30 seconds\", \"10 seconds\"),  # sliding\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_ratings\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"avg_rating\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\")\n",
    "    )\n",
    "\n",
    "query_sliding = sliding_ratings.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"sliding_ratings\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/sliding\") \\\n",
    "    .start()\n",
    "\n",
    "# Dodaj nowe dane\n",
    "generate_rating_batch(10, n=200)\n",
    "time.sleep(5)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT window.start, window.end, total_ratings, avg_rating, unique_users\n",
    "    FROM sliding_ratings\n",
    "    ORDER BY window.start DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "query_sliding.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. Watermarki - obsługa spóźnionych danych\n",
    "\n",
    "W rzeczywistym systemie dane mogą docierać z opóźnieniem. Watermark mówi Sparkowi:\n",
    "\"akceptuj dane spóźnione maksymalnie o X czasu\".\n",
    "\n",
    "Po przekroczeniu watermarki, Spark:\n",
    "- Nie aktualizuje starych okien\n",
    "- Może zwolnić pamięć stanu (state)\n",
    "- Umożliwia `append` mode z agregacjami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyczyść checkpointy\n",
    "for d in os.listdir(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(os.path.join(CHECKPOINT_DIR, d), ignore_errors=True)\n",
    "\n",
    "# Odczytaj strumień od nowa\n",
    "rating_stream4 = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(STREAM_DIR) \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# Z watermarkiem - akceptuj do 30 sekund spóźnienia\n",
    "watermarked = rating_stream4 \\\n",
    "    .withWatermark(\"event_time\", \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"10 seconds\"),\n",
    "        col(\"movie_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"cnt\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"avg_rating\")\n",
    "    )\n",
    "\n",
    "# Z watermarkiem można użyć APPEND mode!\n",
    "# Okna są emitowane dopiero po zamknięciu (watermark minął)\n",
    "query_wm = watermarked.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"watermarked_ratings\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/watermark\") \\\n",
    "    .start()\n",
    "\n",
    "# Generuj dane\n",
    "for i in range(20, 25):\n",
    "    generate_rating_batch(i, n=50)\n",
    "    time.sleep(2)\n",
    "\n",
    "time.sleep(10)\n",
    "spark.sql(\"SELECT * FROM watermarked_ratings ORDER BY window.start DESC\").show(truncate=False)\n",
    "\n",
    "query_wm.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000002",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "Stwórz streaming query z watermarkiem, który:\n",
    "1. Liczy liczbę ocen per użytkownik w oknach 1-minutowych\n",
    "2. Akceptuje spóźnienia do 1 minuty\n",
    "3. Używa append mode\n",
    "\n",
    "Generuj batche i obserwuj wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Stream-Static Join\n",
    "\n",
    "Łączenie strumienia z statycznym DataFrame (np. strumień ocen + tabela filmów)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyczyść checkpointy\n",
    "for d in os.listdir(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(os.path.join(CHECKPOINT_DIR, d), ignore_errors=True)\n",
    "\n",
    "# Załaduj statyczną tabelę filmów\n",
    "movies_static = spark.read.jdbc(jdbc_url, \"movielens.movies\", properties=jdbc_properties)\n",
    "\n",
    "# Odczytaj strumień od nowa\n",
    "rating_stream5 = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(STREAM_DIR) \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# Stream-static join - wzbogacamy strumień o tytuły filmów\n",
    "enriched_stream = rating_stream5.join(movies_static, \"movie_id\")\n",
    "\n",
    "# Agregacja per gatunek w czasie rzeczywistym\n",
    "genre_stats = enriched_stream \\\n",
    "    .groupBy(\"genres\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_ratings\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"avg_rating\")\n",
    "    )\n",
    "\n",
    "query_genre = genre_stats.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"genre_stats\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/genre\") \\\n",
    "    .start()\n",
    "\n",
    "generate_rating_batch(30, n=200)\n",
    "time.sleep(5)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT genres, num_ratings, avg_rating\n",
    "    FROM genre_stats\n",
    "    ORDER BY num_ratings DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "query_genre.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## 9. foreachBatch - zapis wyników do zewnętrznego systemu\n",
    "\n",
    "`foreachBatch` daje pełną kontrolę nad tym co się dzieje z każdym micro-batchem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyczyść checkpointy\n",
    "for d in os.listdir(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(os.path.join(CHECKPOINT_DIR, d), ignore_errors=True)\n",
    "\n",
    "# Odczytaj strumień od nowa\n",
    "rating_stream6 = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(STREAM_DIR) \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "def write_to_postgres(batch_df, batch_id):\n",
    "    \"\"\"Zapisz micro-batch do PostgreSQL.\"\"\"\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    # Agregacja per film w batchu\n",
    "    movie_stats = batch_df.groupBy(\"movie_id\").agg(\n",
    "        count(\"*\").alias(\"batch_ratings\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"batch_avg_rating\"),\n",
    "        max(\"event_time\").alias(\"last_update\")\n",
    "    ).withColumn(\"batch_id\", lit(batch_id))\n",
    "    \n",
    "    # Zapisz do PostgreSQL\n",
    "    movie_stats.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(jdbc_url, \"movielens.streaming_stats\", properties=jdbc_properties)\n",
    "    \n",
    "    print(f\"Batch {batch_id}: zapisano {movie_stats.count()} wierszy do PostgreSQL\")\n",
    "\n",
    "# Uwaga: tabela streaming_stats musi istnieć w PostgreSQL\n",
    "# Możesz ją stworzyć ręcznie lub użyć trybu \"overwrite\" na pierwszym batchu\n",
    "\n",
    "# Alternatywnie - zapisz do Parquet (nie wymaga tabeli w DB)\n",
    "def write_to_parquet(batch_df, batch_id):\n",
    "    \"\"\"Zapisz micro-batch do Parquet.\"\"\"\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    batch_df.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .parquet(\"/tmp/streaming_output\")\n",
    "    print(f\"Batch {batch_id}: zapisano {batch_df.count()} wierszy\")\n",
    "\n",
    "query_foreach = rating_stream6.writeStream \\\n",
    "    .foreachBatch(write_to_parquet) \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/foreach\") \\\n",
    "    .start()\n",
    "\n",
    "generate_rating_batch(40, n=100)\n",
    "time.sleep(5)\n",
    "generate_rating_batch(41, n=100)\n",
    "time.sleep(5)\n",
    "\n",
    "query_foreach.stop()\n",
    "\n",
    "# Sprawdź wyniki\n",
    "spark.read.parquet(\"/tmp/streaming_output\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10000001",
   "metadata": {},
   "source": [
    "## 10. Trending Movies - wykrywanie trendów\n",
    "\n",
    "Znajdź filmy z nagłym wzrostem popularności."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyczyść checkpointy\n",
    "for d in os.listdir(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(os.path.join(CHECKPOINT_DIR, d), ignore_errors=True)\n",
    "\n",
    "# Odczytaj strumień od nowa\n",
    "rating_stream7 = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(STREAM_DIR) \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# Policz oceny w 15-sekundowych oknach z watermarkiem\n",
    "trending = rating_stream7 \\\n",
    "    .withWatermark(\"event_time\", \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"15 seconds\"),\n",
    "        col(\"movie_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"rating_count\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"avg_rating\")\n",
    "    )\n",
    "\n",
    "query_trending = trending.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"trending_movies\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/trending\") \\\n",
    "    .start()\n",
    "\n",
    "# Generuj sporo danych z jednym \"trendującym\" filmem\n",
    "def generate_trending_batch(batch_id, trending_movie_id=318, n=100):\n",
    "    \"\"\"Generuj batch z jednym filmem dominującym (trending).\"\"\"\n",
    "    ratings = []\n",
    "    base_time = datetime.now()\n",
    "    for i in range(n):\n",
    "        # 70% ocen dla trending movie\n",
    "        mid = trending_movie_id if random.random() < 0.7 else random.choice([1, 50, 260, 2571])\n",
    "        ratings.append({\n",
    "            \"user_id\": random.randint(1, 1000),\n",
    "            \"movie_id\": mid,\n",
    "            \"rating\": random.choice([3.5, 4.0, 4.5, 5.0]) if mid == trending_movie_id else random.choice([1.0, 2.0, 3.0, 4.0, 5.0]),\n",
    "            \"timestamp\": (base_time + timedelta(seconds=random.randint(-10, 0))).isoformat()\n",
    "        })\n",
    "    path = os.path.join(STREAM_DIR, f\"trending_{batch_id}.json\")\n",
    "    with open(path, 'w') as f:\n",
    "        for r in ratings:\n",
    "            f.write(json.dumps(r) + '\\n')\n",
    "\n",
    "for i in range(50, 55):\n",
    "    generate_trending_batch(i)\n",
    "    time.sleep(2)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# Który film trenduje?\n",
    "movies_static = spark.read.jdbc(jdbc_url, \"movielens.movies\", properties=jdbc_properties)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT movie_id, SUM(rating_count) as total, ROUND(AVG(avg_rating), 2) as avg\n",
    "    FROM trending_movies\n",
    "    GROUP BY movie_id\n",
    "    ORDER BY total DESC\n",
    "\"\"\").join(movies_static, \"movie_id\").select(\"title\", \"total\", \"avg\").show(truncate=False)\n",
    "\n",
    "query_trending.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11000001",
   "metadata": {},
   "source": [
    "## Zadanie końcowe\n",
    "\n",
    "Zbuduj **\"Real-time Movie Dashboard\"** streaming pipeline:\n",
    "\n",
    "1. Źródło: file stream z symulowanymi ocenami\n",
    "2. Wzbogacenie: stream-static join z movies (tytuły, gatunki)\n",
    "3. Metryki w oknie 30s z watermarkiem 1 min:\n",
    "   - Top 5 filmów po liczbie ocen (trending)\n",
    "   - Średnia ocena per gatunek\n",
    "   - Liczba unikalnych użytkowników\n",
    "4. Zapis: foreachBatch do plików Parquet\n",
    "5. Generuj batche i obserwuj jak metryki się zmieniają"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(STREAM_DIR, ignore_errors=True)\n",
    "shutil.rmtree(CHECKPOINT_DIR, ignore_errors=True)\n",
    "shutil.rmtree(\"/tmp/streaming_output\", ignore_errors=True)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
