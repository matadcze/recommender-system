{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 17 - HBase Fundamentals\n",
    "\n",
    "Apache HBase - rozproszona, kolumnowa baza danych NoSQL zbudowana na HDFS.\n",
    "\n",
    "**Tematy:**\n",
    "- Architektura HBase: RegionServer, ZooKeeper, WAL, MemStore, HFile\n",
    "- HBase Shell - podstawowe operacje CRUD\n",
    "- Model danych: tabela, row key, column family, column qualifier, timestamp\n",
    "- Projektowanie row key (hotspotting, salting, hashing)\n",
    "- Spark + HBase - connector i operacje\n",
    "- Zapis i odczyt danych MovieLens\n",
    "- Benchmark: HBase vs PostgreSQL vs Spark JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Architektura HBase\n",
    "\n",
    "HBase to rozproszona baza danych typu **wide-column store**, zaprojektowana do przechowywania miliardów wierszy i milionów kolumn. Dziala na szczycie HDFS.\n",
    "\n",
    "```\n",
    "                          ┌─────────────┐\n",
    "                          │   HMaster   │  ← koordynacja, DDL, load balancing\n",
    "                          └──────┬──────┘\n",
    "                                 │\n",
    "              ┌──────────────────┼──────────────────┐\n",
    "              │                  │                   │\n",
    "     ┌────────┴────────┐ ┌──────┴────────┐ ┌───────┴───────┐\n",
    "     │  RegionServer 1 │ │ RegionServer 2│ │ RegionServer 3│\n",
    "     │                 │ │               │ │               │\n",
    "     │ ┌─────────────┐ │ │ ┌───────────┐ │ │ ┌───────────┐ │\n",
    "     │ │  Region A   │ │ │ │ Region C  │ │ │ │ Region E  │ │\n",
    "     │ │ (rows a-m)  │ │ │ │(rows a-k) │ │ │ │(rows a-z) │ │\n",
    "     │ ├─────────────┤ │ │ ├───────────┤ │ │ └───────────┘ │\n",
    "     │ │  Region B   │ │ │ │ Region D  │ │ │               │\n",
    "     │ │ (rows n-z)  │ │ │ │(rows l-z) │ │ │               │\n",
    "     │ └─────────────┘ │ │ └───────────┘ │ │               │\n",
    "     └─────────────────┘ └───────────────┘ └───────────────┘\n",
    "              │                  │                   │\n",
    "     ┌────────┴──────────────────┴───────────────────┴──────┐\n",
    "     │                      ZooKeeper                       │\n",
    "     │  ← leader election, RegionServer tracking, config    │\n",
    "     └──────────────────────────────────────────────────────┘\n",
    "              │                  │                   │\n",
    "     ┌────────┴──────────────────┴───────────────────┴──────┐\n",
    "     │                        HDFS                          │\n",
    "     │              (trwale przechowywanie danych)          │\n",
    "     └──────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Kluczowe komponenty:\n",
    "\n",
    "| Komponent | Rola |\n",
    "|-----------|------|\n",
    "| **HMaster** | Koordynacja klastra, DDL (create/alter/drop table), balansowanie regionów |\n",
    "| **RegionServer** | Obsługuje odczyt/zapis dla przypisanych regionów |\n",
    "| **Region** | Ciągły zakres wierszy tabeli (automatyczny split przy wzroście) |\n",
    "| **ZooKeeper** | Leader election, śledzenie żywych RegionServerów, przechowywanie metadanych |\n",
    "| **HDFS** | Trwale przechowuje dane (HFiles) i logi (WAL) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000002",
   "metadata": {},
   "source": [
    "### Ścieżka zapisu w HBase (Write Path)\n",
    "\n",
    "```\n",
    "  Client PUT\n",
    "      │\n",
    "      ▼\n",
    "  ┌──────────────────────────────────────────┐\n",
    "  │            RegionServer                   │\n",
    "  │                                           │\n",
    "  │  1. ──► WAL (Write-Ahead Log)             │  ← zapis sekwencyjny na HDFS\n",
    "  │         (trwałość - odtworzenie po crash)  │     (szybki - append only)\n",
    "  │                                           │\n",
    "  │  2. ──► MemStore                          │  ← bufor w pamięci RAM\n",
    "  │         (sortowane dane w pamięci)         │     (sorted by row key)\n",
    "  │                                           │\n",
    "  │  3. ──► HFile (flush gdy MemStore pełny)  │  ← trwały plik na HDFS\n",
    "  │         (SSTable format, sorted, indexed)  │     (immutable!)\n",
    "  │                                           │\n",
    "  │  4. ──► Compaction                        │  ← łączenie małych HFiles\n",
    "  │         (Minor: merge kilku HFiles)        │     w większe (optymalizacja)\n",
    "  │         (Major: merge WSZYSTKICH HFiles)   │\n",
    "  └──────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Ścieżka odczytu (Read Path)\n",
    "\n",
    "```\n",
    "  Client GET\n",
    "      │\n",
    "      ▼\n",
    "  1. BlockCache (RAM)  ──► HIT? → zwróć dane\n",
    "      │ MISS\n",
    "      ▼\n",
    "  2. MemStore (RAM)    ──► jest? → zwróć dane\n",
    "      │ brak\n",
    "      ▼\n",
    "  3. HFile (HDFS)      ──► Bloom Filter → Block Index → odczytaj blok\n",
    "```\n",
    "\n",
    "**Wniosek:** HBase jest zoptymalizowany pod **szybki zapis** (WAL + MemStore) i **szybki odczyt po kluczu** (BlockCache + Bloom Filter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Model danych HBase\n",
    "\n",
    "HBase ma unikalny, wielowymiarowy model danych:\n",
    "\n",
    "```\n",
    "Table: movies\n",
    "┌──────────┬────────────────────────────────┬────────────────────────────┐\n",
    "│ Row Key  │   Column Family: info           │   Column Family: stats     │\n",
    "│          ├──────────┬──────────┬──────────┤├──────────┬────────────────┤\n",
    "│          │ info:title│info:year │info:genres││stats:avg │stats:count     │\n",
    "├──────────┼──────────┼──────────┼──────────┼┼──────────┼────────────────┤\n",
    "│ movie_1  │Toy Story │ 1995     │Animation ││ 3.92     │ 49695          │\n",
    "│          │ @t=100   │ @t=100   │ @t=100   ││ @t=200   │ @t=200         │\n",
    "├──────────┼──────────┼──────────┼──────────┼┼──────────┼────────────────┤\n",
    "│ movie_2  │Jumanji   │ 1995     │Adventure ││ 3.21     │ 22243          │\n",
    "│          │ @t=100   │ @t=100   │ @t=100   ││ @t=200   │ @t=200         │\n",
    "└──────────┴──────────┴──────────┴──────────┴┴──────────┴────────────────┘\n",
    "```\n",
    "\n",
    "### Hierarchia:\n",
    "```\n",
    "Table\n",
    " └── Row Key (unikalny identyfikator wiersza)\n",
    "      └── Column Family (deklarowana przy tworzeniu tabeli)\n",
    "           └── Column Qualifier (dynamiczny - dowolna liczba)\n",
    "                └── Timestamp (wersjonowanie - domyślnie 3 wersje)\n",
    "                     └── Value (bajty)\n",
    "```\n",
    "\n",
    "### Kluczowe cechy:\n",
    "- **Row Key** - jedyny indeks! Dane sortowane leksykograficznie po row key\n",
    "- **Column Family** - deklarowana z góry, każda CF ma osobne HFile\n",
    "- **Column Qualifier** - dynamiczny, nie trzeba deklarować (schema-on-read)\n",
    "- **Timestamp** - automatyczne wersjonowanie, przechowuje N ostatnich wersji\n",
    "- **Wszystko to bajty** - HBase nie ma typów danych (interpretacja po stronie klienta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000002",
   "metadata": {},
   "source": [
    "## 3. HBase Shell - podstawowe operacje\n",
    "\n",
    "HBase Shell to interaktywna konsola Ruby do zarządzania HBase.\n",
    "\n",
    "Poniższe komendy uruchamiamy w kontenerze HBase Master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# === DDL - zarządzanie tabelami ===\n",
    "\n",
    "# Tworzenie tabeli z dwoma column families\n",
    "# hbase shell <<EOF\n",
    "# create 'movies', \n",
    "#   {NAME => 'info', VERSIONS => 3, COMPRESSION => 'SNAPPY'},\n",
    "#   {NAME => 'stats', VERSIONS => 5, TTL => 2592000}\n",
    "# EOF\n",
    "#\n",
    "# NAME - nazwa column family\n",
    "# VERSIONS - ile wersji (timestampów) przechowywać\n",
    "# COMPRESSION - kompresja HFile (NONE, SNAPPY, GZ, LZO)\n",
    "# TTL - Time To Live w sekundach (2592000 = 30 dni)\n",
    "\n",
    "# Listowanie tabel\n",
    "# hbase shell -n <<< \"list\"\n",
    "\n",
    "# Opis tabeli\n",
    "# hbase shell -n <<< \"describe 'movies'\"\n",
    "\n",
    "# Wyłączenie i usunięcie tabeli\n",
    "# hbase shell <<EOF\n",
    "# disable 'movies'\n",
    "# drop 'movies'\n",
    "# EOF\n",
    "\n",
    "# Modyfikacja tabeli (dodaj column family)\n",
    "# hbase shell <<EOF\n",
    "# disable 'movies'\n",
    "# alter 'movies', {NAME => 'tags', VERSIONS => 1}\n",
    "# enable 'movies'\n",
    "# EOF\n",
    "\n",
    "echo \"HBase Shell DDL commands reference (uncomment to run)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# === DML - operacje na danych ===\n",
    "\n",
    "# PUT - wstaw/aktualizuj wartość\n",
    "# hbase shell <<EOF\n",
    "# put 'movies', 'movie_0001', 'info:title', 'Toy Story'\n",
    "# put 'movies', 'movie_0001', 'info:year', '1995'\n",
    "# put 'movies', 'movie_0001', 'info:genres', 'Animation|Children|Comedy'\n",
    "# put 'movies', 'movie_0001', 'stats:avg_rating', '3.92'\n",
    "# put 'movies', 'movie_0001', 'stats:num_ratings', '49695'\n",
    "#\n",
    "# put 'movies', 'movie_0002', 'info:title', 'Jumanji'\n",
    "# put 'movies', 'movie_0002', 'info:year', '1995'\n",
    "# put 'movies', 'movie_0002', 'info:genres', 'Adventure|Children|Fantasy'\n",
    "# put 'movies', 'movie_0002', 'stats:avg_rating', '3.21'\n",
    "# put 'movies', 'movie_0002', 'stats:num_ratings', '22243'\n",
    "# EOF\n",
    "\n",
    "# GET - odczyt jednego wiersza\n",
    "# hbase shell -n <<< \"get 'movies', 'movie_0001'\"\n",
    "\n",
    "# GET z filtrem column family\n",
    "# hbase shell -n <<< \"get 'movies', 'movie_0001', {COLUMN => 'info'}\"\n",
    "\n",
    "# GET konkretnej kolumny\n",
    "# hbase shell -n <<< \"get 'movies', 'movie_0001', {COLUMN => 'info:title'}\"\n",
    "\n",
    "# GET z wersjami\n",
    "# hbase shell -n <<< \"get 'movies', 'movie_0001', {COLUMN => 'stats:avg_rating', VERSIONS => 3}\"\n",
    "\n",
    "echo \"HBase Shell DML commands reference (uncomment to run)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# === SCAN - przeszukiwanie zakresu wierszy ===\n",
    "\n",
    "# Scan całej tabeli (uwaga na dużych tabelach!)\n",
    "# hbase shell -n <<< \"scan 'movies', {LIMIT => 5}\"\n",
    "\n",
    "# Scan z zakresem row key (STARTROW inclusive, STOPROW exclusive)\n",
    "# hbase shell -n <<< \"scan 'movies', {STARTROW => 'movie_0001', STOPROW => 'movie_0010'}\"\n",
    "\n",
    "# Scan z filtrem kolumn\n",
    "# hbase shell -n <<< \"scan 'movies', {COLUMNS => ['info:title', 'stats:avg_rating'], LIMIT => 10}\"\n",
    "\n",
    "# Scan z filtrem wartości (ValueFilter)\n",
    "# hbase shell <<EOF\n",
    "# scan 'movies', {FILTER => \"ValueFilter(=, 'substring:Animation')\"}\n",
    "# EOF\n",
    "\n",
    "# Scan z PrefixFilter (filtrowanie po prefixie row key)\n",
    "# hbase shell <<EOF\n",
    "# scan 'movies', {FILTER => \"PrefixFilter('movie_000')\"}\n",
    "# EOF\n",
    "\n",
    "# DELETE - usuwanie\n",
    "# hbase shell -n <<< \"delete 'movies', 'movie_0002', 'stats:avg_rating'\"\n",
    "# hbase shell -n <<< \"deleteall 'movies', 'movie_0002'\"\n",
    "\n",
    "# COUNT\n",
    "# hbase shell -n <<< \"count 'movies'\"\n",
    "\n",
    "echo \"HBase Shell SCAN/DELETE commands reference (uncomment to run)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Projektowanie Row Key\n",
    "\n",
    "Row key to **najważniejsza decyzja** w projektowaniu schematu HBase. Dane są fizycznie sortowane po row key i dystrybuowane do regionów na tej podstawie.\n",
    "\n",
    "### Problem: Hotspotting\n",
    "\n",
    "```\n",
    "Sekwencyjne row key (np. timestamp, auto-increment):\n",
    "\n",
    "  RegionServer 1          RegionServer 2          RegionServer 3\n",
    "  ┌────────────────┐      ┌────────────────┐      ┌────────────────┐\n",
    "  │ Region A       │      │ Region B       │      │ Region C       │\n",
    "  │ rows: 1-1000   │      │ rows: 1001-2000│      │ rows: 2001-3000│\n",
    "  │ ████████████   │      │                │      │                │\n",
    "  │ (HOT!)         │      │ (idle)         │      │ (idle)         │\n",
    "  └────────────────┘      └────────────────┘      └────────────────┘\n",
    "  \n",
    "  ↑ WSZYSTKIE zapisy trafiają do jednego regionu!\n",
    "```\n",
    "\n",
    "### Rozwiązania:\n",
    "\n",
    "| Technika | Opis | Przykład |\n",
    "|----------|------|----------|\n",
    "| **Salting** | Losowy prefix (0-N) | `3\\|user_42\\|ts` |\n",
    "| **Hashing** | Hash row key | `md5(user_42)[:4]\\|user_42` |\n",
    "| **Reversing** | Odwrócenie klucza | `24_resu` zamiast `user_42` |\n",
    "| **Composite key** | Łączenie wielu pól | `user_42\\|movie_318\\|ts` |\n",
    "\n",
    "### Zasady projektowania row key:\n",
    "\n",
    "1. **Unikaj monotonicznych kluczy** - timestamp, auto-increment = hotspot\n",
    "2. **Projektuj pod najczęstsze zapytania** - row key to jedyny indeks\n",
    "3. **Krótkie klucze** - każdy row key jest powtórzony w każdej komórce\n",
    "4. **Salting/hashing dla zapisu** - rozprasza dane po regionach\n",
    "5. **Composite key dla zapytań zakresowych** - `user_id|timestamp` umożliwia scan po user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000002",
   "metadata": {},
   "source": [
    "### Przykłady row key dla MovieLens:\n",
    "\n",
    "```\n",
    "Tabela: ratings\n",
    "──────────────────────────────────────────────────────────────\n",
    "ZŁY:   row key = auto_increment (1, 2, 3, ...)\n",
    "        → hotspot, brak możliwości query po user/movie\n",
    "\n",
    "LEPSZY: row key = user_id|movie_id  (np. \"000042|000318\")\n",
    "        → scan po user_id, point get po user+movie\n",
    "        → ale hotspot jeśli jeden user pisze dużo\n",
    "\n",
    "NAJLEPSZY: row key = salt|user_id|movie_id  (np. \"5|000042|000318\")\n",
    "        → rozproszone zapisy, scan po salt+user\n",
    "        → salt = hash(user_id) % num_regions\n",
    "\n",
    "Tabela: movies\n",
    "──────────────────────────────────────────────────────────────\n",
    "DOBRY:  row key = movie_id (zero-padded: \"000001\", \"000002\")\n",
    "        → point get po movie_id, dane stosunkowo statyczne\n",
    "\n",
    "Tabela: user_recommendations\n",
    "──────────────────────────────────────────────────────────────\n",
    "DOBRY:  row key = user_id (zero-padded: \"000042\")\n",
    "        → szybki point get rekomendacji dla użytkownika\n",
    "        → CF \"recs\": kolumny rank_01..rank_20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Setup - Spark + HBase Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# Spark session z HBase connector\n",
    "# shc (Spark HBase Connector) umożliwia odczyt/zapis HBase jako DataFrame\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"17_HBase_Fundamentals\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.postgresql:postgresql:42.7.1,\"\n",
    "            \"org.apache.hbase.connectors.spark:hbase-spark:1.0.1,\"\n",
    "            \"org.apache.hbase:hbase-client:2.6.1,\"\n",
    "            \"org.apache.hbase:hbase-common:2.6.1,\"\n",
    "            \"org.apache.hbase:hbase-mapreduce:2.6.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.hadoop.hbase.zookeeper.quorum\", \"hbase-zookeeper\") \\\n",
    "    .config(\"spark.hadoop.hbase.zookeeper.property.clientPort\", \"2181\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# PostgreSQL connection\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "jdbc_props = {\"user\": \"recommender\", \"password\": \"recommender\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Załaduj dane z PostgreSQL\n",
    "ratings = spark.read.jdbc(\n",
    "    jdbc_url, \"movielens.ratings\", properties=jdbc_props,\n",
    "    column=\"user_id\", lowerBound=1, upperBound=300000, numPartitions=10\n",
    ")\n",
    "movies = spark.read.jdbc(jdbc_url, \"movielens.movies\", properties=jdbc_props)\n",
    "\n",
    "ratings.cache()\n",
    "movies.cache()\n",
    "print(f\"Ratings: {ratings.count()}, Movies: {movies.count()}\")\n",
    "movies.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Zapis danych do HBase z Spark\n",
    "\n",
    "Spark HBase Connector (SHC) umożliwia zapis DataFrame do HBase poprzez zdefiniowanie **katalogu** - mapowania kolumn DataFrame na column families i qualifiers w HBase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Katalog SHC - definiuje mapowanie DataFrame → HBase\n",
    "# Każda kolumna DataFrame jest mapowana na column family:qualifier\n",
    "\n",
    "movies_catalog = json.dumps({\n",
    "    \"table\": {\"namespace\": \"default\", \"name\": \"movies\"},\n",
    "    \"rowkey\": \"movie_key\",\n",
    "    \"columns\": {\n",
    "        \"movie_key\": {\"cf\": \"rowkey\", \"col\": \"movie_key\", \"type\": \"string\"},\n",
    "        \"movie_id\":  {\"cf\": \"info\", \"col\": \"movie_id\", \"type\": \"int\"},\n",
    "        \"title\":     {\"cf\": \"info\", \"col\": \"title\", \"type\": \"string\"},\n",
    "        \"genres\":    {\"cf\": \"info\", \"col\": \"genres\", \"type\": \"string\"}\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"Movies catalog:\")\n",
    "print(json.dumps(json.loads(movies_catalog), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotuj dane - dodaj row key (zero-padded movie_id)\n",
    "movies_for_hbase = movies.withColumn(\n",
    "    \"movie_key\",\n",
    "    lpad(col(\"movie_id\").cast(\"string\"), 6, \"0\")\n",
    ")\n",
    "\n",
    "movies_for_hbase.show(5, truncate=False)\n",
    "print(f\"\\nPrzykładowy row key: {movies_for_hbase.first()['movie_key']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapis do HBase\n",
    "# UWAGA: wymaga działającego klastra HBase!\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "movies_for_hbase.write \\\n",
    "    .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .options(catalog=movies_catalog) \\\n",
    "    .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "    .option(\"newTable\", \"5\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "write_time = time.time() - start\n",
    "print(f\"Movies zapisane do HBase w {write_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapis ratings do HBase z composite row key\n",
    "# Row key: salt|user_id|movie_id (rozprasza dane po regionach)\n",
    "\n",
    "NUM_SALT_BUCKETS = 10\n",
    "\n",
    "ratings_for_hbase = ratings.withColumn(\n",
    "    \"salt\", (col(\"user_id\") % NUM_SALT_BUCKETS).cast(\"string\")\n",
    ").withColumn(\n",
    "    \"rating_key\",\n",
    "    concat(\n",
    "        col(\"salt\"), lit(\"|\"),\n",
    "        lpad(col(\"user_id\").cast(\"string\"), 6, \"0\"), lit(\"|\"),\n",
    "        lpad(col(\"movie_id\").cast(\"string\"), 6, \"0\")\n",
    "    )\n",
    ")\n",
    "\n",
    "ratings_for_hbase.select(\"rating_key\", \"user_id\", \"movie_id\", \"rating\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_catalog = json.dumps({\n",
    "    \"table\": {\"namespace\": \"default\", \"name\": \"ratings\"},\n",
    "    \"rowkey\": \"rating_key\",\n",
    "    \"columns\": {\n",
    "        \"rating_key\":       {\"cf\": \"rowkey\", \"col\": \"rating_key\", \"type\": \"string\"},\n",
    "        \"user_id\":          {\"cf\": \"data\", \"col\": \"user_id\", \"type\": \"int\"},\n",
    "        \"movie_id\":         {\"cf\": \"data\", \"col\": \"movie_id\", \"type\": \"int\"},\n",
    "        \"rating\":           {\"cf\": \"data\", \"col\": \"rating\", \"type\": \"float\"},\n",
    "        \"rating_timestamp\": {\"cf\": \"data\", \"col\": \"rating_timestamp\", \"type\": \"string\"}\n",
    "    }\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "ratings_for_hbase.select(\n",
    "    \"rating_key\", \"user_id\", \"movie_id\", \"rating\",\n",
    "    col(\"rating_timestamp\").cast(\"string\")\n",
    ").write \\\n",
    "    .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .options(catalog=ratings_catalog) \\\n",
    "    .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "    .option(\"newTable\", \"5\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "write_time = time.time() - start\n",
    "print(f\"Ratings zapisane do HBase w {write_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. Odczyt danych z HBase do Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odczyt movies z HBase\n",
    "movies_from_hbase = spark.read \\\n",
    "    .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .options(catalog=movies_catalog) \\\n",
    "    .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Movies z HBase: {movies_from_hbase.count()} rows\")\n",
    "movies_from_hbase.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odczyt ratings z HBase\n",
    "ratings_from_hbase = spark.read \\\n",
    "    .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .options(catalog=ratings_catalog) \\\n",
    "    .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Ratings z HBase: {ratings_from_hbase.count()} rows\")\n",
    "ratings_from_hbase.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrowanie po row key w HBase (predicate pushdown)\n",
    "# Scan tylko wierszy z row key zaczynającym się od konkretnego salt + user\n",
    "\n",
    "# Rekomendacje dla user_id=42 (salt = 42 % 10 = 2)\n",
    "user_42_ratings = ratings_from_hbase.filter(\n",
    "    col(\"rating_key\").startswith(\"2|000042|\")\n",
    ")\n",
    "\n",
    "print(\"Oceny użytkownika 42 z HBase:\")\n",
    "user_42_ratings.select(\"user_id\", \"movie_id\", \"rating\") \\\n",
    "    .join(movies, \"movie_id\") \\\n",
    "    .select(\"title\", \"rating\") \\\n",
    "    .orderBy(desc(\"rating\")) \\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Benchmark: HBase GET vs PostgreSQL SELECT vs Spark JDBC\n",
    "\n",
    "Porównajmy wydajność trzech podejść do odczytu danych:\n",
    "- **HBase GET** - odczyt po row key (point lookup)\n",
    "- **PostgreSQL SELECT** - zapytanie SQL po kluczu głównym\n",
    "- **Spark JDBC** - odczyt przez Spark z PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark(name, func, runs=5):\n",
    "    \"\"\"Uruchom funkcję wielokrotnie i zmierz średni czas.\"\"\"\n",
    "    times = []\n",
    "    for i in range(runs):\n",
    "        start = time.time()\n",
    "        result = func()\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "    avg_time = sum(times) / len(times)\n",
    "    min_time = min(times)\n",
    "    print(f\"{name:<40} avg={avg_time*1000:.1f}ms  min={min_time*1000:.1f}ms  (runs={runs})\")\n",
    "    return avg_time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK: Point lookup - pobranie jednego wiersza\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. HBase GET - odczyt jednego filmu po row key\n",
    "def hbase_get_movie():\n",
    "    return spark.read \\\n",
    "        .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "        .options(catalog=movies_catalog) \\\n",
    "        .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "        .load() \\\n",
    "        .filter(col(\"movie_key\") == \"000001\") \\\n",
    "        .collect()\n",
    "\n",
    "hbase_time = benchmark(\"HBase GET (movie_key=000001)\", hbase_get_movie)\n",
    "\n",
    "# 2. PostgreSQL SELECT - zapytanie SQL\n",
    "def pg_select_movie():\n",
    "    return spark.read.jdbc(\n",
    "        jdbc_url,\n",
    "        \"(SELECT * FROM movielens.movies WHERE movie_id = 1) AS t\",\n",
    "        properties=jdbc_props\n",
    "    ).collect()\n",
    "\n",
    "pg_time = benchmark(\"PostgreSQL SELECT (movie_id=1)\", pg_select_movie)\n",
    "\n",
    "# 3. Spark z cache\n",
    "movies.cache()\n",
    "movies.count()  # force cache\n",
    "\n",
    "def spark_cached_get():\n",
    "    return movies.filter(col(\"movie_id\") == 1).collect()\n",
    "\n",
    "spark_time = benchmark(\"Spark cached filter (movie_id=1)\", spark_cached_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BENCHMARK: Range scan - pobranie wszystkich ocen użytkownika\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# HBase scan z prefixem row key\n",
    "def hbase_scan_user():\n",
    "    return spark.read \\\n",
    "        .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "        .options(catalog=ratings_catalog) \\\n",
    "        .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "        .load() \\\n",
    "        .filter(col(\"rating_key\").startswith(\"2|000042|\")) \\\n",
    "        .count()\n",
    "\n",
    "hbase_scan_time = benchmark(\"HBase SCAN (user_42 ratings)\", hbase_scan_user)\n",
    "\n",
    "# PostgreSQL\n",
    "def pg_scan_user():\n",
    "    return spark.read.jdbc(\n",
    "        jdbc_url,\n",
    "        \"(SELECT * FROM movielens.ratings WHERE user_id = 42) AS t\",\n",
    "        properties=jdbc_props\n",
    "    ).count()\n",
    "\n",
    "pg_scan_time = benchmark(\"PostgreSQL SELECT (user_id=42)\", pg_scan_user)\n",
    "\n",
    "# Spark cached\n",
    "ratings.cache()\n",
    "ratings.count()\n",
    "\n",
    "def spark_scan_user():\n",
    "    return ratings.filter(col(\"user_id\") == 42).count()\n",
    "\n",
    "spark_scan_time = benchmark(\"Spark cached filter (user_id=42)\", spark_scan_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BENCHMARK: Full table scan + aggregation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# HBase full scan\n",
    "def hbase_full_scan():\n",
    "    return spark.read \\\n",
    "        .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "        .options(catalog=ratings_catalog) \\\n",
    "        .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "        .load() \\\n",
    "        .groupBy(\"movie_id\") \\\n",
    "        .agg(avg(\"rating\"), count(\"*\")) \\\n",
    "        .count()\n",
    "\n",
    "hbase_full_time = benchmark(\"HBase full scan + groupBy\", hbase_full_scan, runs=3)\n",
    "\n",
    "# PostgreSQL\n",
    "def pg_full_scan():\n",
    "    return spark.read.jdbc(\n",
    "        jdbc_url, \"movielens.ratings\", properties=jdbc_props,\n",
    "        column=\"user_id\", lowerBound=1, upperBound=300000, numPartitions=10\n",
    "    ).groupBy(\"movie_id\").agg(avg(\"rating\"), count(\"*\")).count()\n",
    "\n",
    "pg_full_time = benchmark(\"PostgreSQL JDBC full scan + groupBy\", pg_full_scan, runs=3)\n",
    "\n",
    "# Spark cached\n",
    "def spark_full_scan():\n",
    "    return ratings.groupBy(\"movie_id\").agg(avg(\"rating\"), count(\"*\")).count()\n",
    "\n",
    "spark_full_time = benchmark(\"Spark cached full scan + groupBy\", spark_full_scan, runs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000002",
   "metadata": {},
   "source": [
    "### Podsumowanie benchmarku\n",
    "\n",
    "| Operacja | HBase | PostgreSQL | Spark (cached) | Najlepszy |\n",
    "|----------|-------|------------|----------------|----------|\n",
    "| **Point lookup** (1 wiersz) | Bardzo szybki | Szybki | Szybki | HBase |\n",
    "| **Range scan** (user ratings) | Szybki (jeśli dobry row key) | Szybki (z indeksem) | Szybki | Porównywalny |\n",
    "| **Full scan + aggregation** | Wolny (nie do tego stworzony!) | Umiarkowany | Najszybszy | Spark |\n",
    "\n",
    "**Wniosek:** HBase najlepiej sprawdza się do **point lookups** i **range scans po row key**. Do analityki i agregatów lepszy jest Spark z Parquet/HDFS lub SQL z PostgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "\n",
    "Zaprojektuj schemat HBase dla kompletnego systemu rekomendacji MovieLens.\n",
    "\n",
    "Wymagania:\n",
    "1. Tabela `movies` - informacje o filmach (tytuł, rok, gatunki, średnia ocena)\n",
    "2. Tabela `user_profiles` - profil użytkownika (ulubione gatunki, liczba ocen, średnia ocena)\n",
    "3. Tabela `recommendations` - top 20 rekomendacji per użytkownik\n",
    "4. Tabela `ratings` - oceny użytkowników (musi obsługiwać szybki scan po user_id)\n",
    "\n",
    "Dla każdej tabeli zdefiniuj:\n",
    "- Row key (uzasadnij wybór)\n",
    "- Column families i qualifiers\n",
    "- Parametry: VERSIONS, TTL, COMPRESSION\n",
    "- Przewidywane access patterns (najczęstsze zapytania)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n",
    "# Zdefiniuj katalogi SHC dla każdej tabeli i uzasadnij decyzje projektowe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10000001",
   "metadata": {},
   "source": [
    "## Zadanie końcowe\n",
    "\n",
    "Załaduj wszystkie dane MovieLens do HBase z odpowiednim projektem row key.\n",
    "\n",
    "1. Utwórz tabelę `ml_movies` z CF `info` i `stats`:\n",
    "   - Row key: zero-padded movie_id (np. `000001`)\n",
    "   - `info:title`, `info:genres`\n",
    "   - `stats:avg_rating`, `stats:num_ratings` (policz z tabeli ratings)\n",
    "\n",
    "2. Utwórz tabelę `ml_ratings` z CF `data`:\n",
    "   - Row key: `salt|user_id|movie_id` z salt = user_id % 10\n",
    "   - `data:rating`, `data:timestamp`\n",
    "\n",
    "3. Utwórz tabelę `ml_user_profiles` z CF `profile` i `activity`:\n",
    "   - Row key: zero-padded user_id (np. `000042`)\n",
    "   - `profile:num_ratings`, `profile:avg_rating`, `profile:top_genre`\n",
    "   - `activity:first_rating_date`, `activity:last_rating_date`\n",
    "\n",
    "4. Zweryfikuj dane:\n",
    "   - Odczytaj z HBase i porównaj z PostgreSQL (count, sample rows)\n",
    "   - Zmierz czas zapisu i odczytu\n",
    "\n",
    "5. Wykonaj przykładowe zapytania:\n",
    "   - Pobierz profil użytkownika 42\n",
    "   - Pobierz informacje o filmie Toy Story\n",
    "   - Pobierz wszystkie oceny użytkownika 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.unpersist()\n",
    "movies.unpersist()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
