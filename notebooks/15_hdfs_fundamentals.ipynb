{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 15 - HDFS Fundamentals\n",
    "\n",
    "Hadoop Distributed File System - rozproszony system plików stanowiący fundament ekosystemu Hadoop.\n",
    "\n",
    "**Tematy:**\n",
    "- Architektura HDFS: NameNode, DataNode, bloki\n",
    "- HDFS CLI - podstawowe operacje\n",
    "- Upload danych MovieLens do HDFS\n",
    "- Spark + HDFS - odczyt i zapis\n",
    "- Replication factor i fault tolerance\n",
    "- HDFS Web UI i monitoring\n",
    "- Porównanie: local fs vs HDFS vs PostgreSQL JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Architektura HDFS\n",
    "\n",
    "```\n",
    "                  ┌──────────────┐\n",
    "                  │   NameNode   │  ← metadane (nazwy plików, bloki, lokalizacje)\n",
    "                  │   (master)   │  ← Single Point of Failure (dlatego HA!)\n",
    "                  └──────┬───────┘\n",
    "                         │\n",
    "            ┌────────────┼────────────┐\n",
    "            ▼            ▼            ▼\n",
    "     ┌────────────┐┌────────────┐┌────────────┐\n",
    "     │ DataNode 1 ││ DataNode 2 ││ DataNode 3 │\n",
    "     │            ││            ││            │\n",
    "     │ Block A    ││ Block A    ││ Block B    │  ← dane (bloki 128MB)\n",
    "     │ Block B    ││ Block C    ││ Block C    │  ← replication factor=2\n",
    "     └────────────┘└────────────┘└────────────┘\n",
    "```\n",
    "\n",
    "### Kluczowe koncepty:\n",
    "- **Block size**: domyślnie 128MB (duże bloki → mniej metadanych w NameNode)\n",
    "- **Replication factor**: domyślnie 3 (każdy blok na 3 DataNodeach)\n",
    "- **Write-once, read-many**: pliki nie są edytowane, tylko dopisywane lub nadpisywane\n",
    "- **Data locality**: Spark przetwarza dane tam gdzie są (unika transferu sieciowego)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Setup i połączenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"15_HDFS_Fundamentals\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# HDFS connection\n",
    "HDFS_URL = \"hdfs://namenode:9000\"\n",
    "HDFS_DATA = f\"{HDFS_URL}/data/movielens\"\n",
    "\n",
    "# PostgreSQL\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "jdbc_props = {\"user\": \"recommender\", \"password\": \"recommender\", \"driver\": \"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. HDFS CLI\n",
    "\n",
    "HDFS ma CLI bardzo podobne do standardowych poleceń Unix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podstawowe komendy HDFS (uruchamiane z kontenera z hadoop)\n",
    "# Tutaj symulujemy via pyspark\n",
    "\n",
    "# Sprawdź konfigurację Hadoop z poziomu Spark\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "print(f\"Default FS: {hadoop_conf.get('fs.defaultFS', 'not set')}\")\n",
    "print(f\"Replication: {hadoop_conf.get('dfs.replication', 'not set')}\")\n",
    "print(f\"Block size: {hadoop_conf.get('dfs.blocksize', 'not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# HDFS CLI - podstawowe operacje\n",
    "# (wymaga dostępu do komendy hdfs w kontenerze)\n",
    "\n",
    "# Listowanie katalogu root\n",
    "hdfs dfs -ls /\n",
    "\n",
    "# Tworzenie katalogu\n",
    "hdfs dfs -mkdir -p /data/movielens/raw\n",
    "hdfs dfs -mkdir -p /data/movielens/bronze\n",
    "hdfs dfs -mkdir -p /data/movielens/silver\n",
    "hdfs dfs -mkdir -p /data/movielens/gold\n",
    "\n",
    "# Sprawdź strukturę\n",
    "hdfs dfs -ls -R /data/movielens/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Upload pliku do HDFS\n",
    "# hdfs dfs -put /local/path/rating.csv /data/movielens/raw/\n",
    "\n",
    "# Download z HDFS\n",
    "# hdfs dfs -get /data/movielens/raw/rating.csv /local/path/\n",
    "\n",
    "# Podgląd pliku (pierwsze linie)\n",
    "# hdfs dfs -head /data/movielens/raw/rating.csv\n",
    "\n",
    "# Rozmiar pliku\n",
    "# hdfs dfs -du -h /data/movielens/raw/\n",
    "\n",
    "# Informacje o blokach pliku\n",
    "# hdfs fsck /data/movielens/raw/rating.csv -blocks -locations\n",
    "\n",
    "# Usuwanie\n",
    "# hdfs dfs -rm -r /data/movielens/raw/rating.csv\n",
    "\n",
    "# Zmiana replication factor\n",
    "# hdfs dfs -setrep 2 /data/movielens/raw/rating.csv\n",
    "\n",
    "echo \"HDFS CLI commands reference (uncomment to run)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Upload danych do HDFS przez Spark\n",
    "\n",
    "Zamiast CLI, możemy użyć Spark do zapisu danych na HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Załaduj dane z PostgreSQL\n",
    "ratings = spark.read.jdbc(\n",
    "    jdbc_url, \"movielens.ratings\", properties=jdbc_props,\n",
    "    column=\"user_id\", lowerBound=1, upperBound=300000, numPartitions=10\n",
    ")\n",
    "movies = spark.read.jdbc(jdbc_url, \"movielens.movies\", properties=jdbc_props)\n",
    "\n",
    "# Zapisz na HDFS jako Parquet\n",
    "ratings.write.mode(\"overwrite\").parquet(f\"{HDFS_DATA}/raw/ratings\")\n",
    "movies.write.mode(\"overwrite\").parquet(f\"{HDFS_DATA}/raw/movies\")\n",
    "\n",
    "print(f\"Ratings zapisane na HDFS: {ratings.count()} rows\")\n",
    "print(f\"Movies zapisane na HDFS: {movies.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odczytaj z HDFS\n",
    "ratings_hdfs = spark.read.parquet(f\"{HDFS_DATA}/raw/ratings\")\n",
    "movies_hdfs = spark.read.parquet(f\"{HDFS_DATA}/raw/movies\")\n",
    "\n",
    "print(f\"Ratings z HDFS: {ratings_hdfs.count()} rows\")\n",
    "ratings_hdfs.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacje o plikach na HDFS (via Hadoop FileSystem API)\n",
    "fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "    spark.sparkContext._jvm.java.net.URI(HDFS_URL),\n",
    "    spark.sparkContext._jsc.hadoopConfiguration()\n",
    ")\n",
    "\n",
    "path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(f\"{HDFS_DATA}/raw/ratings\")\n",
    "status = fs.listStatus(path)\n",
    "\n",
    "print(f\"Pliki w {HDFS_DATA}/raw/ratings/:\")\n",
    "total_size = 0\n",
    "for s in status:\n",
    "    size_mb = s.getLen() / 1024 / 1024\n",
    "    total_size += s.getLen()\n",
    "    print(f\"  {s.getPath().getName():<40} {size_mb:.1f} MB  replication={s.getReplication()}\")\n",
    "\n",
    "print(f\"\\nTotal: {total_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Benchmark: Local vs HDFS vs PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Zapisz też lokalnie dla porównania\n",
    "ratings.write.mode(\"overwrite\").parquet(\"/tmp/ratings_local\")\n",
    "\n",
    "def benchmark(name, func, runs=3):\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.time()\n",
    "        func()\n",
    "        times.append(time.time() - start)\n",
    "    avg = sum(times) / len(times)\n",
    "    print(f\"{name:<35} {avg:.2f}s (avg of {runs})\")\n",
    "    return avg\n",
    "\n",
    "query = lambda df: df.filter(col(\"rating\") >= 4.0).groupBy(\"movie_id\").count().count()\n",
    "\n",
    "print(\"=== Read + filter + groupBy benchmark ===\")\n",
    "benchmark(\"Local Parquet (/tmp)\",\n",
    "    lambda: query(spark.read.parquet(\"/tmp/ratings_local\")))\n",
    "\n",
    "benchmark(\"HDFS Parquet\",\n",
    "    lambda: query(spark.read.parquet(f\"{HDFS_DATA}/raw/ratings\")))\n",
    "\n",
    "benchmark(\"PostgreSQL JDBC\",\n",
    "    lambda: query(spark.read.jdbc(jdbc_url, \"movielens.ratings\", properties=jdbc_props)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Replication i Fault Tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz z różnym replication factor\n",
    "for rep in [1, 2, 3]:\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\"dfs.replication\", str(rep))\n",
    "    ratings.limit(10000).write.mode(\"overwrite\") \\\n",
    "        .parquet(f\"{HDFS_DATA}/test_replication/rep_{rep}\")\n",
    "\n",
    "# Sprawdź rozmiar na dysku per replication\n",
    "for rep in [1, 2, 3]:\n",
    "    path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(\n",
    "        f\"{HDFS_DATA}/test_replication/rep_{rep}\")\n",
    "    content_summary = fs.getContentSummary(path)\n",
    "    logical = content_summary.getLength() / 1024 / 1024\n",
    "    physical = content_summary.getSpaceConsumed() / 1024 / 1024\n",
    "    print(f\"Replication={rep}: logical={logical:.1f}MB, physical={physical:.1f}MB (ratio={physical/logical:.1f}x)\")\n",
    "\n",
    "# Przywróć domyślny replication\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"dfs.replication\", \"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000002",
   "metadata": {},
   "source": [
    "### Co się dzieje gdy DataNode padnie?\n",
    "\n",
    "1. NameNode wykrywa brak heartbeatu (timeout 10 min)\n",
    "2. Bloki z tego DataNode są oznaczone jako under-replicated\n",
    "3. NameNode zleca replikację na inny DataNode\n",
    "4. Dane są nadal dostępne z pozostałych replik\n",
    "\n",
    "**Replication factor = 3 → klaster przeżywa utratę 2 DataNodeów jednocześnie.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. HDFS Web UI\n",
    "\n",
    "NameNode Web UI: **http://namenode:9870**\n",
    "\n",
    "Co można zobaczyć:\n",
    "- Overview: pojemność, wykorzystanie, live/dead DataNodes\n",
    "- Datanodes: status każdego DataNode\n",
    "- Browse: przeglądarka plików HDFS\n",
    "- Block Scanner: status replikacji bloków\n",
    "\n",
    "### Zadanie 1\n",
    "1. Otwórz HDFS Web UI\n",
    "2. Znajdź plik ratings w `/data/movielens/raw/`\n",
    "3. Sprawdź na których DataNodeach są bloki tego pliku\n",
    "4. Ile bloków ma plik? Jaki jest ich rozmiar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie / notatki:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Data Locality w Spark\n",
    "\n",
    "Spark próbuje uruchomić task na tym samym node gdzie są dane (data locality levels):\n",
    "\n",
    "| Level | Opis | Szybkość |\n",
    "|-------|------|---------|\n",
    "| PROCESS_LOCAL | Dane w pamięci tego samego executor | Najszybsze |\n",
    "| NODE_LOCAL | Dane na tym samym node (HDFS DataNode) | Szybkie |\n",
    "| RACK_LOCAL | Dane w tym samym racku | OK |\n",
    "| ANY | Dane na dowolnym node | Najwolniejsze (transfer sieciowy) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź locality w Spark UI → Stages → Task Locality Level\n",
    "# Po uruchomieniu tego joba, otwórz Spark UI i sprawdź locality\n",
    "\n",
    "result = spark.read.parquet(f\"{HDFS_DATA}/raw/ratings\") \\\n",
    "    .groupBy(\"movie_id\") \\\n",
    "    .agg(avg(\"rating\"), count(\"*\")) \\\n",
    "    .count()\n",
    "\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Sprawdź Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(\"→ Stages → kliknij stage → Locality Level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## Zadanie końcowe\n",
    "\n",
    "Zbuduj Medallion Architecture na HDFS:\n",
    "\n",
    "1. **Bronze**: załaduj surowe dane z PostgreSQL → HDFS `/data/movielens/bronze/` (Parquet)\n",
    "2. **Silver**: oczyść dane (deduplikacja, nulls, enrichment) → HDFS `/data/movielens/silver/`\n",
    "3. **Gold**: agregaty (movie_stats, user_profiles) → HDFS `/data/movielens/gold/`\n",
    "4. Porównaj czas odczytu Gold z HDFS vs Gold z PostgreSQL\n",
    "5. Sprawdź rozmiar każdej warstwy na HDFS (`du -h`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
