{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 07 - Partycjonowanie Danych\n",
    "\n",
    "Zrozumienie partycjonowania - kluczowy aspekt wydajności w Spark.\n",
    "\n",
    "**Tematy:**\n",
    "- Czym są partycje i dlaczego są ważne\n",
    "- repartition vs coalesce\n",
    "- Partition pruning - partycjonowanie przy zapisie\n",
    "- Shuffle - co to jest i jak go minimalizować\n",
    "- explain() - czytanie planów wykonania\n",
    "- Broadcast join vs shuffle join\n",
    "- Bucket join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"07_Data_Partitioning\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "properties = {\n",
    "    \"user\": \"recommender\",\n",
    "    \"password\": \"recommender\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "ratings = spark.read.jdbc(\n",
    "    jdbc_url, \"movielens.ratings\", properties=properties,\n",
    "    column=\"user_id\", lowerBound=1, upperBound=300000, numPartitions=10\n",
    ")\n",
    "movies = spark.read.jdbc(jdbc_url, \"movielens.movies\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Czym są partycje?\n",
    "\n",
    "Partycja = kawałek danych przetwarzany przez jeden task.\n",
    "\n",
    "- Więcej partycji → więcej paralelizmu (ale overhead na task)\n",
    "- Mniej partycji → mniej overhead (ale mniejszy paralelizm)\n",
    "- Optymalna liczba: 2-4x liczba rdzeni\n",
    "- Optymalna wielkość: 128 MB - 200 MB per partycja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź liczbę partycji\n",
    "print(f\"Ratings partitions: {ratings.rdd.getNumPartitions()}\")\n",
    "print(f\"Movies partitions: {movies.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Rozmiar partycji (przybliżony)\n",
    "def partition_sizes(df):\n",
    "    return df.rdd.mapPartitions(lambda it: [sum(1 for _ in it)]).collect()\n",
    "\n",
    "sizes = partition_sizes(ratings)\n",
    "print(f\"\\nRozmiary partycji ratings: {sizes}\")\n",
    "print(f\"Min: {min(sizes)}, Max: {max(sizes)}, Avg: {sum(sizes)/len(sizes):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. repartition vs coalesce\n",
    "\n",
    "- **repartition(n)** - zmienia liczbę partycji (full shuffle), równomierny rozkład\n",
    "- **coalesce(n)** - zmniejsza liczbę partycji BEZ shuffle (łączy sąsiednie partycje)\n",
    "\n",
    "**Zasada:** `coalesce` do zmniejszania, `repartition` do zwiększania lub równomiernego rozłożenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition - full shuffle\n",
    "ratings_20 = ratings.repartition(20)\n",
    "print(f\"Po repartition(20): {ratings_20.rdd.getNumPartitions()} partycji\")\n",
    "print(f\"Rozmiary: {partition_sizes(ratings_20)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce - bez shuffle (tylko zmniejsza)\n",
    "ratings_4 = ratings.coalesce(4)\n",
    "print(f\"Po coalesce(4): {ratings_4.rdd.getNumPartitions()} partycji\")\n",
    "print(f\"Rozmiary: {partition_sizes(ratings_4)}\")\n",
    "\n",
    "# Uwaga: coalesce może dać nierównomierny rozkład!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition po kolumnie - dane z tym samym kluczem trafiają do tej samej partycji\n",
    "# Przydatne przed groupBy lub join po tej kolumnie!\n",
    "ratings_by_user = ratings.repartition(10, \"user_id\")\n",
    "print(f\"Partycje: {ratings_by_user.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Sprawdź rozkład - dane jednego usera są teraz w jednej partycji\n",
    "print(f\"Rozmiary: {partition_sizes(ratings_by_user)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000002",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "Porównaj czas groupBy(\"movie_id\").count() na:\n",
    "1. Oryginalnym DataFrame (10 partycji)\n",
    "2. repartition(10, \"movie_id\") - repartycjonowanie po kluczu grupowania\n",
    "3. coalesce(2)\n",
    "\n",
    "Czy repartition po kluczu pomaga?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. explain() - czytanie planów wykonania\n",
    "\n",
    "Spark kompiluje operacje DataFrame do planu wykonania. `explain()` pokazuje ten plan.\n",
    "\n",
    "- **Parsed Logical Plan** - co napisałeś\n",
    "- **Analyzed Logical Plan** - po rozwiązaniu nazw\n",
    "- **Optimized Logical Plan** - po optymalizacji Catalyst\n",
    "- **Physical Plan** - jak Spark to wykona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prosty explain\n",
    "ratings.filter(col(\"rating\") >= 4.0).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozszerzony explain - wszystkie poziomy\n",
    "ratings.filter(col(\"rating\") >= 4.0) \\\n",
    "    .groupBy(\"movie_id\") \\\n",
    "    .count() \\\n",
    "    .explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatted explain - najczytelniejszy\n",
    "ratings.join(movies, \"movie_id\") \\\n",
    "    .groupBy(\"title\") \\\n",
    "    .agg(avg(\"rating\").alias(\"avg_rating\")) \\\n",
    "    .orderBy(desc(\"avg_rating\")) \\\n",
    "    .explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000002",
   "metadata": {},
   "source": [
    "### Co szukać w planie:\n",
    "\n",
    "- **Exchange** = shuffle (kosztowna operacja!)\n",
    "- **BroadcastHashJoin** = broadcast join (szybki, mały dataset rozesłany do wszystkich workerów)\n",
    "- **SortMergeJoin** = shuffle join (oba datasety sortowane i łączone)\n",
    "- **HashAggregate** = agregacja z hashmap\n",
    "- **Filter** = filtrowanie (dobrze jeśli jest push-down do źródła)\n",
    "- **Scan** = odczyt danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównanie planów: z i bez filtra\n",
    "print(\"=== Bez filtra ===\")\n",
    "ratings.groupBy(\"movie_id\").count().explain()\n",
    "\n",
    "print(\"\\n=== Z filtrem ===\")\n",
    "ratings.filter(col(\"user_id\") < 1000).groupBy(\"movie_id\").count().explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Shuffle - wróg wydajności\n",
    "\n",
    "Shuffle = przenoszenie danych między partycjami (przez sieć!).\n",
    "\n",
    "**Operacje powodujące shuffle:**\n",
    "- `groupBy` / `agg`\n",
    "- `join` (shuffle join)\n",
    "- `repartition`\n",
    "- `distinct`\n",
    "- `orderBy` (globalne sortowanie)\n",
    "\n",
    "**Jak minimalizować shuffle:**\n",
    "1. Filtruj wcześniej (mniej danych do shuffle)\n",
    "2. Broadcast join zamiast shuffle join\n",
    "3. repartition po kluczu przed groupBy\n",
    "4. Używaj coalesce zamiast repartition gdy zmniejszasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql.shuffle.partitions kontroluje liczbę partycji po shuffle\n",
    "# Domyślnie 200 - za dużo dla małych danych, za mało dla dużych\n",
    "\n",
    "print(f\"Domyślne shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "# Zmniejsz dla naszego datasetu\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "\n",
    "# Po groupBy - ile partycji?\n",
    "result = ratings.groupBy(\"movie_id\").count()\n",
    "print(f\"Po groupBy z shuffle.partitions=10: {result.rdd.getNumPartitions()} partycji\")\n",
    "\n",
    "# Przywróć\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive Query Execution (AQE) - Spark sam optymalizuje partycje\n",
    "# Domyślnie włączone w Spark 3.x+\n",
    "print(f\"AQE enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "\n",
    "# AQE automatycznie:\n",
    "# - Łączy małe partycje po shuffle (coalesce)\n",
    "# - Konwertuje sort-merge join na broadcast join gdy dane są małe\n",
    "# - Optymalizuje skew join (nierównomiernie rozłożone dane)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Broadcast Join vs Shuffle Join\n",
    "\n",
    "- **Broadcast join** - mały DataFrame jest kopiowany na każdy executor. Brak shuffle dużego DataFrame.\n",
    "- **Shuffle join** - oba DataFrames są shufflowane po kluczu joina.\n",
    "\n",
    "**Zasada:** Jeśli jeden DataFrame jest mały (<10MB domyślnie), Spark automatycznie użyje broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark automatycznie broadcastuje movies (mały DataFrame)\n",
    "# Sprawdź plan - powinien być BroadcastHashJoin\n",
    "ratings.join(movies, \"movie_id\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wymuś broadcast hint\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Explicit broadcast\n",
    "result_broadcast = ratings.join(broadcast(movies), \"movie_id\")\n",
    "result_broadcast.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównanie czasu: broadcast vs shuffle join\n",
    "import time\n",
    "\n",
    "# Wyłącz auto broadcast aby wymusić shuffle join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "start = time.time()\n",
    "ratings.join(movies, \"movie_id\").count()\n",
    "shuffle_time = time.time() - start\n",
    "print(f\"Shuffle join: {shuffle_time:.2f}s\")\n",
    "\n",
    "# Włącz broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10MB default\n",
    "\n",
    "start = time.time()\n",
    "ratings.join(broadcast(movies), \"movie_id\").count()\n",
    "broadcast_time = time.time() - start\n",
    "print(f\"Broadcast join: {broadcast_time:.2f}s\")\n",
    "print(f\"Broadcast szybszy {shuffle_time/broadcast_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000002",
   "metadata": {},
   "source": [
    "### Zadanie 2\n",
    "Porównaj plany wykonania (explain) dla:\n",
    "1. `ratings.join(movies, \"movie_id\")` - automatyczny broadcast\n",
    "2. Self-join: `ratings.alias(\"r1\").join(ratings.alias(\"r2\"), col(\"r1.movie_id\") == col(\"r2.movie_id\"))` - shuffle join\n",
    "\n",
    "Zwróć uwagę na Exchange (shuffle) w planie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. Partycjonowanie przy zapisie\n",
    "\n",
    "Partycjonowanie danych na dysku pozwala na **partition pruning** - Spark czyta tylko potrzebne partycje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz dane z partycjonowaniem po roku\n",
    "ratings_with_year = ratings.withColumn(\"year\", year(col(\"rating_timestamp\")))\n",
    "\n",
    "# Zapis partycjonowany - tworzy podkatalogi year=2005, year=2006, ...\n",
    "ratings_with_year.write \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/ratings_by_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odczytaj partycjonowane dane\n",
    "ratings_partitioned = spark.read.parquet(\"/tmp/ratings_by_year\")\n",
    "\n",
    "# Partition pruning - Spark czyta TYLKO partycję year=2015\n",
    "# Sprawdź plan - powinien być PartitionFilters\n",
    "ratings_partitioned.filter(col(\"year\") == 2015).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównanie czasu: z i bez partition pruning\n",
    "import time\n",
    "\n",
    "# Bez partition pruning (skan całego datasetu)\n",
    "start = time.time()\n",
    "ratings.withColumn(\"year\", year(col(\"rating_timestamp\"))) \\\n",
    "    .filter(col(\"year\") == 2015) \\\n",
    "    .count()\n",
    "no_pruning = time.time() - start\n",
    "\n",
    "# Z partition pruning (czyta tylko partycję year=2015)\n",
    "start = time.time()\n",
    "ratings_partitioned.filter(col(\"year\") == 2015).count()\n",
    "with_pruning = time.time() - start\n",
    "\n",
    "print(f\"Bez partition pruning: {no_pruning:.2f}s\")\n",
    "print(f\"Z partition pruning: {with_pruning:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000002",
   "metadata": {},
   "source": [
    "### Uwaga na liczbę partycji przy zapisie!\n",
    "\n",
    "Problem: Jeśli mamy 200 shuffle partitions i partycjonujemy po 15 latach → 200 * 15 = 3000 małych plików.\n",
    "\n",
    "**Rozwiązanie:** coalesce przed zapisem lub użyj `repartition(\"year\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lepszy zapis - kontrolujemy liczbę plików per partycja\n",
    "ratings_with_year \\\n",
    "    .repartition(\"year\") \\\n",
    "    .write \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/ratings_by_year_optimized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Data Skew - nierównomierny rozkład danych\n",
    "\n",
    "Jeśli dane są nierównomiernie rozłożone (np. 80% ocen od 10% użytkowników), niektóre partycje będą ogromne a inne prawie puste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź skew - rozkład ocen per użytkownik\n",
    "user_counts = ratings.groupBy(\"user_id\").count()\n",
    "\n",
    "user_counts.summary().show()\n",
    "\n",
    "# Top 10 najaktywniejszych\n",
    "user_counts.orderBy(desc(\"count\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizacja skew - rozkład partycji po repartition(10, \"user_id\")\n",
    "ratings_by_user = ratings.repartition(10, \"user_id\")\n",
    "sizes = partition_sizes(ratings_by_user)\n",
    "\n",
    "print(\"Rozmiary partycji po repartition(user_id):\")\n",
    "for i, s in enumerate(sizes):\n",
    "    bar = \"#\" * (s // 50000)\n",
    "    print(f\"  Partition {i:2d}: {s:>8d} rows  {bar}\")\n",
    "\n",
    "print(f\"\\nSkew ratio: {max(sizes)/min(sizes):.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technika: salting - dodaj losowy klucz żeby rozłożyć dane\n",
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "NUM_SALTS = 5\n",
    "\n",
    "# Dodaj salt do klucza\n",
    "ratings_salted = ratings \\\n",
    "    .withColumn(\"salt\", (rand() * NUM_SALTS).cast(\"int\")) \\\n",
    "    .withColumn(\"user_id_salted\", concat(col(\"user_id\"), lit(\"_\"), col(\"salt\")))\n",
    "\n",
    "# GroupBy z salted key (pierwszy krok)\n",
    "partial_agg = ratings_salted \\\n",
    "    .groupBy(\"user_id\", \"salt\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"partial_count\"),\n",
    "        sum(\"rating\").alias(\"partial_sum\")\n",
    "    )\n",
    "\n",
    "# Finalna agregacja (drugi krok - już bez skew)\n",
    "final_agg = partial_agg \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        sum(\"partial_count\").alias(\"total_count\"),\n",
    "        round(sum(\"partial_sum\") / sum(\"partial_count\"), 2).alias(\"avg_rating\")\n",
    "    )\n",
    "\n",
    "final_agg.orderBy(desc(\"total_count\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## 9. Formaty zapisu i ich wpływ na wydajność\n",
    "\n",
    "| Format | Kompresja | Column pruning | Predicate pushdown | Opis |\n",
    "|--------|-----------|----------------|-------------------|------|\n",
    "| Parquet | Tak | Tak | Tak | Domyślny, najszybszy do analiz |\n",
    "| ORC | Tak | Tak | Tak | Popularny w Hive |\n",
    "| CSV | Nie | Nie | Nie | Wolny, duży, czytelny |\n",
    "| JSON | Nie | Nie | Nie | Wolny, czytelny |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Zapisz w różnych formatach\n",
    "for fmt in [\"parquet\", \"csv\", \"json\"]:\n",
    "    start = time.time()\n",
    "    ratings.write.mode(\"overwrite\").format(fmt).save(f\"/tmp/ratings_{fmt}\")\n",
    "    write_time = time.time() - start\n",
    "    print(f\"Zapis {fmt}: {write_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównaj odczyt z filtrowaniem\n",
    "for fmt in [\"parquet\", \"csv\", \"json\"]:\n",
    "    start = time.time()\n",
    "    df = spark.read.format(fmt).load(f\"/tmp/ratings_{fmt}\")\n",
    "    if fmt == \"csv\":\n",
    "        df = df.withColumn(\"rating\", col(\"rating\").cast(\"double\"))\n",
    "    cnt = df.filter(col(\"rating\") >= 4.5).count()\n",
    "    read_time = time.time() - start\n",
    "    print(f\"Odczyt + filter {fmt}: {read_time:.2f}s ({cnt} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10000001",
   "metadata": {},
   "source": [
    "## Zadanie końcowe\n",
    "\n",
    "Zoptymalizuj następujący pipeline:\n",
    "\n",
    "1. Załaduj ratings i movies\n",
    "2. Join ratings z movies\n",
    "3. Filtruj filmy z gatunkiem \"Action\" wydane po 2010\n",
    "4. GroupBy per film: avg_rating, count\n",
    "5. Sortuj po avg_rating DESC\n",
    "\n",
    "Zoptymalizuj:\n",
    "- Gdzie umieścić filtr? (przed czy po join?)\n",
    "- Jaki typ joina wybrać?\n",
    "- Ile shuffle partitions?\n",
    "- Porównaj explain() przed i po optymalizacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wersja nieoptymalna:\n",
    "result_slow = ratings.join(movies, \"movie_id\") \\\n",
    "    .filter(col(\"genres\").contains(\"Action\")) \\\n",
    "    .filter(col(\"title\").rlike(r\"\\(201[0-9]\\)\")) \\\n",
    "    .groupBy(\"movie_id\", \"title\") \\\n",
    "    .agg(round(avg(\"rating\"), 2).alias(\"avg_rating\"), count(\"*\").alias(\"cnt\")) \\\n",
    "    .orderBy(desc(\"avg_rating\"))\n",
    "\n",
    "result_slow.explain()\n",
    "result_slow.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoja zoptymalizowana wersja:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
