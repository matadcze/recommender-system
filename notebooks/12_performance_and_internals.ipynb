{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 12 - Performance & Spark Internals\n",
    "\n",
    "Zrozumienie wewnętrznych mechanizmów Spark i optymalizacja wydajności.\n",
    "\n",
    "**Tematy:**\n",
    "- Architektura Spark: Driver, Executor, Task, Stage\n",
    "- Spark UI - czytanie DAG i metryk\n",
    "- Catalyst Optimizer - jak Spark optymalizuje zapytania\n",
    "- Tungsten - zarządzanie pamięcią i codegen\n",
    "- Predicate pushdown i column pruning\n",
    "- Porównanie strategii joinów\n",
    "- AQE (Adaptive Query Execution)\n",
    "- Troubleshooting typowych problemów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"12_Performance\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Spark UI: http://spark-master:4040 (lub sprawdź port w logach)\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "properties = {\n",
    "    \"user\": \"recommender\",\n",
    "    \"password\": \"recommender\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "ratings = spark.read.jdbc(\n",
    "    jdbc_url, \"movielens.ratings\", properties=properties,\n",
    "    column=\"user_id\", lowerBound=1, upperBound=300000, numPartitions=10\n",
    ")\n",
    "movies = spark.read.jdbc(jdbc_url, \"movielens.movies\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Architektura Spark\n",
    "\n",
    "```\n",
    "Driver Program\n",
    "  ├── SparkContext / SparkSession\n",
    "  ├── DAGScheduler → dzieli job na stages\n",
    "  └── TaskScheduler → wysyła tasks do executors\n",
    "\n",
    "Cluster Manager (Standalone/YARN/K8s)\n",
    "  ├── Executor 1\n",
    "  │   ├── Task 1.1\n",
    "  │   └── Task 1.2\n",
    "  └── Executor 2\n",
    "      ├── Task 2.1\n",
    "      └── Task 2.2\n",
    "```\n",
    "\n",
    "### Kluczowe pojęcia:\n",
    "- **Job** = jedna akcja (count, show, write)\n",
    "- **Stage** = zbiór tasków bez shuffle (shuffle = granica stage)\n",
    "- **Task** = operacja na jednej partycji\n",
    "- **Shuffle** = wymiana danych między executors (przez sieć!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź konfigurację\n",
    "print(\"=== Spark Configuration ===\")\n",
    "for key in [\"spark.executor.memory\", \"spark.executor.cores\", \n",
    "            \"spark.driver.memory\", \"spark.sql.shuffle.partitions\",\n",
    "            \"spark.sql.adaptive.enabled\", \"spark.default.parallelism\"]:\n",
    "    try:\n",
    "        val = spark.conf.get(key)\n",
    "    except:\n",
    "        val = \"not set\"\n",
    "    print(f\"  {key} = {val}\")\n",
    "\n",
    "print(f\"\\n=== Cluster ===\")\n",
    "sc = spark.sparkContext\n",
    "print(f\"  Master: {sc.master}\")\n",
    "print(f\"  App ID: {sc.applicationId}\")\n",
    "print(f\"  Default parallelism: {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. Lazy Evaluation i DAG\n",
    "\n",
    "Spark nie wykonuje operacji od razu - buduje DAG (Directed Acyclic Graph) i wykonuje dopiero przy akcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacje - NIE wykonują się od razu (lazy)\n",
    "result = ratings \\\n",
    "    .filter(col(\"rating\") >= 4.0) \\\n",
    "    .join(movies, \"movie_id\") \\\n",
    "    .groupBy(\"title\") \\\n",
    "    .agg(count(\"*\").alias(\"cnt\"), avg(\"rating\").alias(\"avg\")) \\\n",
    "    .orderBy(desc(\"cnt\"))\n",
    "\n",
    "# Nic się nie wykonało - tylko zbudowano DAG\n",
    "print(\"DAG zbudowany (ale nie wykonany)\")\n",
    "print(f\"Plan has {len(result.explain(mode='simple') or '')} chars\")\n",
    "\n",
    "# Teraz AKCJA - wykonuje się cały DAG\n",
    "start = time.time()\n",
    "result.show(5)  # <-- TU się wykonuje!\n",
    "print(f\"\\nCzas wykonania: {time.time() - start:.2f}s\")\n",
    "\n",
    "# Sprawdź Spark UI → Jobs tab → zobaczysz ten job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Catalyst Optimizer\n",
    "\n",
    "Catalyst to optymalizator zapytań Spark SQL. Automatycznie optymalizuje plan wykonania.\n",
    "\n",
    "### Fazy optymalizacji:\n",
    "1. **Analysis** - rozwiązuje nazwy kolumn i typy\n",
    "2. **Logical Optimization** - predicate pushdown, column pruning, constant folding\n",
    "3. **Physical Planning** - wybiera strategię joina, agregacji itp.\n",
    "4. **Code Generation** - Tungsten generuje bytecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain(mode=\"extended\") - pokazuje wszystkie fazy\n",
    "query = ratings \\\n",
    "    .filter(col(\"rating\") >= 4.0) \\\n",
    "    .filter(col(\"user_id\") < 1000) \\\n",
    "    .select(\"user_id\", \"movie_id\", \"rating\")\n",
    "\n",
    "query.explain(mode=\"extended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicate pushdown - Catalyst przesuwa filtry bliżej źródła danych\n",
    "# Obserwuj: filtry pojawiają się w Scan (przed odczytem danych!)\n",
    "\n",
    "print(\"=== Predicate pushdown ===\")\n",
    "ratings.filter(col(\"user_id\") == 42) \\\n",
    "    .select(\"movie_id\", \"rating\") \\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column pruning - Catalyst czyta tylko potrzebne kolumny\n",
    "# Obserwuj: w Scan pojawiają się tylko wybrane kolumny\n",
    "\n",
    "print(\"=== Column pruning ===\")\n",
    "# Mimo że ratings ma 4 kolumny, Spark przeczyta tylko 2:\n",
    "ratings.select(\"user_id\", \"rating\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant folding - Catalyst upraszcza wyrażenia stałe\n",
    "print(\"=== Constant folding ===\")\n",
    "ratings.filter(lit(1) == lit(1)).explain()  # filtr zostanie usunięty\n",
    "\n",
    "print(\"\\n=== Filter before join (optimization) ===\")\n",
    "# Catalyst sam przesuwa filtr PRZED join!\n",
    "ratings.join(movies, \"movie_id\") \\\n",
    "    .filter(col(\"rating\") >= 4.5) \\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000002",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "Porównaj plany wykonania tych dwóch zapytań:\n",
    "\n",
    "**A:** `ratings.filter(rating >= 4.0).join(movies, \"movie_id\").select(\"title\", \"rating\")`\n",
    "\n",
    "**B:** `ratings.join(movies, \"movie_id\").select(\"title\", \"rating\").filter(col(\"rating\") >= 4.0)`\n",
    "\n",
    "Czy Catalyst optymalizuje B do tego samego planu co A?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Tungsten - pamięć i codegen\n",
    "\n",
    "Tungsten to silnik wykonawczy Spark:\n",
    "- **Off-heap memory** - zarządzanie pamięcią poza JVM GC\n",
    "- **Cache-aware computation** - optymalizacja pod cache CPU\n",
    "- **Whole-stage codegen** - generuje optymalizowany bytecode Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź czy codegen jest włączony\n",
    "print(f\"Whole-stage codegen: {spark.conf.get('spark.sql.codegen.wholeStage', 'true')}\")\n",
    "\n",
    "# W explain() szukaj: *WholeStageCodegen* = Tungsten generuje kod\n",
    "ratings.groupBy(\"movie_id\") \\\n",
    "    .agg(count(\"*\"), avg(\"rating\")) \\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: codegen ON vs OFF\n",
    "ratings.cache()\n",
    "ratings.count()  # warm up\n",
    "\n",
    "# Codegen ON\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n",
    "start = time.time()\n",
    "ratings.groupBy(\"movie_id\").agg(count(\"*\"), avg(\"rating\"), stddev(\"rating\")).count()\n",
    "on_time = time.time() - start\n",
    "\n",
    "# Codegen OFF\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")\n",
    "start = time.time()\n",
    "ratings.groupBy(\"movie_id\").agg(count(\"*\"), avg(\"rating\"), stddev(\"rating\")).count()\n",
    "off_time = time.time() - start\n",
    "\n",
    "# Przywróć\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n",
    "\n",
    "print(f\"Codegen ON:  {on_time:.2f}s\")\n",
    "print(f\"Codegen OFF: {off_time:.2f}s\")\n",
    "print(f\"Codegen szybszy {off_time/on_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Porównanie strategii joinów\n",
    "\n",
    "| Strategia | Kiedy | Koszt |\n",
    "|-----------|-------|-------|\n",
    "| **Broadcast Hash Join** | Mały dataset (<10MB) | Brak shuffle dużego DF |\n",
    "| **Sort Merge Join** | Oba duże, sortowalne | Shuffle + sort obu |\n",
    "| **Shuffle Hash Join** | Jeden mieści się w pamięci executora | Shuffle + hash table |\n",
    "| **Cartesian Join** | cross join | O(n*m) - unikaj! |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast Hash Join - automatyczny (movies jest mały)\n",
    "print(\"=== Broadcast Hash Join ===\")\n",
    "ratings.join(movies, \"movie_id\").explain()\n",
    "\n",
    "start = time.time()\n",
    "ratings.join(movies, \"movie_id\").count()\n",
    "broadcast_time = time.time() - start\n",
    "print(f\"Czas: {broadcast_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort Merge Join - wymuś wyłączając broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "print(\"=== Sort Merge Join ===\")\n",
    "ratings.join(movies, \"movie_id\").explain()\n",
    "\n",
    "start = time.time()\n",
    "ratings.join(movies, \"movie_id\").count()\n",
    "smj_time = time.time() - start\n",
    "print(f\"Czas: {smj_time:.2f}s\")\n",
    "\n",
    "# Przywróć\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Hash Join - hint\n",
    "print(\"=== Shuffle Hash Join ===\")\n",
    "ratings.hint(\"shuffle_hash\").join(movies, \"movie_id\").explain()\n",
    "\n",
    "start = time.time()\n",
    "ratings.hint(\"shuffle_hash\").join(movies, \"movie_id\").count()\n",
    "shj_time = time.time() - start\n",
    "print(f\"Czas: {shj_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n=== Porównanie ===\")\n",
    "print(f\"Broadcast Hash Join: {broadcast_time:.2f}s\")\n",
    "print(f\"Sort Merge Join:     {smj_time:.2f}s\")\n",
    "print(f\"Shuffle Hash Join:   {shj_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. AQE - Adaptive Query Execution\n",
    "\n",
    "AQE (Spark 3.0+) optymalizuje plan W TRAKCIE wykonania na podstawie statystyk runtime.\n",
    "\n",
    "### Optymalizacje AQE:\n",
    "1. **Coalescing post-shuffle partitions** - łączy małe partycje po shuffle\n",
    "2. **Switching join strategies** - zmienia SortMerge na Broadcast gdy dane okazują się małe\n",
    "3. **Optimizing skew joins** - rozbija duże partycje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AQE enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "\n",
    "# Porównanie: AQE ON vs OFF\n",
    "# AQE ON\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # domyślne - za dużo\n",
    "\n",
    "start = time.time()\n",
    "result_aqe = ratings.groupBy(\"movie_id\").count()\n",
    "result_aqe.count()\n",
    "aqe_time = time.time() - start\n",
    "aqe_partitions = result_aqe.rdd.getNumPartitions()\n",
    "\n",
    "# AQE OFF\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "start = time.time()\n",
    "result_noaqe = ratings.groupBy(\"movie_id\").count()\n",
    "result_noaqe.count()\n",
    "noaqe_time = time.time() - start\n",
    "noaqe_partitions = result_noaqe.rdd.getNumPartitions()\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "print(f\"AQE ON:  {aqe_time:.2f}s, {aqe_partitions} partitions (AQE coalesced!)\")\n",
    "print(f\"AQE OFF: {noaqe_time:.2f}s, {noaqe_partitions} partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AQE skew join optimization\n",
    "# Symulujemy skew: dodajmy dużo ocen dla jednego filmu\n",
    "skewed_rows = spark.range(500000).select(\n",
    "    lit(999999).alias(\"user_id\"),\n",
    "    lit(1).alias(\"movie_id\"),  # Toy Story - bardzo dużo ocen\n",
    "    (rand() * 5).cast(\"double\").alias(\"rating\"),\n",
    "    current_timestamp().alias(\"rating_timestamp\")\n",
    ")\n",
    "\n",
    "skewed_ratings = ratings.union(skewed_rows)\n",
    "\n",
    "# Z AQE - powinno automatycznie rozwiązać skew\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "print(\"Plan z AQE (szukaj SkewJoin):\")\n",
    "skewed_ratings.join(movies, \"movie_id\") \\\n",
    "    .groupBy(\"title\") \\\n",
    "    .count() \\\n",
    "    .explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Benchmark: operacje na danych\n",
    "\n",
    "Systematyczne porównanie wydajności różnych podejść."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(name, func, runs=3):\n",
    "    \"\"\"Zmierz czas wykonania funkcji (średnia z N uruchomień).\"\"\"\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start = time.time()\n",
    "        func()\n",
    "        times.append(time.time() - start)\n",
    "    avg_time = sum(times) / len(times)\n",
    "    print(f\"{name:<40} {avg_time:.3f}s (avg of {runs})\")\n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: UDF vs built-in functions\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(StringType())\n",
    "def label_udf(rating):\n",
    "    if rating >= 4.0: return \"good\"\n",
    "    elif rating >= 2.5: return \"ok\"\n",
    "    return \"bad\"\n",
    "\n",
    "def label_builtin(r):\n",
    "    return when(r >= 4.0, \"good\").when(r >= 2.5, \"ok\").otherwise(\"bad\")\n",
    "\n",
    "print(\"=== UDF vs Built-in ===\")\n",
    "benchmark(\"Python UDF\", lambda: ratings.withColumn(\"l\", label_udf(col(\"rating\"))).count())\n",
    "benchmark(\"Built-in when/otherwise\", lambda: ratings.withColumn(\"l\", label_builtin(col(\"rating\"))).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: repartition strategies before groupBy\n",
    "print(\"=== Repartition strategies ===\")\n",
    "\n",
    "benchmark(\"No repartition (10 parts)\",\n",
    "    lambda: ratings.groupBy(\"movie_id\").agg(avg(\"rating\")).count())\n",
    "\n",
    "benchmark(\"repartition(10, movie_id)\",\n",
    "    lambda: ratings.repartition(10, \"movie_id\").groupBy(\"movie_id\").agg(avg(\"rating\")).count())\n",
    "\n",
    "benchmark(\"coalesce(2)\",\n",
    "    lambda: ratings.coalesce(2).groupBy(\"movie_id\").agg(avg(\"rating\")).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: filter order\n",
    "print(\"=== Filter order ===\")\n",
    "\n",
    "benchmark(\"Selective filter first (user_id=42)\",\n",
    "    lambda: ratings.filter(col(\"user_id\") == 42).filter(col(\"rating\") >= 4.0).count())\n",
    "\n",
    "benchmark(\"Less selective filter first (rating>=4)\",\n",
    "    lambda: ratings.filter(col(\"rating\") >= 4.0).filter(col(\"user_id\") == 42).count())\n",
    "\n",
    "# Catalyst powinien je zoptymalizować do tego samego planu!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## 9. Typowe problemy wydajnościowe i ich rozwiązania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Collecting do drivera\n",
    "# NIGDY nie rób collect() na dużym DataFrame!\n",
    "\n",
    "# ŹLE:\n",
    "# all_data = ratings.collect()  # OOM na driverze!\n",
    "\n",
    "# DOBRZE:\n",
    "sample = ratings.limit(100).collect()  # mała ilość\n",
    "# lub\n",
    "result = ratings.groupBy(\"movie_id\").count()  # agreguj w Spark\n",
    "\n",
    "print(\"Tip: Zawsze agreguj/filtruj w Spark, nie ściągaj surowych danych do drivera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Zbyt wiele małych plików po zapisie\n",
    "# Rozwiązanie: coalesce przed zapisem\n",
    "\n",
    "# ŹLE:\n",
    "# ratings.write.parquet(\"/tmp/bad\")  # 200 małych plików (shuffle.partitions)\n",
    "\n",
    "# DOBRZE:\n",
    "# ratings.coalesce(4).write.parquet(\"/tmp/good\")  # 4 pliki\n",
    "\n",
    "# Sprawdź ile plików powstanie:\n",
    "print(f\"Bez coalesce: {ratings.rdd.getNumPartitions()} plików\")\n",
    "print(f\"Z coalesce(4): 4 pliki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3: Wielokrotne obliczanie tego samego DataFrame\n",
    "# Rozwiązanie: cache()\n",
    "\n",
    "expensive_df = ratings.join(movies, \"movie_id\") \\\n",
    "    .withColumn(\"genre\", explode(split(col(\"genres\"), \"\\\\|\"))) \\\n",
    "    .groupBy(\"genre\", \"user_id\") \\\n",
    "    .agg(avg(\"rating\").alias(\"avg_rating\"))\n",
    "\n",
    "# BEZ cache - oblicza 2x\n",
    "start = time.time()\n",
    "expensive_df.filter(col(\"genre\") == \"Comedy\").count()\n",
    "expensive_df.filter(col(\"genre\") == \"Drama\").count()\n",
    "no_cache_time = time.time() - start\n",
    "\n",
    "# Z cache - oblicza 1x\n",
    "expensive_df.cache()\n",
    "start = time.time()\n",
    "expensive_df.filter(col(\"genre\") == \"Comedy\").count()\n",
    "expensive_df.filter(col(\"genre\") == \"Drama\").count()\n",
    "cache_time = time.time() - start\n",
    "\n",
    "expensive_df.unpersist()\n",
    "\n",
    "print(f\"Bez cache (2x obliczenie): {no_cache_time:.2f}s\")\n",
    "print(f\"Z cache (1x + 2x odczyt): {cache_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4: Cartesian join (cross join) - eksplozja danych\n",
    "# Spark domyślnie blokuje implicit cross join\n",
    "\n",
    "# To NIGDY nie powinno się zdarzyć na dużych danych:\n",
    "# ratings.crossJoin(movies)  # 20M × 27K = 540 MILIARDÓW wierszy!\n",
    "\n",
    "small_a = spark.createDataFrame([(1,), (2,), (3,)], [\"a\"])\n",
    "small_b = spark.createDataFrame([(\"x\",), (\"y\",)], [\"b\"])\n",
    "print(\"Cross join na małych danych (3 × 2 = 6 wierszy):\")\n",
    "small_a.crossJoin(small_b).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10000001",
   "metadata": {},
   "source": [
    "## 10. Spark UI Checklist\n",
    "\n",
    "Gdy masz problem z wydajnością, sprawdź w Spark UI:\n",
    "\n",
    "### Jobs tab:\n",
    "- Czy joby kończą się? Ile trwają?\n",
    "- Który stage jest bottleneckiem?\n",
    "\n",
    "### Stages tab:\n",
    "- **Shuffle Read/Write** - duże wartości = dużo shuffla\n",
    "- **Task Duration** - czy jest skew (1 task trwa 10x dłużej)?\n",
    "- **GC Time** - >10% = za mało pamięci\n",
    "- **Spill (Memory/Disk)** - dane nie mieszczą się w pamięci\n",
    "\n",
    "### Storage tab:\n",
    "- Cached DataFrames\n",
    "- Ile pamięci zajmują\n",
    "\n",
    "### SQL tab:\n",
    "- Plan fizyczny zapytania\n",
    "- Czas każdego operatora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11000001",
   "metadata": {},
   "source": [
    "## Zadanie końcowe\n",
    "\n",
    "Masz ten (celowo nieefektywny) pipeline. Zoptymalizuj go i zmierz różnicę:\n",
    "\n",
    "```python\n",
    "# Wersja \"naiwna\"\n",
    "result = ratings \\\n",
    "    .join(movies, \"movie_id\") \\\n",
    "    .withColumn(\"genre\", explode(split(col(\"genres\"), \"\\\\|\"))) \\\n",
    "    .filter(col(\"genre\") == \"Sci-Fi\") \\\n",
    "    .filter(col(\"rating\") >= 4.0) \\\n",
    "    .groupBy(\"title\") \\\n",
    "    .agg(count(\"*\").alias(\"cnt\"), avg(\"rating\").alias(\"avg\")) \\\n",
    "    .filter(col(\"cnt\") >= 100) \\\n",
    "    .orderBy(desc(\"avg\"))\n",
    "```\n",
    "\n",
    "Wskazówki:\n",
    "1. Kolejność filtrów\n",
    "2. Broadcast hint\n",
    "3. Filtrowanie na gatunku przed explode (genres LIKE '%Sci-Fi%')\n",
    "4. shuffle.partitions\n",
    "5. Porównaj explain() obu wersji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wersja naiwna:\n",
    "start = time.time()\n",
    "naive = ratings \\\n",
    "    .join(movies, \"movie_id\") \\\n",
    "    .withColumn(\"genre\", explode(split(col(\"genres\"), \"\\\\|\"))) \\\n",
    "    .filter(col(\"genre\") == \"Sci-Fi\") \\\n",
    "    .filter(col(\"rating\") >= 4.0) \\\n",
    "    .groupBy(\"title\") \\\n",
    "    .agg(count(\"*\").alias(\"cnt\"), avg(\"rating\").alias(\"avg\")) \\\n",
    "    .filter(col(\"cnt\") >= 100) \\\n",
    "    .orderBy(desc(\"avg\"))\n",
    "\n",
    "naive.show(10, truncate=False)\n",
    "naive_time = time.time() - start\n",
    "print(f\"Naive: {naive_time:.2f}s\")\n",
    "naive.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoja zoptymalizowana wersja:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.unpersist()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
