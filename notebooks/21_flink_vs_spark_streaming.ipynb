{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 21 - Apache Flink vs Spark Streaming\n",
    "\n",
    "Porownanie dwoch najpopularniejszych frameworkow do przetwarzania strumieniowego w ekosystemie Big Data.\n",
    "\n",
    "**Tematy:**\n",
    "- Architektura Flink: JobManager, TaskManager, Slots\n",
    "- Porownanie: Flink DataStream API vs Spark Structured Streaming\n",
    "- Flink w Python (PyFlink): Table API i DataStream API\n",
    "- Event time vs processing time\n",
    "- Watermarks i late data handling\n",
    "- Exactly-once semantics: Flink checkpointing vs Spark WAL\n",
    "- State management: Flink (RocksDB) vs Spark (in-memory/HDFS)\n",
    "- Windowing: tumbling, sliding, session windows\n",
    "- Benchmark: latency Flink vs Spark na strumieniu ratings\n",
    "- Kiedy wybrac Flink, a kiedy Spark Streaming?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2000001",
   "metadata": {},
   "source": [
    "## 1. Architektura Apache Flink\n",
    "\n",
    "Flink to framework do **natywnego przetwarzania strumieniowego** (stream-first). W przeciwienstwie do Sparka, ktory traktuje strumien jako serie micro-batchy, Flink przetwarza kazde zdarzenie indywidualnie.\n",
    "\n",
    "```\n",
    "                    ┌──────────────────────┐\n",
    "                    │   Flink Client        │\n",
    "                    │   (submit job JAR)    │\n",
    "                    └──────────┬───────────┘\n",
    "                               │\n",
    "                    ┌──────────▼───────────┐\n",
    "                    │    JobManager         │  <- koordynator klastra\n",
    "                    │  ┌─────────────────┐  │\n",
    "                    │  │   Dispatcher    │  │  <- przyjmuje joby\n",
    "                    │  │   ResourceMgr   │  │  <- zarzadza zasobami\n",
    "                    │  │   JobMaster     │  │  <- wykonuje DAG jednego joba\n",
    "                    │  └─────────────────┘  │\n",
    "                    └──────────┬───────────┘\n",
    "                               │\n",
    "            ┌──────────────────┼──────────────────┐\n",
    "            │                  │                   │\n",
    "   ┌────────▼────────┐ ┌──────▼────────┐ ┌───────▼───────┐\n",
    "   │  TaskManager 1  │ │ TaskManager 2 │ │ TaskManager 3 │\n",
    "   │                 │ │               │ │               │\n",
    "   │ ┌─────┐┌─────┐ │ │ ┌─────┐┌─────┐│ │ ┌─────┐┌─────┐│\n",
    "   │ │Slot1││Slot2│ │ │ │Slot1││Slot2││ │ │Slot1││Slot2││\n",
    "   │ │Task ││Task │ │ │ │Task ││Task ││ │ │Task ││Task ││\n",
    "   │ └─────┘└─────┘ │ │ └─────┘└─────┘│ │ └─────┘└─────┘│\n",
    "   └─────────────────┘ └──────────────┘ └───────────────┘\n",
    "```\n",
    "\n",
    "### Kluczowe komponenty:\n",
    "\n",
    "| Komponent | Rola | Odpowiednik w Spark |\n",
    "|-----------|------|--------------------|\n",
    "| **JobManager** | Koordynacja, scheduling, checkpointing | Driver |\n",
    "| **TaskManager** | Wykonywanie taskow (worker) | Executor |\n",
    "| **Slot** | Jednostka zasobow w TaskManager | Core/Thread w Executor |\n",
    "| **JobGraph** | DAG operatorow | DAG stages |\n",
    "| **Checkpoint** | Snapshot stanu do odtworzenia | Checkpoint (WAL/state store) |\n",
    "\n",
    "### Flink vs Spark - roznice architektoniczne:\n",
    "\n",
    "| Cecha | Flink | Spark Structured Streaming |\n",
    "|-------|-------|---------------------------|\n",
    "| **Model** | Natywny streaming (event-by-event) | Micro-batch (domyslnie) |\n",
    "| **Latencja** | Milisekundy | Sekundy (micro-batch interval) |\n",
    "| **Stan** | Wbudowany (RocksDB / heap) | Zewnetrzny (HDFS state store) |\n",
    "| **Checkpointing** | Asynchroniczne barierowe | Synchroniczne per micro-batch |\n",
    "| **Batch** | Streaming jako special case | Natywny batch engine |\n",
    "| **API** | DataStream, Table/SQL | DataFrame/Dataset, SQL |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3000001",
   "metadata": {},
   "source": [
    "## 2. Setup - Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"21_Flink_vs_Spark_Streaming\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "jdbc_props = {\"user\": \"recommender\", \"password\": \"recommender\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "# Katalogi na dane strumieniowe\n",
    "STREAM_DIR = \"/tmp/flink_vs_spark_stream\"\n",
    "CHECKPOINT_DIR = \"/tmp/flink_vs_spark_ckpt\"\n",
    "os.makedirs(STREAM_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Wyczysc stare dane\n",
    "for f in os.listdir(STREAM_DIR):\n",
    "    os.remove(os.path.join(STREAM_DIR, f))\n",
    "\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"Stream dir: {STREAM_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4000001",
   "metadata": {},
   "source": [
    "## 3. Setup - PyFlink (Table API)\n",
    "\n",
    "PyFlink to oficjalny Python API dla Apache Flink. Oferuje dwa glowne interfejsy:\n",
    "- **Table API** - deklaratywny, relacyjny (podobny do DataFrame)\n",
    "- **DataStream API** - niski poziom, pelna kontrola nad przetwarzaniem\n",
    "\n",
    "W naszym srodowisku Flink JobManager jest dostepny pod adresem `flink-jobmanager:8081`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
    "from pyflink.table.expressions import col as flink_col, lit as flink_lit\n",
    "from pyflink.table.window import Tumble, Slide, Session\n",
    "from pyflink.common import Row\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "\n",
    "# Flink Table Environment - tryb strumieniowy\n",
    "env_settings = EnvironmentSettings.in_streaming_mode()\n",
    "t_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "# Konfiguracja Flink\n",
    "t_env.get_config().set(\"parallelism.default\", \"2\")\n",
    "t_env.get_config().set(\"pipeline.jars\", \n",
    "    \"file:///opt/flink/lib/flink-connector-jdbc-3.1.0-1.17.jar;\"\n",
    "    \"file:///opt/flink/lib/postgresql-42.7.1.jar\")\n",
    "\n",
    "print(\"PyFlink Table Environment utworzony (streaming mode)\")\n",
    "print(f\"Flink JobManager UI: http://flink-jobmanager:8081\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5000001",
   "metadata": {},
   "source": [
    "## 4. Event Time vs Processing Time\n",
    "\n",
    "Kluczowa koncepcja w przetwarzaniu strumieniowym - jaki czas przypisac zdarzeniu?\n",
    "\n",
    "```\n",
    "Zdarzenie: Uzytkownik ocenia film o 14:00:00\n",
    "Siec opoznia o 3 sekundy\n",
    "System odbiera o 14:00:03\n",
    "\n",
    "Event time:      14:00:00  (kiedy zdarzenie WYSTAPILO)\n",
    "Processing time: 14:00:03  (kiedy system je ODEBRALO)\n",
    "Ingestion time:  14:00:03  (kiedy weszlo do systemu)\n",
    "```\n",
    "\n",
    "| Cecha | Event Time | Processing Time |\n",
    "|-------|------------|----------------|\n",
    "| **Determinizm** | Tak (powtarzalne wyniki) | Nie (zalezne od szybkosci) |\n",
    "| **Opoznione dane** | Obsluguje (watermarks) | Ignoruje problem |\n",
    "| **Zlozonosc** | Wyzsza (wymaga watermarks) | Prostsza |\n",
    "| **Uzycie** | Analityka, billing, audyt | Monitoring, alerty |\n",
    "\n",
    "### Flink:\n",
    "- Event time jest **domyslnym** trybem od Flink 1.12+\n",
    "- Watermarks definiowane per source\n",
    "\n",
    "### Spark Structured Streaming:\n",
    "- Event time obslugiwany przez kolumne timestamp w DataFrame\n",
    "- Watermarks definiowane przez `.withWatermark()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generator danych strumieniowych (wspolny dla Spark i Flink) ---\n",
    "\n",
    "def generate_rating_events(batch_id, n=100, late_fraction=0.1):\n",
    "    \"\"\"Generuj batch ocen z czescia opoznionych zdarzen (late events).\"\"\"\n",
    "    ratings = []\n",
    "    base_time = datetime.now()\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Niektore zdarzenia sa \"opoznione\" - event_time znacznie wczesniejszy\n",
    "        if random.random() < late_fraction:\n",
    "            event_time = base_time - timedelta(seconds=random.randint(30, 120))\n",
    "            is_late = True\n",
    "        else:\n",
    "            event_time = base_time - timedelta(seconds=random.randint(0, 5))\n",
    "            is_late = False\n",
    "        \n",
    "        ratings.append({\n",
    "            \"user_id\": random.randint(1, 1000),\n",
    "            \"movie_id\": random.choice([1, 2, 50, 110, 260, 296, 318, 356, 480, 527]),\n",
    "            \"rating\": random.choice([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]),\n",
    "            \"event_time\": event_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"is_late\": is_late\n",
    "        })\n",
    "    \n",
    "    path = os.path.join(STREAM_DIR, f\"batch_{batch_id}.json\")\n",
    "    with open(path, 'w') as f:\n",
    "        for r in ratings:\n",
    "            f.write(json.dumps(r) + '\\n')\n",
    "    return path\n",
    "\n",
    "# Wygeneruj poczatkowe dane\n",
    "for i in range(3):\n",
    "    generate_rating_events(i, n=100, late_fraction=0.15)\n",
    "\n",
    "print(f\"Wygenerowano 3 batche po 100 zdarzen (15% opoznionych)\")\n",
    "print(f\"Przykladowe zdarzenie:\")\n",
    "with open(os.path.join(STREAM_DIR, \"batch_0.json\")) as f:\n",
    "    print(json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6000001",
   "metadata": {},
   "source": [
    "## 5. Watermarks i Late Data Handling\n",
    "\n",
    "Watermark to mechanizm informujacy system: \"wszystkie zdarzenia z czasem <= W juz dotarly\".\n",
    "\n",
    "```\n",
    "Strumien zdarzen (event time):\n",
    "  [14:00:01] [14:00:03] [14:00:02] [14:00:05] [13:59:50] [14:00:06]\n",
    "                                                ^^^^^^^^^\n",
    "                                                opoznione o 16s!\n",
    "\n",
    "Watermark z tolerancja 10s:\n",
    "  Max event time = 14:00:06\n",
    "  Watermark = 14:00:06 - 10s = 13:59:56\n",
    "  \n",
    "  Zdarzenie 13:59:50 < 13:59:56 → ODRZUCONE (za pozno!)\n",
    "  Gdyby tolerancja byla 20s → Watermark = 13:59:46 → ZAAKCEPTOWANE\n",
    "```\n",
    "\n",
    "### Porownanie Watermarks:\n",
    "\n",
    "| Cecha | Flink | Spark Structured Streaming |\n",
    "|-------|-------|---------------------------|\n",
    "| **Definicja** | `WatermarkStrategy.forBoundedOutOfOrderness()` | `.withWatermark(\"col\", \"delay\")` |\n",
    "| **Granularnosc** | Per event (rekord po rekordzie) | Per micro-batch |\n",
    "| **Custom logic** | Pelna kontrola (`WatermarkGenerator`) | Tylko opoznienie stalej dlugosci |\n",
    "| **Late data** | Side output (oddzielny strumien) | Odrzucenie (drop) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SPARK: Watermarks i obsluga opoznionych danych ---\n",
    "\n",
    "import shutil\n",
    "for d in os.listdir(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(os.path.join(CHECKPOINT_DIR, d), ignore_errors=True)\n",
    "\n",
    "rating_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType()),\n",
    "    StructField(\"movie_id\", IntegerType()),\n",
    "    StructField(\"rating\", DoubleType()),\n",
    "    StructField(\"event_time\", StringType()),\n",
    "    StructField(\"is_late\", BooleanType())\n",
    "])\n",
    "\n",
    "spark_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(STREAM_DIR) \\\n",
    "    .withColumn(\"event_ts\", to_timestamp(\"event_time\"))\n",
    "\n",
    "# Watermark: akceptuj opoznienia do 30 sekund\n",
    "windowed_spark = spark_stream \\\n",
    "    .withWatermark(\"event_ts\", \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_ts\"), \"15 seconds\"),\n",
    "        col(\"movie_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"cnt\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"avg_rating\")\n",
    "    )\n",
    "\n",
    "query_spark_wm = windowed_spark.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"spark_watermark_demo\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/spark_wm\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT window.start, window.end, movie_id, cnt, avg_rating\n",
    "    FROM spark_watermark_demo\n",
    "    ORDER BY window.start DESC, cnt DESC\n",
    "    LIMIT 15\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "query_spark_wm.stop()\n",
    "print(\"Spark: zdarzenia opoznione >30s sa odrzucane (brak side output)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FLINK: Watermarks z Table API ---\n",
    "# Flink oferuje side output dla opoznionych danych (niedostepne w Spark)\n",
    "\n",
    "# Definicja tabeli zrodlowej z watermarkiem w Flink SQL\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TEMPORARY TABLE rating_stream (\n",
    "        user_id INT,\n",
    "        movie_id INT,\n",
    "        rating DOUBLE,\n",
    "        event_time TIMESTAMP(3),\n",
    "        is_late BOOLEAN,\n",
    "        WATERMARK FOR event_time AS event_time - INTERVAL '30' SECOND\n",
    "    ) WITH (\n",
    "        'connector' = 'filesystem',\n",
    "        'path' = '/tmp/flink_vs_spark_stream',\n",
    "        'format' = 'json'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Flink: okno tumbling 15s z watermarkiem\n",
    "result = t_env.execute_sql(\"\"\"\n",
    "    SELECT \n",
    "        TUMBLE_START(event_time, INTERVAL '15' SECOND) AS window_start,\n",
    "        TUMBLE_END(event_time, INTERVAL '15' SECOND) AS window_end,\n",
    "        movie_id,\n",
    "        COUNT(*) AS cnt,\n",
    "        ROUND(AVG(rating), 2) AS avg_rating\n",
    "    FROM rating_stream\n",
    "    GROUP BY \n",
    "        TUMBLE(event_time, INTERVAL '15' SECOND),\n",
    "        movie_id\n",
    "\"\"\")\n",
    "\n",
    "# Wyswietl wyniki (Flink wykonuje job na JobManager)\n",
    "print(\"Flink: wyniki okna tumbling 15s z watermark 30s\")\n",
    "print(\"Flink moze dodatkowo wyslac opoznione dane do side output!\")\n",
    "with result.collect() as results:\n",
    "    for i, row in enumerate(results):\n",
    "        if i >= 15:\n",
    "            break\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7000001",
   "metadata": {},
   "source": [
    "## 6. Exactly-Once Semantics i State Management\n",
    "\n",
    "### Gwarancje dostarczenia:\n",
    "\n",
    "```\n",
    "At-most-once:   Zdarzenie moze zostac utracone        (najszybsze)\n",
    "At-least-once:  Zdarzenie przetworzone >= 1 raz       (mozliwe duplikaty)\n",
    "Exactly-once:   Zdarzenie przetworzone dokladnie 1 raz (najtrudniejsze)\n",
    "```\n",
    "\n",
    "### Flink Checkpointing (Chandy-Lamport):\n",
    "```\n",
    "Source ──► Op1 ──► Op2 ──► Sink\n",
    "  │         │        │       │\n",
    "  ▼         ▼        ▼       ▼\n",
    " [barrier] przeplyw barierowy\n",
    "  │         │        │       │\n",
    "  ▼         ▼        ▼       ▼\n",
    " State     State    State   Commit\n",
    " snapshot  snapshot snapshot offset\n",
    "  │         │        │       │\n",
    "  └─────────┴────────┴───────┘\n",
    "              │\n",
    "        Distributed Snapshot\n",
    "        (RocksDB / Heap → HDFS/S3)\n",
    "```\n",
    "\n",
    "### Spark Checkpoint + WAL:\n",
    "```\n",
    "Micro-batch N:\n",
    "  1. Zapisz offsety do WAL (Write-Ahead Log)\n",
    "  2. Przetworzenie batcha\n",
    "  3. Zapisz state do HDFS state store\n",
    "  4. Commit offsetow\n",
    "\n",
    "Awaria → odtworzenie z ostatniego checkpointu + replay WAL\n",
    "```\n",
    "\n",
    "### State Management - porownanie:\n",
    "\n",
    "| Cecha | Flink | Spark Structured Streaming |\n",
    "|-------|-------|---------------------------|\n",
    "| **Backend** | RocksDB (dysk) lub Heap (RAM) | HDFS State Store (domyslny) |\n",
    "| **Skalowalnosc** | TB stanu (RocksDB) | Ograniczona pamiec executora |\n",
    "| **Inkrementalnosc** | Incremental checkpoints | Pelny snapshot co batch |\n",
    "| **Queryable state** | Tak (eksperymentalne) | Nie |\n",
    "| **TTL stanu** | Wbudowany State TTL | Recznie (mapGroupsWithState) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FLINK: Konfiguracja checkpointingu ---\n",
    "\n",
    "# W PyFlink konfigurujemy checkpointing przez StreamExecutionEnvironment\n",
    "stream_env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "# Wlacz checkpointing co 10 sekund\n",
    "stream_env.enable_checkpointing(10000)  # 10s interval\n",
    "\n",
    "# Konfiguracja checkpoint\n",
    "from pyflink.datastream.checkpointing_mode import CheckpointingMode\n",
    "stream_env.get_checkpoint_config().set_checkpointing_mode(CheckpointingMode.EXACTLY_ONCE)\n",
    "stream_env.get_checkpoint_config().set_min_pause_between_checkpoints(5000)  # min 5s miedzy ckpt\n",
    "stream_env.get_checkpoint_config().set_checkpoint_timeout(60000)  # timeout 60s\n",
    "stream_env.get_checkpoint_config().set_max_concurrent_checkpoints(1)\n",
    "\n",
    "# State backend - RocksDB dla duzych stanow\n",
    "# stream_env.set_state_backend(RocksDBStateBackend(\"hdfs://namenode:9000/flink/checkpoints\"))\n",
    "\n",
    "print(\"Flink checkpointing skonfigurowany:\")\n",
    "print(f\"  Mode: EXACTLY_ONCE\")\n",
    "print(f\"  Interval: 10s\")\n",
    "print(f\"  Min pause: 5s\")\n",
    "print(f\"  Timeout: 60s\")\n",
    "print(f\"  Max concurrent: 1\")\n",
    "\n",
    "# Dla porownania - Spark checkpointing konfigurujemy per query:\n",
    "print(\"\\nSpark checkpointing (per query):\")\n",
    "print('  .option(\"checkpointLocation\", \"hdfs://namenode:9000/spark/checkpoints/query_name\")')\n",
    "print(\"  Spark automatycznie zapisuje state + offsets co micro-batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8000001",
   "metadata": {},
   "source": [
    "## 7. Windowing - porownanie implementacji\n",
    "\n",
    "Trzy glowne typy okien czasowych:\n",
    "\n",
    "```\n",
    "TUMBLING (stale, nieprzekrywajace sie):\n",
    "  |---W1---|---W2---|---W3---|---W4---|\n",
    "  0       10      20      30      40   (sekundy)\n",
    "\n",
    "SLIDING (przesuwane, przekrywajace sie):\n",
    "  |------W1------|\n",
    "       |------W2------|\n",
    "            |------W3------|\n",
    "  Okno=15s, Slide=5s\n",
    "\n",
    "SESSION (dynamiczne, oparte na aktywnosci):\n",
    "  |--W1--|    gap    |----W2----|   gap   |--W3--|\n",
    "  zdarzenia   >10s   zdarzenia   >10s    zdarzenia\n",
    "  (gap = session timeout)\n",
    "```\n",
    "\n",
    "### Session Windows - kluczowa roznica!\n",
    "\n",
    "| Cecha | Flink | Spark |\n",
    "|-------|-------|-------|\n",
    "| **Tumbling** | `TUMBLE(time, INTERVAL)` | `window(col, \"duration\")` |\n",
    "| **Sliding** | `HOP(time, slide, size)` | `window(col, \"size\", \"slide\")` |\n",
    "| **Session** | `SESSION(time, gap)` - natywne! | Brak natywnej obslugi! |\n",
    "\n",
    "Flink obsluguje **session windows** natywnie, co jest ogromna zaleta przy analizie sesji uzytkownikow. Spark wymaga obejscia przez `mapGroupsWithState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SPARK: Tumbling i Sliding windows ---\n",
    "\n",
    "for d in os.listdir(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(os.path.join(CHECKPOINT_DIR, d), ignore_errors=True)\n",
    "\n",
    "# Wygeneruj wiecej danych\n",
    "for i in range(10, 16):\n",
    "    generate_rating_events(i, n=80)\n",
    "\n",
    "spark_stream2 = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 2) \\\n",
    "    .load(STREAM_DIR) \\\n",
    "    .withColumn(\"event_ts\", to_timestamp(\"event_time\"))\n",
    "\n",
    "# Tumbling window - 15s\n",
    "tumbling_spark = spark_stream2 \\\n",
    "    .withWatermark(\"event_ts\", \"30 seconds\") \\\n",
    "    .groupBy(window(col(\"event_ts\"), \"15 seconds\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_ratings\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"avg_rating\")\n",
    "    )\n",
    "\n",
    "q_tumbling = tumbling_spark.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"spark_tumbling\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/tumbling\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(5)\n",
    "print(\"=== SPARK: Tumbling Window 15s ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT window.start, window.end, total_ratings, unique_users, avg_rating\n",
    "    FROM spark_tumbling ORDER BY window.start DESC LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Sliding window - okno 30s, slide 10s\n",
    "spark_stream3 = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 2) \\\n",
    "    .load(STREAM_DIR) \\\n",
    "    .withColumn(\"event_ts\", to_timestamp(\"event_time\"))\n",
    "\n",
    "sliding_spark = spark_stream3 \\\n",
    "    .withWatermark(\"event_ts\", \"30 seconds\") \\\n",
    "    .groupBy(window(col(\"event_ts\"), \"30 seconds\", \"10 seconds\")) \\\n",
    "    .agg(count(\"*\").alias(\"total\"), round(avg(\"rating\"), 2).alias(\"avg\"))\n",
    "\n",
    "q_sliding = sliding_spark.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"spark_sliding\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/sliding\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(5)\n",
    "print(\"\\n=== SPARK: Sliding Window 30s / slide 10s ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT window.start, window.end, total, avg\n",
    "    FROM spark_sliding ORDER BY window.start DESC LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "q_tumbling.stop()\n",
    "q_sliding.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FLINK: Tumbling, Sliding, i Session Windows ---\n",
    "\n",
    "# Flink SQL: Tumbling window\n",
    "print(\"=== FLINK: Tumbling Window 15s ===\")\n",
    "result_tumble = t_env.execute_sql(\"\"\"\n",
    "    SELECT\n",
    "        TUMBLE_START(event_time, INTERVAL '15' SECOND) AS w_start,\n",
    "        TUMBLE_END(event_time, INTERVAL '15' SECOND) AS w_end,\n",
    "        COUNT(*) AS total_ratings,\n",
    "        COUNT(DISTINCT user_id) AS unique_users,\n",
    "        ROUND(AVG(rating), 2) AS avg_rating\n",
    "    FROM rating_stream\n",
    "    GROUP BY TUMBLE(event_time, INTERVAL '15' SECOND)\n",
    "\"\"\")\n",
    "with result_tumble.collect() as results:\n",
    "    for i, row in enumerate(results):\n",
    "        if i >= 10:\n",
    "            break\n",
    "        print(row)\n",
    "\n",
    "# Flink SQL: Sliding (HOP) window\n",
    "print(\"\\n=== FLINK: Sliding (HOP) Window 30s / slide 10s ===\")\n",
    "result_hop = t_env.execute_sql(\"\"\"\n",
    "    SELECT\n",
    "        HOP_START(event_time, INTERVAL '10' SECOND, INTERVAL '30' SECOND) AS w_start,\n",
    "        HOP_END(event_time, INTERVAL '10' SECOND, INTERVAL '30' SECOND) AS w_end,\n",
    "        COUNT(*) AS total,\n",
    "        ROUND(AVG(rating), 2) AS avg\n",
    "    FROM rating_stream\n",
    "    GROUP BY HOP(event_time, INTERVAL '10' SECOND, INTERVAL '30' SECOND)\n",
    "\"\"\")\n",
    "with result_hop.collect() as results:\n",
    "    for i, row in enumerate(results):\n",
    "        if i >= 10:\n",
    "            break\n",
    "        print(row)\n",
    "\n",
    "# Flink SQL: Session window - NIEDOSTEPNE w Spark!\n",
    "print(\"\\n=== FLINK: Session Window (gap=20s) - unikalna funkcja Flink! ===\")\n",
    "result_session = t_env.execute_sql(\"\"\"\n",
    "    SELECT\n",
    "        SESSION_START(event_time, INTERVAL '20' SECOND) AS session_start,\n",
    "        SESSION_END(event_time, INTERVAL '20' SECOND) AS session_end,\n",
    "        user_id,\n",
    "        COUNT(*) AS ratings_in_session,\n",
    "        ROUND(AVG(rating), 2) AS avg_rating\n",
    "    FROM rating_stream\n",
    "    GROUP BY SESSION(event_time, INTERVAL '20' SECOND), user_id\n",
    "\"\"\")\n",
    "with result_session.collect() as results:\n",
    "    for i, row in enumerate(results):\n",
    "        if i >= 10:\n",
    "            break\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9000001",
   "metadata": {},
   "source": [
    "## 8. Benchmark: Latency - Flink vs Spark\n",
    "\n",
    "Kluczowa roznica miedzy Flink a Spark Streaming to **latencja**:\n",
    "- **Flink**: przetwarza zdarzenie natychmiast po przyjsciu (ms)\n",
    "- **Spark**: czeka na zebranie micro-batcha, nastepnie przetwarza (sekundy)\n",
    "\n",
    "Zmierzymy czas od wygenerowania zdarzenia do pojawienia sie wyniku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Benchmark latencji: Spark Structured Streaming ---\n",
    "\n",
    "import shutil\n",
    "# Wyczysc katalogi\n",
    "for f in os.listdir(STREAM_DIR):\n",
    "    os.remove(os.path.join(STREAM_DIR, f))\n",
    "for d in os.listdir(CHECKPOINT_DIR):\n",
    "    shutil.rmtree(os.path.join(CHECKPOINT_DIR, d), ignore_errors=True)\n",
    "\n",
    "spark_stream_bench = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(rating_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(STREAM_DIR) \\\n",
    "    .withColumn(\"event_ts\", to_timestamp(\"event_time\"))\n",
    "\n",
    "bench_query = spark_stream_bench \\\n",
    "    .groupBy(\"movie_id\") \\\n",
    "    .agg(count(\"*\").alias(\"cnt\"), round(avg(\"rating\"), 2).alias(\"avg\")) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"spark_bench\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/bench\") \\\n",
    "    .start()\n",
    "\n",
    "# Mierz czas od wstawienia danych do pojawienia sie wynikow\n",
    "spark_latencies = []\n",
    "for i in range(5):\n",
    "    t_start = time.time()\n",
    "    generate_rating_events(100 + i, n=50)\n",
    "    \n",
    "    # Czekaj na przetworzenie\n",
    "    prev_count = 0\n",
    "    while True:\n",
    "        time.sleep(0.1)\n",
    "        current = spark.sql(\"SELECT SUM(cnt) as total FROM spark_bench\").collect()[0][0]\n",
    "        if current and current > prev_count:\n",
    "            latency = time.time() - t_start\n",
    "            spark_latencies.append(latency)\n",
    "            prev_count = current\n",
    "            break\n",
    "        if time.time() - t_start > 30:\n",
    "            spark_latencies.append(30.0)\n",
    "            break\n",
    "\n",
    "bench_query.stop()\n",
    "\n",
    "print(\"=== Spark Structured Streaming - Latency ===\")\n",
    "print(f\"Latencje per batch: {[f'{l:.2f}s' for l in spark_latencies]}\")\n",
    "print(f\"Srednia latencja: {sum(spark_latencies)/len(spark_latencies):.2f}s\")\n",
    "print(f\"Min latencja: {min(spark_latencies):.2f}s\")\n",
    "print(f\"\\nSpark przetwarza w micro-batchach, wiec latencja = trigger interval + processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Benchmark latencji: PyFlink ---\n",
    "# Flink przetwarza event-by-event, wiec oczekujemy nizszej latencji\n",
    "\n",
    "# Mierzymy czas wykonania zapytania Flink SQL na tych samych danych\n",
    "flink_latencies = []\n",
    "\n",
    "for i in range(5):\n",
    "    t_start = time.time()\n",
    "    \n",
    "    result = t_env.execute_sql(\"\"\"\n",
    "        SELECT movie_id, COUNT(*) AS cnt, ROUND(AVG(rating), 2) AS avg_rating\n",
    "        FROM rating_stream\n",
    "        GROUP BY movie_id\n",
    "    \"\"\")\n",
    "    \n",
    "    # Odczytaj pierwszy wynik\n",
    "    with result.collect() as results:\n",
    "        first_row = next(iter(results), None)\n",
    "    \n",
    "    latency = time.time() - t_start\n",
    "    flink_latencies.append(latency)\n",
    "\n",
    "print(\"=== PyFlink - Latency ===\")\n",
    "print(f\"Latencje per run: {[f'{l:.2f}s' for l in flink_latencies]}\")\n",
    "print(f\"Srednia latencja: {sum(flink_latencies)/len(flink_latencies):.2f}s\")\n",
    "print(f\"Min latencja: {min(flink_latencies):.2f}s\")\n",
    "\n",
    "print(\"\\n=== Porownanie ===\")\n",
    "print(f\"Spark srednia: {sum(spark_latencies)/len(spark_latencies):.2f}s\")\n",
    "print(f\"Flink srednia: {sum(flink_latencies)/len(flink_latencies):.2f}s\")\n",
    "print(\"\\nUWAGA: W produkcyjnym klastrze roznica bylaby jeszcze wieksza.\")\n",
    "print(\"Flink osiaga latencje <100ms, Spark typowo 500ms-2s (micro-batch).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10000001",
   "metadata": {},
   "source": [
    "## 9. Kiedy wybrac Flink, a kiedy Spark Streaming?\n",
    "\n",
    "### Wybierz **Apache Flink** gdy:\n",
    "\n",
    "| Scenariusz | Dlaczego Flink? |\n",
    "|------------|----------------|\n",
    "| Ultra-niska latencja (<100ms) | Natywny streaming, event-by-event |\n",
    "| Skomplikowany stan (TB) | RocksDB backend, incremental checkpoints |\n",
    "| Session windows | Natywna obsluga, brak w Spark |\n",
    "| Complex Event Processing (CEP) | Wbudowana biblioteka Flink CEP |\n",
    "| Opoznione dane wymagaja obslugi | Side output dla late events |\n",
    "| Event time jest krytyczny | Domyslny tryb, zaawansowane watermarks |\n",
    "\n",
    "### Wybierz **Spark Structured Streaming** gdy:\n",
    "\n",
    "| Scenariusz | Dlaczego Spark? |\n",
    "|------------|----------------|\n",
    "| Juz uzywasz Spark do batch | Ten sam kod batch i streaming! |\n",
    "| Latencja ~1s jest akceptowalna | Micro-batch wystarczy |\n",
    "| Potrzebujesz ML w strumieniu | MLlib, integracja z pandas UDF |\n",
    "| Zespol zna Spark/SQL | Latwiejsze wdrozenie |\n",
    "| Laczenie batch + streaming | Unified API (Delta Lake, Iceberg) |\n",
    "| Ekosystem Hadoop | Lepsza integracja z HDFS, Hive, HBase |\n",
    "\n",
    "### Podsumowanie dla naszego systemu rekomendacji MovieLens:\n",
    "\n",
    "```\n",
    "Przypadek uzycia                    Lepszy wybor\n",
    "─────────────────────────────────────────────────\n",
    "Batch ETL (zaladuj ratings)          Spark\n",
    "Trening modelu ALS                   Spark (MLlib)\n",
    "Real-time trending movies            Flink (niska latencja)\n",
    "Sesje uzytkownikow                   Flink (session windows)\n",
    "Streaming + zapis do HDFS            Spark (natywna integracja)\n",
    "Alerty o anomaliach w ocenach        Flink (CEP)\n",
    "Dashboard z opoznieniem ~2s          Spark (prostsze)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podsumowanie porownania w formie tabeli\n",
    "import pandas as pd\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Cecha\": [\n",
    "        \"Model przetwarzania\", \"Typowa latencja\", \"Gwarancje dostarczenia\",\n",
    "        \"State backend\", \"Session windows\", \"Late data handling\",\n",
    "        \"Checkpointing\", \"API Python\", \"Batch processing\",\n",
    "        \"Ekosystem ML\", \"Dojrzalosc\", \"Krzywa uczenia\"\n",
    "    ],\n",
    "    \"Apache Flink\": [\n",
    "        \"Natywny streaming\", \"<100ms\", \"Exactly-once (Chandy-Lamport)\",\n",
    "        \"RocksDB / Heap (TB stanu)\", \"Natywne\", \"Side output (oddzielny strumien)\",\n",
    "        \"Asynchroniczne, inkrementalne\", \"PyFlink (Table + DataStream)\", \"Streaming jako special case\",\n",
    "        \"Ograniczony (FlinkML exp.)\", \"Dojrzaly (prod od 2016)\", \"Stroma\"\n",
    "    ],\n",
    "    \"Spark Structured Streaming\": [\n",
    "        \"Micro-batch (domyslnie)\", \"500ms - 2s\", \"Exactly-once (WAL + checkpoint)\",\n",
    "        \"HDFS State Store (ograniczony)\", \"Brak natywnych\", \"Drop (odrzucenie)\",\n",
    "        \"Synchroniczne, pelny snapshot\", \"PySpark (DataFrame + SQL)\", \"Natywny batch engine\",\n",
    "        \"MLlib, pandas UDF\", \"Bardzo dojrzaly\", \"Lagodna (jesli znasz Spark)\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Wyswietl bez obcinania\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 150)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11000001",
   "metadata": {},
   "source": [
    "## Zadanie koncowe\n",
    "\n",
    "Zaimplementuj **real-time anomaly detection** w strumieniu ocen filmow, uzywajac zarowno Spark Structured Streaming jak i PyFlink.\n",
    "\n",
    "### Scenariusz:\n",
    "System powinien wykrywac anomalie w ocenach - np. film nagle dostaje wiele niskich ocen (atak botow?) lub jeden uzytkownik ocenia setki filmow w krotkim czasie.\n",
    "\n",
    "### Wymagania:\n",
    "\n",
    "**Czesc 1 - Spark Structured Streaming:**\n",
    "1. Stworz strumien z file source (uzyj `generate_rating_events`)\n",
    "2. Tumbling window 30s z watermarkiem 1 min\n",
    "3. Wykryj filmy z >20 ocenami w oknie i srednia <2.0 (podejrzane)\n",
    "4. Wykryj uzytkownikow z >10 ocenami w oknie (podejrzana aktywnosc)\n",
    "5. Wyswietl alerty w konsoli\n",
    "\n",
    "**Czesc 2 - PyFlink:**\n",
    "1. Uzyj Table API z tym samym zrodlem danych\n",
    "2. Session window z gap 20s per uzytkownik (wykryj sesje)\n",
    "3. Wykryj sesje z >15 ocenami (anomalia)\n",
    "4. Porownaj wyniki z czescia Spark\n",
    "\n",
    "**Czesc 3 - Porownanie:**\n",
    "1. Zmierz latencje obu podejsc\n",
    "2. Ktore podejscie lepiej wykrywa anomalie? Dlaczego?\n",
    "3. Napisz krotkie podsumowanie (3-5 zdan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiazanie:\n",
    "\n",
    "# --- Czesc 1: Spark Structured Streaming ---\n",
    "\n",
    "\n",
    "# --- Czesc 2: PyFlink ---\n",
    "\n",
    "\n",
    "# --- Czesc 3: Porownanie ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree(STREAM_DIR, ignore_errors=True)\n",
    "shutil.rmtree(CHECKPOINT_DIR, ignore_errors=True)\n",
    "\n",
    "spark.stop()\n",
    "print(\"Sesje Spark i Flink zamkniete. Dane tymczasowe usuniete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
