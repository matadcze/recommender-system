{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 11 - Zaawansowane MLlib\n",
    "\n",
    "Zaawansowane techniki ML w Spark MLlib.\n",
    "\n",
    "**Tematy:**\n",
    "- Pipeline API - łańcuch transformacji i estymatorów\n",
    "- Content-based filtering z TF-IDF na gatunkach/tagach\n",
    "- Clustering użytkowników - K-Means na wektorach latentnych ALS\n",
    "- FPGrowth - frequent itemsets (\"ludzie którzy lubią X, lubią też Y\")\n",
    "- Hybrydowy model - łączenie ALS + content-based\n",
    "- Feature engineering z Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"11_Advanced_MLlib\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "properties = {\n",
    "    \"user\": \"recommender\",\n",
    "    \"password\": \"recommender\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "ratings = spark.read.jdbc(\n",
    "    jdbc_url, \"movielens.ratings\", properties=properties,\n",
    "    column=\"user_id\", lowerBound=1, upperBound=300000, numPartitions=10\n",
    ")\n",
    "movies = spark.read.jdbc(jdbc_url, \"movielens.movies\", properties=properties)\n",
    "\n",
    "ratings.cache()\n",
    "movies.cache()\n",
    "print(f\"Ratings: {ratings.count()}, Movies: {movies.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Pipeline API\n",
    "\n",
    "Pipeline łączy wiele kroków przetwarzania w jeden obiekt:\n",
    "- **Transformer** - przekształca DataFrame (np. VectorAssembler, StringIndexer)\n",
    "- **Estimator** - uczy się z danych i produkuje Transformer (np. ALS, KMeans)\n",
    "- **Pipeline** - sekwencja Transformers i Estimators\n",
    "\n",
    "```\n",
    "Pipeline: [StringIndexer → VectorAssembler → ALS → Evaluator]\n",
    "         fit(training)  →  PipelineModel\n",
    "         PipelineModel.transform(test)  →  predictions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Prosty pipeline ALS\n",
    "als = ALS(\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"movie_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[als])\n",
    "\n",
    "# ParamGrid na pipeline\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20]) \\\n",
    "    .addGrid(als.regParam, [0.1, 0.3]) \\\n",
    "    .addGrid(als.maxIter, [10]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# CrossValidator na pipeline\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Trenuj na próbce\n",
    "sample = ratings.sample(0.05, seed=42).cache()\n",
    "(train, test) = sample.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training: {train.count()}, Test: {test.count()}\")\n",
    "cv_model = cv.fit(train)\n",
    "\n",
    "# Ewaluuj\n",
    "predictions = cv_model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"\\nBest Pipeline RMSE: {rmse:.4f}\")\n",
    "print(f\"Best params: {cv_model.bestModel.stages[0].rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz cały pipeline model\n",
    "cv_model.bestModel.write().overwrite().save(\"/tmp/als_pipeline\")\n",
    "\n",
    "# Odczytaj\n",
    "from pyspark.ml import PipelineModel\n",
    "loaded_pipeline = PipelineModel.load(\"/tmp/als_pipeline\")\n",
    "loaded_predictions = loaded_pipeline.transform(test)\n",
    "print(f\"Loaded pipeline RMSE: {evaluator.evaluate(loaded_predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. Content-Based Filtering z TF-IDF\n",
    "\n",
    "Zamiast collaborative filtering (ALS) - rekomendacje na podstawie **cech filmów** (gatunki, tagi).\n",
    "\n",
    "1. Zamień gatunki na wektor TF-IDF\n",
    "2. Policz cosine similarity między filmami\n",
    "3. Rekomenduj filmy podobne do tych, które user lubi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Przygotuj \"dokument\" per film = lista gatunków\n",
    "movies_docs = movies \\\n",
    "    .withColumn(\"genres_list\", split(col(\"genres\"), \"\\\\|\")) \\\n",
    "    .withColumn(\"year\", regexp_extract(col(\"title\"), r\"\\((\\d{4})\\)\", 1).cast(\"int\"))\n",
    "\n",
    "movies_docs.select(\"movie_id\", \"title\", \"genres_list\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF pipeline na gatunkach\n",
    "# HashingTF: zamienia listę słów na sparse vector (term frequency)\n",
    "# IDF: ważenie inverse document frequency (rzadkie gatunki mają większą wagę)\n",
    "\n",
    "hashing_tf = HashingTF(inputCol=\"genres_list\", outputCol=\"raw_features\", numFeatures=30)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"tfidf_features\")\n",
    "\n",
    "tfidf_pipeline = Pipeline(stages=[hashing_tf, idf])\n",
    "tfidf_model = tfidf_pipeline.fit(movies_docs)\n",
    "\n",
    "movies_tfidf = tfidf_model.transform(movies_docs)\n",
    "movies_tfidf.select(\"movie_id\", \"title\", \"genres\", \"tfidf_features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Znajdź filmy podobne content-wise do Toy Story\n",
    "target_movie = 1  # Toy Story\n",
    "\n",
    "target_features = movies_tfidf.filter(col(\"movie_id\") == target_movie) \\\n",
    "    .select(\"tfidf_features\").collect()[0][0]\n",
    "target_np = target_features.toArray()\n",
    "\n",
    "@udf(DoubleType())\n",
    "def cosine_sim(features):\n",
    "    v = features.toArray()\n",
    "    denom = np.linalg.norm(target_np) * np.linalg.norm(v)\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(target_np, v) / denom)\n",
    "\n",
    "content_similar = movies_tfidf \\\n",
    "    .withColumn(\"similarity\", cosine_sim(col(\"tfidf_features\"))) \\\n",
    "    .filter(col(\"movie_id\") != target_movie) \\\n",
    "    .orderBy(desc(\"similarity\"))\n",
    "\n",
    "print(f\"Content-based: filmy podobne do Toy Story (gatunki):\")\n",
    "content_similar.select(\"title\", \"genres\", round(col(\"similarity\"), 4).alias(\"sim\")) \\\n",
    "    .show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000002",
   "metadata": {},
   "source": [
    "### Zadanie 1\n",
    "Stwórz content-based recommender, który dla danego użytkownika:\n",
    "1. Znajdzie jego top 5 filmów (najwyżej ocenione)\n",
    "2. Dla każdego z nich znajdzie 5 najbardziej podobnych content-wise\n",
    "3. Odfiltruje filmy, które user już widział\n",
    "4. Zwróci top 10 rekomendacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Clustering użytkowników z K-Means\n",
    "\n",
    "Użyjemy wektorów latentnych z ALS jako features i pogrupujemy użytkowników w klastry.\n",
    "\n",
    "Klastry = segmenty użytkowników o podobnym guście."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Najpierw wytrenuj ALS, żeby dostać user factors\n",
    "als = ALS(\n",
    "    maxIter=10, regParam=0.1, rank=20,\n",
    "    userCol=\"user_id\", itemCol=\"movie_id\", ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\", seed=42\n",
    ")\n",
    "\n",
    "als_model = als.fit(ratings)\n",
    "\n",
    "# User factors - wektory latentne użytkowników\n",
    "user_factors = als_model.userFactors \\\n",
    "    .withColumnRenamed(\"id\", \"user_id\")\n",
    "\n",
    "print(f\"User factors: {user_factors.count()} users × rank=20\")\n",
    "user_factors.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method - znajdź optymalną liczbę klastrów\n",
    "silhouette_scores = []\n",
    "cost_scores = []\n",
    "K_values = [3, 5, 8, 10, 15, 20]\n",
    "\n",
    "evaluator_cluster = ClusteringEvaluator(\n",
    "    predictionCol=\"cluster\", featuresCol=\"features\", metricName=\"silhouette\"\n",
    ")\n",
    "\n",
    "for k in K_values:\n",
    "    kmeans = KMeans(k=k, featuresCol=\"features\", predictionCol=\"cluster\", seed=42)\n",
    "    model = kmeans.fit(user_factors)\n",
    "    predictions = model.transform(user_factors)\n",
    "    \n",
    "    silhouette = evaluator_cluster.evaluate(predictions)\n",
    "    cost = model.summary.trainingCost\n",
    "    \n",
    "    silhouette_scores.append(silhouette)\n",
    "    cost_scores.append(cost)\n",
    "    print(f\"K={k:2d}: silhouette={silhouette:.4f}, cost={cost:.0f}\")\n",
    "\n",
    "# Najlepszy K wg silhouette\n",
    "best_k = K_values[silhouette_scores.index(max(silhouette_scores))]\n",
    "print(f\"\\nNajlepszy K wg silhouette: {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trenuj finalny model\n",
    "kmeans_final = KMeans(k=best_k, featuresCol=\"features\", predictionCol=\"cluster\", seed=42)\n",
    "kmeans_model = kmeans_final.fit(user_factors)\n",
    "user_clusters = kmeans_model.transform(user_factors)\n",
    "\n",
    "# Rozmiary klastrów\n",
    "print(\"Rozmiary klastrów:\")\n",
    "user_clusters.groupBy(\"cluster\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"cluster\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profil każdego klastra - jakie gatunki preferują?\n",
    "# Join z ratings i movies\n",
    "cluster_profiles = user_clusters.select(\"user_id\", \"cluster\") \\\n",
    "    .join(ratings.select(\"user_id\", \"movie_id\", \"rating\"), \"user_id\") \\\n",
    "    .join(movies, \"movie_id\")\n",
    "\n",
    "# Rozbij gatunki\n",
    "cluster_genres = cluster_profiles \\\n",
    "    .withColumn(\"genre\", explode(split(col(\"genres\"), \"\\\\|\"))) \\\n",
    "    .groupBy(\"cluster\", \"genre\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_ratings\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"avg_rating\")\n",
    "    )\n",
    "\n",
    "# Top 3 gatunki per klaster\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy(\"cluster\").orderBy(desc(\"num_ratings\"))\n",
    "\n",
    "top_genres = cluster_genres \\\n",
    "    .withColumn(\"rank\", row_number().over(w)) \\\n",
    "    .filter(col(\"rank\") <= 3)\n",
    "\n",
    "print(\"Top 3 gatunki per klaster:\")\n",
    "top_genres.orderBy(\"cluster\", \"rank\").show(best_k * 3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Średnia ocena per klaster\n",
    "cluster_stats = cluster_profiles \\\n",
    "    .groupBy(\"cluster\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"user_id\").alias(\"num_users\"),\n",
    "        round(avg(\"rating\"), 2).alias(\"avg_rating\"),\n",
    "        round(stddev(\"rating\"), 2).alias(\"std_rating\"),\n",
    "        round(avg(when(col(\"rating\") >= 4.0, 1).otherwise(0)), 2).alias(\"pct_positive\")\n",
    "    ) \\\n",
    "    .orderBy(\"cluster\")\n",
    "\n",
    "print(\"Statystyki klastrów:\")\n",
    "cluster_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. FPGrowth - Frequent Itemsets\n",
    "\n",
    "\"Użytkownicy, którzy lubią film X, lubią też film Y.\"\n",
    "\n",
    "FPGrowth znajduje **częste wzorce** w transakcjach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "# Przygotuj \"koszyki\" - filmy wysoko ocenione przez każdego użytkownika\n",
    "# (traktujemy jak zakupy: user \"kupił\" filmy które ocenił >= 4.0)\n",
    "\n",
    "# Pracujemy na próbce - FPGrowth jest kosztowne\n",
    "user_baskets = ratings \\\n",
    "    .filter(col(\"rating\") >= 4.0) \\\n",
    "    .filter(col(\"user_id\") <= 10000) \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(collect_set(\"movie_id\").alias(\"items\")) \\\n",
    "    .filter(size(col(\"items\")) >= 5)  # min 5 filmów w koszyku\n",
    "\n",
    "print(f\"Koszyki: {user_baskets.count()} użytkowników\")\n",
    "user_baskets.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPGrowth\n",
    "fp = FPGrowth(\n",
    "    itemsCol=\"items\",\n",
    "    minSupport=0.05,     # item musi pojawić się w min 5% koszyków\n",
    "    minConfidence=0.3    # reguła musi mieć min 30% confidence\n",
    ")\n",
    "\n",
    "fp_model = fp.fit(user_baskets)\n",
    "\n",
    "# Frequent itemsets - najczęstsze zestawy filmów\n",
    "print(f\"Frequent itemsets: {fp_model.freqItemsets.count()}\")\n",
    "fp_model.freqItemsets \\\n",
    "    .orderBy(desc(\"freq\")) \\\n",
    "    .show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association rules - reguły \"jeśli X to Y\"\n",
    "rules = fp_model.associationRules\n",
    "print(f\"Association rules: {rules.count()}\")\n",
    "\n",
    "# Zamień movie_id na tytuły dla czytelności\n",
    "rules_top = rules.orderBy(desc(\"confidence\")).limit(30).collect()\n",
    "\n",
    "# Cache movies jako dict\n",
    "movie_names = {r.movie_id: r.title for r in movies.collect()}\n",
    "\n",
    "print(\"\\nTop reguły asocjacyjne:\")\n",
    "print(f\"{'Antecedent':<60} → {'Consequent':<40} conf={'':<6} lift\")\n",
    "print(\"-\" * 130)\n",
    "for rule in rules_top:\n",
    "    ant = \", \".join([movie_names.get(mid, str(mid))[:25] for mid in rule.antecedent])\n",
    "    con = \", \".join([movie_names.get(mid, str(mid))[:25] for mid in rule.consequent])\n",
    "    print(f\"{ant:<60} → {con:<40} {rule.confidence:.2f}   {rule.lift:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Użyj reguł do rekomendacji\n",
    "# Dla usera: jakie filmy mu rekomendujemy na podstawie jego historii?\n",
    "\n",
    "user_id = 42\n",
    "user_movies = ratings.filter(\n",
    "    (col(\"user_id\") == user_id) & (col(\"rating\") >= 4.0)\n",
    ").select(\"movie_id\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "user_movies_set = set(user_movies)\n",
    "print(f\"User {user_id} lubi {len(user_movies_set)} filmów\")\n",
    "\n",
    "# Znajdź reguły, których antecedent jest podzbiorem filmów usera\n",
    "@udf(\"boolean\")\n",
    "def is_subset(antecedent):\n",
    "    return all(mid in user_movies_set for mid in antecedent)\n",
    "\n",
    "@udf(\"boolean\")\n",
    "def not_seen(consequent):\n",
    "    return all(mid not in user_movies_set for mid in consequent)\n",
    "\n",
    "recommendations = rules \\\n",
    "    .filter(is_subset(col(\"antecedent\"))) \\\n",
    "    .filter(not_seen(col(\"consequent\"))) \\\n",
    "    .orderBy(desc(\"confidence\"))\n",
    "\n",
    "print(f\"\\nRekomendacje FPGrowth dla user {user_id}:\")\n",
    "recs = recommendations.limit(10).collect()\n",
    "for r in recs:\n",
    "    ant = \", \".join([movie_names.get(m, str(m))[:30] for m in r.antecedent])\n",
    "    con = \", \".join([movie_names.get(m, str(m))[:30] for m in r.consequent])\n",
    "    print(f\"  Bo lubisz [{ant}] → polecamy [{con}] (conf={r.confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Hybrydowy model - ALS + Content-Based\n",
    "\n",
    "Łączymy wyniki ALS (collaborative) i TF-IDF (content-based) w jeden ranking.\n",
    "\n",
    "**Strategia: weighted hybrid**\n",
    "- `final_score = α * als_score + (1 - α) * content_score`\n",
    "- α to waga ALS (np. 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS rekomendacje dla usera 42\n",
    "user_42_df = spark.createDataFrame([(42,)], [\"user_id\"])\n",
    "als_recs = als_model.recommendForUserSubset(user_42_df, 50)\n",
    "\n",
    "als_scores = als_recs.select(\n",
    "    explode(\"recommendations\").alias(\"rec\")\n",
    ").select(\n",
    "    col(\"rec.movie_id\"),\n",
    "    col(\"rec.rating\").alias(\"als_score\")\n",
    ")\n",
    "\n",
    "# Znormalizuj ALS scores do [0, 1]\n",
    "als_min = als_scores.agg(min(\"als_score\")).collect()[0][0]\n",
    "als_max = als_scores.agg(max(\"als_score\")).collect()[0][0]\n",
    "\n",
    "als_normalized = als_scores.withColumn(\n",
    "    \"als_norm\",\n",
    "    (col(\"als_score\") - als_min) / (als_max - als_min)\n",
    ")\n",
    "\n",
    "print(\"ALS recommendations (normalized):\")\n",
    "als_normalized.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-based scores: similarity do ulubionych filmów usera\n",
    "# Weź top 5 filmów usera, policz średnią similarity do wszystkich filmów\n",
    "\n",
    "user_top_movies = ratings.filter(\n",
    "    (col(\"user_id\") == 42) & (col(\"rating\") >= 4.0)\n",
    ").join(movies_tfidf, \"movie_id\").select(\"movie_id\", \"tfidf_features\").collect()\n",
    "\n",
    "# Średni wektor TF-IDF ulubionych filmów = \"profil użytkownika\"\n",
    "user_profile = np.mean([r.tfidf_features.toArray() for r in user_top_movies], axis=0)\n",
    "user_profile_norm = np.linalg.norm(user_profile)\n",
    "\n",
    "@udf(DoubleType())\n",
    "def content_score(features):\n",
    "    v = features.toArray()\n",
    "    denom = user_profile_norm * np.linalg.norm(v)\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(user_profile, v) / denom)\n",
    "\n",
    "content_scores = movies_tfidf \\\n",
    "    .withColumn(\"content_score\", content_score(col(\"tfidf_features\"))) \\\n",
    "    .select(\"movie_id\", \"content_score\")\n",
    "\n",
    "# Znormalizuj do [0, 1]\n",
    "cs_min = content_scores.agg(min(\"content_score\")).collect()[0][0]\n",
    "cs_max = content_scores.agg(max(\"content_score\")).collect()[0][0]\n",
    "\n",
    "content_normalized = content_scores.withColumn(\n",
    "    \"content_norm\",\n",
    "    (col(\"content_score\") - cs_min) / (cs_max - cs_min)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid: połącz ALS i content-based\n",
    "ALPHA = 0.7  # waga ALS\n",
    "\n",
    "hybrid = als_normalized.join(content_normalized, \"movie_id\") \\\n",
    "    .withColumn(\n",
    "        \"hybrid_score\",\n",
    "        ALPHA * col(\"als_norm\") + (1 - ALPHA) * col(\"content_norm\")\n",
    "    ) \\\n",
    "    .join(movies, \"movie_id\") \\\n",
    "    .filter(~col(\"movie_id\").isin(user_movies)) \\\n",
    "    .orderBy(desc(\"hybrid_score\"))\n",
    "\n",
    "print(\"Hybrid recommendations (ALS + Content-Based):\")\n",
    "hybrid.select(\n",
    "    \"title\", \"genres\",\n",
    "    round(col(\"als_norm\"), 3).alias(\"als\"),\n",
    "    round(col(\"content_norm\"), 3).alias(\"content\"),\n",
    "    round(col(\"hybrid_score\"), 3).alias(\"hybrid\")\n",
    ").show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000002",
   "metadata": {},
   "source": [
    "### Zadanie 2\n",
    "Porównaj jakość trzech podejść (ALS, content-based, hybrid) na zbiorze testowym:\n",
    "1. Dla 100 losowych użytkowników, generuj top-10 rekomendacji z każdego podejścia\n",
    "2. Policz Precision@10 dla każdego (ground truth = filmy ocenione >= 4.0 w zbiorze testowym)\n",
    "3. Który approach daje najlepsze wyniki?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering Pipeline\n",
    "\n",
    "Budowa features, które mogą zasilać dowolny model ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "\n",
    "# Movie features\n",
    "movie_features = movies \\\n",
    "    .withColumn(\"year\", regexp_extract(col(\"title\"), r\"\\((\\d{4})\\)\", 1).cast(\"int\")) \\\n",
    "    .withColumn(\"num_genres\", size(split(col(\"genres\"), \"\\\\|\"))) \\\n",
    "    .withColumn(\"is_comedy\", col(\"genres\").contains(\"Comedy\").cast(\"int\")) \\\n",
    "    .withColumn(\"is_drama\", col(\"genres\").contains(\"Drama\").cast(\"int\")) \\\n",
    "    .withColumn(\"is_action\", col(\"genres\").contains(\"Action\").cast(\"int\")) \\\n",
    "    .withColumn(\"is_horror\", col(\"genres\").contains(\"Horror\").cast(\"int\")) \\\n",
    "    .withColumn(\"is_scifi\", col(\"genres\").contains(\"Sci-Fi\").cast(\"int\")) \\\n",
    "    .withColumn(\"title_length\", length(col(\"title\")))\n",
    "\n",
    "# Agregaty z ratings\n",
    "movie_agg = ratings.groupBy(\"movie_id\").agg(\n",
    "    count(\"*\").alias(\"num_ratings\"),\n",
    "    round(avg(\"rating\"), 2).alias(\"avg_rating\"),\n",
    "    round(stddev(\"rating\"), 2).alias(\"std_rating\"),\n",
    "    countDistinct(\"user_id\").alias(\"unique_raters\")\n",
    ")\n",
    "\n",
    "# Połącz\n",
    "movie_full = movie_features.join(movie_agg, \"movie_id\", \"left\").fillna(0)\n",
    "\n",
    "# VectorAssembler - zbierz features w jeden wektor\n",
    "feature_cols = [\n",
    "    \"year\", \"num_genres\", \"is_comedy\", \"is_drama\", \"is_action\",\n",
    "    \"is_horror\", \"is_scifi\", \"title_length\", \"num_ratings\",\n",
    "    \"avg_rating\", \"std_rating\", \"unique_raters\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"raw_features\")\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "feature_pipeline = Pipeline(stages=[assembler, scaler])\n",
    "feature_model = feature_pipeline.fit(movie_full)\n",
    "movies_featured = feature_model.transform(movie_full)\n",
    "\n",
    "movies_featured.select(\"title\", \"scaled_features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## Zadanie końcowe\n",
    "\n",
    "Zbuduj kompletny **Recommendation Engine** z:\n",
    "\n",
    "1. **ALS collaborative filtering** - wytrenuj z optymalnymi parametrami\n",
    "2. **Content-based** - TF-IDF na gatunkach\n",
    "3. **FPGrowth rules** - reguły asocjacyjne\n",
    "4. **Hybrid scorer** - łączony ranking:\n",
    "   - ALS score (waga 0.5)\n",
    "   - Content similarity (waga 0.3)  \n",
    "   - FPGrowth confidence boost (waga 0.2)\n",
    "\n",
    "5. Dla 5 wybranych użytkowników pokaż:\n",
    "   - Ich historię (co lubią)\n",
    "   - Top 10 hybrydowych rekomendacji\n",
    "   - Porównanie z czystym ALS\n",
    "\n",
    "6. Zapisz pipeline i modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiązanie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.unpersist()\n",
    "movies.unpersist()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
