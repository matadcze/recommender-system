{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd4b5b4-7ffe-4a2d-a707-8b601a4c488e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/site-packages (4.1.1)\n",
      "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in /usr/local/lib/python3.12/site-packages (from pyspark) (0.10.9.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark numpy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8a891ef-36d1-48ea-bd4a-8773ad23a5be",
   "metadata": {},
   "source": [
    "wget https://jdbc.postgresql.org/download/postgresql-42.7.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4cd70ec-f2b2-498c-92c0-f133340f7fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/app/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /root/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /root/.ivy2.5.2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a2e4e618-82ea-4514-8445-d95e541fdbc2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.7.1 in central\n",
      "\tfound org.checkerframework#checker-qual;3.41.0 in central\n",
      ":: resolution report :: resolve 58ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.41.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a2e4e618-82ea-4514-8445-d95e541fdbc2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n",
      "26/01/29 05:44:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    " .appName(\"MovieRecommender\") \\\n",
    " .master(\"spark://spark-master:7077\") \\\n",
    " .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.1\") \\\n",
    " .config(\"spark.driver.memory\", \"2g\") \\\n",
    " .config(\"spark.executor.memory\", \"3g\") \\\n",
    " .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    " .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    " .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "properties = {\n",
    "    \"user\": \"recommender\",\n",
    "    \"password\": \"recommender\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7721c7f0-c875-4cd5-9afe-3f0d10dc5e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-------------------+\n",
      "|user_id|movie_id|rating|   rating_timestamp|\n",
      "+-------+--------+------+-------------------+\n",
      "|    441|    5418|   3.5|2005-02-15 23:53:58|\n",
      "|    441|    5464|   3.5|2005-07-05 19:14:13|\n",
      "|    441|    5989|   3.5|2005-01-27 20:06:56|\n",
      "|    441|    5995|   4.5|2005-01-27 19:56:22|\n",
      "|    441|    6539|   2.5|2005-01-27 20:00:11|\n",
      "+-------+--------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "+--------+--------------------+--------------------+\n",
      "|movie_id|               title|              genres|\n",
      "+--------+--------------------+--------------------+\n",
      "|       1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|       2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|       3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|       4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|       5|Father of the Bri...|              Comedy|\n",
      "|       6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|       7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|       8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|       9| Sudden Death (1995)|              Action|\n",
      "|      10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Load tables\n",
    "ratings = spark.read.jdbc(\n",
    "    jdbc_url, \"movielens.ratings\", properties=properties,\n",
    "    column=\"user_id\", lowerBound=1, upperBound=300000, numPartitions=10\n",
    ")\n",
    "movies = spark.read.jdbc(jdbc_url, \"movielens.movies\", properties=properties)\n",
    "\n",
    "ratings.show(5)\n",
    "movies.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47687bd-1fa3-4fb0-ad39-085d7924c3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:>                                                        (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8081274264270969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Prepare data\n",
    "(training, test) = ratings.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Build ALS model\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"movie_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\") # jak interpretować wartość"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6dd384-7c68-4633-ad31-523c9ba3d5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user_id|recommendations                                                                                                                                                                                              |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1      |[{120821, 5.371759}, {121029, 5.2287755}, {102119, 5.0840597}, {107623, 5.048971}, {109887, 5.0230913}, {98328, 5.0230913}, {26793, 4.984653}, {116899, 4.969732}, {86401, 4.9678717}, {96631, 4.948954}]    |\n",
      "|3      |[{121029, 5.8692026}, {120821, 5.8110275}, {116899, 5.5893846}, {109887, 5.568152}, {98328, 5.568152}, {107623, 5.563184}, {102119, 5.5622487}, {112423, 5.5323677}, {101538, 5.4985304}, {103593, 5.473386}]|\n",
      "|5      |[{121029, 6.12786}, {77736, 6.0321593}, {120821, 5.9850607}, {85205, 5.6557817}, {110173, 5.4642415}, {102602, 5.454567}, {129243, 5.4524016}, {111755, 5.4392066}, {56869, 5.3866215}, {86237, 5.3817854}]  |\n",
      "|6      |[{3226, 5.5374923}, {110588, 5.4535384}, {108713, 5.3816667}, {56869, 5.309029}, {130347, 5.2265925}, {68273, 5.1615653}, {112423, 5.1504593}, {117849, 5.0811796}, {121029, 5.0747066}, {116899, 5.069838}] |\n",
      "|9      |[{112577, 4.7005177}, {62054, 4.6232142}, {110138, 4.5629187}, {77736, 4.4956837}, {121029, 4.370908}, {81357, 4.289655}, {101538, 4.210347}, {89371, 4.2098913}, {107623, 4.1716156}, {56869, 4.141973}]    |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+--------------------------------------------------------------+--------------------+\n",
      "|movie_id|user_id|rating   |title                                                         |genres              |\n",
      "+--------+-------+---------+--------------------------------------------------------------+--------------------+\n",
      "|121029  |42     |5.4180927|No Distance Left to Run (2010)                                |Documentary         |\n",
      "|77736   |42     |5.376377 |Crazy Stone (Fengkuang de shitou) (2006)                      |Comedy|Crime        |\n",
      "|56869   |42     |5.2917137|Drained (O cheiro do Ralo) (2006)                             |Comedy              |\n",
      "|110173  |42     |5.182763 |Wolf (2013)                                                   |Crime|Drama|Thriller|\n",
      "|130347  |42     |5.0884967|Bill Hicks: Sane Man (1989)                                   |Comedy              |\n",
      "|108713  |42     |5.070996 |Tomorrow Night (1998)                                         |Comedy|Drama        |\n",
      "|101717  |42     |5.017169 |Elusive Summer of '68, The (Varljivo leto '68) (1984)         |Comedy|Drama|Romance|\n",
      "|107623  |42     |5.016391 |2013 Rock and Roll Hall of Fame Induction Ceremony, The (2013)|Documentary|Musical |\n",
      "|72235   |42     |5.001583 |Between the Devil and the Deep Blue Sea (1995)                |Drama               |\n",
      "|112423  |42     |4.964688 |I Belong (Som du ser meg) (2012)                              |Drama               |\n",
      "+--------+-------+---------+--------------------------------------------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top 10 recommendations for each user\n",
    "user_recs = model.recommendForAllUsers(10)\n",
    "user_recs.show(5, truncate=False)\n",
    "\n",
    "# jakie filmy ogladal wczesniej a jakie dostal rekomendacje\n",
    "\n",
    "# Top 10 recommendations for a specific user\n",
    "user_42_recs = model.recommendForUserSubset(\n",
    "    spark.createDataFrame([(42,)], [\"user_id\"]), 10\n",
    ")\n",
    "\n",
    "# Join with movie titles\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "user_42_recs_flat = user_42_recs.select(\n",
    "    \"user_id\",\n",
    "    explode(\"recommendations\").alias(\"rec\")\n",
    ").select(\"user_id\", \"rec.movie_id\", \"rec.rating\")\n",
    "\n",
    "user_42_recs_flat.join(movies, \"movie_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86148295-bb16-435e-b03e-2e82529fda8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movie_id: integer (nullable = false)\n",
      " |-- recommendations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- user_id: integer (nullable = true)\n",
      " |    |    |-- rating: float (nullable = true)\n",
      "\n",
      "+--------+-------+---------+\n",
      "|movie_id|user_id|    score|\n",
      "+--------+-------+---------+\n",
      "|       1|  27735|  5.61217|\n",
      "|       1|  99259|  5.54677|\n",
      "|       1| 108993| 5.512836|\n",
      "|       1|  76693| 5.466339|\n",
      "|       1|  53212|5.4587703|\n",
      "|       1| 129211| 5.441646|\n",
      "|       1|  30542| 5.427482|\n",
      "|       1| 119513|5.3881965|\n",
      "|       1|  61498| 5.371231|\n",
      "|       1| 110387| 5.367866|\n",
      "+--------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "movie_recs = model.recommendForItemSubset(\n",
    "    spark.createDataFrame([(1,)], [\"movie_id\"]), 10\n",
    ")\n",
    "\n",
    "# Check the schema\n",
    "movie_recs.printSchema()\n",
    "\n",
    "# Explode correctly - it's user_id, not movie_id\n",
    "movie_recs_flat = movie_recs.select(\n",
    "    col(\"movie_id\"),\n",
    "    explode(\"recommendations\").alias(\"rec\")\n",
    ").select(\n",
    "    \"movie_id\",\n",
    "    col(\"rec.user_id\").alias(\"user_id\"),\n",
    "    col(\"rec.rating\").alias(\"score\")\n",
    ")\n",
    "\n",
    "movie_recs_flat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41a8d8d1-1f5d-434d-9fea-2a32c0328b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (2.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf16eea6-f9b4-4058-b5a0-2b4096eb1e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/29 05:45:41 WARN TaskSetManager: Lost task 0.0 in stage 384.0 (TID 517) (172.18.0.5 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3369, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3276, in read_udfs\n",
      "    read_single_udf(\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1305, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 64, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 473, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 469, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:799)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:42)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$12(limit.scala:361)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "26/01/29 05:45:41 ERROR TaskSetManager: Task 2 in stage 384.0 failed 4 times; aborting job\n",
      "26/01/29 05:45:41 WARN TaskSetManager: Lost task 3.2 in stage 384.0 (TID 527) (172.18.0.5 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 384.0 failed 4 times, most recent failure: Lost task 2.3 in stage 384.0 (TID 526) (172.18.0.5 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3369, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3276, in read_udfs\n",
      "    read_single_udf(\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1305, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 64, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 473, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 469, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:645)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:123)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:106)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:596)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:611)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:593)\n",
      "\tat scala.collection.convert.JavaCollectionWrappers$IteratorWrapper.hasNext(JavaCollectionWrappers.scala:32)\n",
      "\tat org.sparkproject.guava.collect.Ordering.leastOf(Ordering.java:799)\n",
      "\tat org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:42)\n",
      "\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$doExecute$12(limit.scala:361)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3369, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3276, in read_udfs\n    read_single_udf(\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1305, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 64, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 473, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 469, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPythonException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     22\u001b[39m similar_movies = item_factors \\\n\u001b[32m     23\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m, cosine_udf(col(\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m))) \\\n\u001b[32m     24\u001b[39m     .filter(col(\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m) != \u001b[32m1\u001b[39m) \\\n\u001b[32m     25\u001b[39m     .orderBy(col(\u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m).desc()) \\\n\u001b[32m     26\u001b[39m     .limit(\u001b[32m10\u001b[39m) \\\n\u001b[32m     27\u001b[39m     .withColumnRenamed(\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Join with movie titles\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[43msimilar_movies\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmovie_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmovie_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimilarity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:316\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    309\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    310\u001b[39m         messageParameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m         },\n\u001b[32m    314\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/app/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/pyspark.zip/pyspark/worker.py:3369\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3367\u001b[39m \n\u001b[32m   3368\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m3369\u001b[39m func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/pyspark.zip/pyspark/worker.py:3276\u001b[39m, in \u001b[36mread_udfs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3274\u001b[39m \n\u001b[32m   3275\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m3276\u001b[39m read_single_udf(\n\u001b[32m   3277\u001b[39m \n\u001b[32m   3278\u001b[39m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/pyspark.zip/pyspark/worker.py:1305\u001b[39m, in \u001b[36mread_single_udf\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1303\u001b[39m \n\u001b[32m   1304\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1305\u001b[39m f, return_type = read_command(pickleSer, infile)\n\u001b[32m   1306\u001b[39m \n\u001b[32m   1307\u001b[39m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py:64\u001b[39m, in \u001b[36mread_command\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m command = serializer._read_with_length(file)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py:173\u001b[39m, in \u001b[36m_read_with_length\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    171\u001b[39m \n\u001b[32m    172\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m return self.loads(obj)\n\u001b[32m    174\u001b[39m \n\u001b[32m    175\u001b[39m \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py:473\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    471\u001b[39m \n\u001b[32m    472\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m return cloudpickle.loads(obj, encoding=encoding)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py:469\u001b[39m, in \u001b[36msubimport\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m __import__(name)\n",
      "\u001b[31mPythonException\u001b[39m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3369, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 3276, in read_udfs\n    read_single_udf(\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1305, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker_util.py\", line 64, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 473, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 469, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'numpy'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "\n",
    "# Get item factors\n",
    "item_factors = model.itemFactors\n",
    "\n",
    "# Get Toy Story's factor vector\n",
    "toy_story_factor = item_factors.filter(col(\"id\") == 1).select(\"features\").collect()[0][0]\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def cosine_sim(v):\n",
    "    a = np.array(toy_story_factor)\n",
    "    b = np.array(v)\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "cosine_udf = udf(cosine_sim, DoubleType())\n",
    "\n",
    "# Find similar movies\n",
    "similar_movies = item_factors \\\n",
    "    .withColumn(\"similarity\", cosine_udf(col(\"features\"))) \\\n",
    "    .filter(col(\"id\") != 1) \\\n",
    "    .orderBy(col(\"similarity\").desc()) \\\n",
    "    .limit(10) \\\n",
    "    .withColumnRenamed(\"id\", \"movie_id\")\n",
    "\n",
    "# Join with movie titles\n",
    "similar_movies.join(movies, \"movie_id\") \\\n",
    "    .select(\"movie_id\", \"title\", \"similarity\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606055c5-74d2-4c68-9a57-db122301cba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
