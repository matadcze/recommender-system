{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 18 - HBase jako Serving Layer dla rekomendacji\n",
    "\n",
    "Wykorzystanie HBase jako warstwy serwującej rekomendacje w systemie produkcyjnym.\n",
    "\n",
    "**Tematy:**\n",
    "- Architektura serving layer z HBase\n",
    "- Row key design: user_id + CF recs + rank columns\n",
    "- ALS training i eksport rekomendacji do HBase\n",
    "- Bulk load vs individual puts\n",
    "- TTL i automatyczne wygasanie rekomendacji\n",
    "- Wersjonowanie rekomendacji z HBase timestamps\n",
    "- Filtrowanie z HBase Scan filters\n",
    "- Tabela podobnych filmów w HBase\n",
    "- Pełny pipeline: train, export, serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Architektura Serving Layer\n",
    "\n",
    "W systemie rekomendacji rozdzielamy **offline training** od **online serving**:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    OFFLINE PIPELINE (batch)                     │\n",
    "│                                                                 │\n",
    "│  PostgreSQL ──► Spark ──► ALS Training ──► Top-K Recs          │\n",
    "│  (ratings)      (ETL)     (MLlib)          (per user)          │\n",
    "│                                                │                │\n",
    "│                                    Bulk Load   │                │\n",
    "│                                                ▼                │\n",
    "│                                    ┌───────────────────┐       │\n",
    "│                                    │      HBase        │       │\n",
    "│                                    │                   │       │\n",
    "│                                    │ user_recs table   │       │\n",
    "│                                    │ movie_sim table   │       │\n",
    "│                                    └─────────┬─────────┘       │\n",
    "└──────────────────────────────────────────────┼─────────────────┘\n",
    "                                               │\n",
    "┌──────────────────────────────────────────────┼─────────────────┐\n",
    "│                    ONLINE SERVING (real-time)  │                │\n",
    "│                                               │                │\n",
    "│  User Request ──► FastAPI ──► HBase GET ──────┘                │\n",
    "│  (GET /recs/42)   (API)      (< 10ms!)                        │\n",
    "│                      │                                         │\n",
    "│                      ▼                                         │\n",
    "│              JSON Response                                     │\n",
    "│              {recs: [...]}                                     │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Dlaczego HBase jako serving layer?\n",
    "\n",
    "| Cecha | HBase | PostgreSQL | Redis |\n",
    "|-------|-------|------------|-------|\n",
    "| Latencja GET | ~1-5ms | ~5-20ms | ~0.5ms |\n",
    "| Skalowalność | Horyzontalna | Wertykalna | Ograniczona RAM |\n",
    "| Dane na dysku | Tak (HDFS) | Tak | Opcjonalnie |\n",
    "| TTL per cell | Tak | Nie (trigger) | Tak |\n",
    "| Wersjonowanie | Wbudowane | Ręczne | Nie |\n",
    "| Koszt 1TB | Niski (HDFS) | Średni | Wysoki (RAM!) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import json\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"18_HBase_Serving_Layer\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.postgresql:postgresql:42.7.1,\"\n",
    "            \"org.apache.hbase.connectors.spark:hbase-spark:1.0.1,\"\n",
    "            \"org.apache.hbase:hbase-client:2.6.1,\"\n",
    "            \"org.apache.hbase:hbase-common:2.6.1,\"\n",
    "            \"org.apache.hbase:hbase-mapreduce:2.6.1\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.host\", \"recommender-jupyter\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.hadoop.hbase.zookeeper.quorum\", \"hbase-zookeeper\") \\\n",
    "    .config(\"spark.hadoop.hbase.zookeeper.property.clientPort\", \"2181\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/recommender\"\n",
    "jdbc_props = {\"user\": \"recommender\", \"password\": \"recommender\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zaladuj dane\n",
    "ratings = spark.read.jdbc(\n",
    "    jdbc_url, \"movielens.ratings\", properties=jdbc_props,\n",
    "    column=\"user_id\", lowerBound=1, upperBound=300000, numPartitions=10\n",
    ")\n",
    "movies = spark.read.jdbc(jdbc_url, \"movielens.movies\", properties=jdbc_props)\n",
    "\n",
    "ratings.cache()\n",
    "movies.cache()\n",
    "print(f\"Ratings: {ratings.count()}, Movies: {movies.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. Row Key Design dla rekomendacji\n",
    "\n",
    "```\n",
    "Tabela: user_recommendations\n",
    "┌──────────────┬────────────────────────────────────────────────────────┐\n",
    "│ Row Key      │ Column Family: recs                                   │\n",
    "│ (user_id)    ├───────────┬───────────┬───────────┬─────┬────────────┤\n",
    "│              │ recs:r_01 │ recs:r_02 │ recs:r_03 │ ... │ recs:r_20  │\n",
    "├──────────────┼───────────┼───────────┼───────────┼─────┼────────────┤\n",
    "│ 000042       │ 318       │ 858       │ 527       │ ... │ 4993       │\n",
    "│              │ @t2 3.95  │ @t2 3.89  │ @t2 3.85  │     │ @t2 3.21   │\n",
    "│              │ @t1 296   │ @t1 1196  │ @t1 260   │     │ @t1 2571   │\n",
    "├──────────────┼───────────┼───────────┼───────────┼─────┼────────────┤\n",
    "│ 000100       │ 2571      │ 1        │ 4993      │ ... │ 110        │\n",
    "└──────────────┴───────────┴───────────┴───────────┴─────┴────────────┘\n",
    "\n",
    "Kazdy rank column przechowuje movie_id.\n",
    "@t2 = nowsza wersja rekomendacji (po retrenowaniu modelu)\n",
    "@t1 = starsza wersja (automatycznie dostepna dzieki wersjonowaniu HBase)\n",
    "```\n",
    "\n",
    "### Zalety tego designu:\n",
    "- **Jeden GET** pobiera wszystkie 20 rekomendacji dla usera\n",
    "- **Wersjonowanie** daje historię zmian rekomendacji\n",
    "- **TTL** automatycznie usuwa przestarzałe rekomendacje\n",
    "- **Row key = user_id** - naturalny access pattern API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Trening ALS i generowanie rekomendacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział danych\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2], seed=42)\n",
    "training.cache()\n",
    "\n",
    "# Trening modelu ALS\n",
    "als = ALS(\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    rank=20,\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"movie_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "model = als.fit(training)\n",
    "train_time = time.time() - start\n",
    "print(f\"Model ALS wytrenowany w {train_time:.1f}s (rank={model.rank})\")\n",
    "\n",
    "# Ewaluacja\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE na zbiorze testowym: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generowanie top 20 rekomendacji dla WSZYSTKICH uzytkownikow\n",
    "TOP_K = 20\n",
    "\n",
    "start = time.time()\n",
    "all_recs = model.recommendForAllUsers(TOP_K)\n",
    "all_recs.cache()\n",
    "num_users = all_recs.count()\n",
    "recs_time = time.time() - start\n",
    "\n",
    "print(f\"Wygenerowano rekomendacje dla {num_users} uzytkownikow w {recs_time:.1f}s\")\n",
    "all_recs.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozpakuj rekomendacje do formatu plaskiego (flat)\n",
    "# Z: user_id, [{movie_id, rating}, ...]\n",
    "# Do: user_id, r_01, r_02, ..., r_20 (movie_ids), s_01, s_02, ..., s_20 (scores)\n",
    "\n",
    "recs_flat = all_recs.select(\n",
    "    col(\"user_id\"),\n",
    "    *[col(\"recommendations\")[i][\"movie_id\"].alias(f\"r_{i+1:02d}\") for i in range(TOP_K)],\n",
    "    *[round(col(\"recommendations\")[i][\"rating\"], 4).alias(f\"s_{i+1:02d}\") for i in range(TOP_K)]\n",
    ")\n",
    "\n",
    "# Dodaj row key (zero-padded user_id)\n",
    "recs_flat = recs_flat.withColumn(\n",
    "    \"user_key\", lpad(col(\"user_id\").cast(\"string\"), 6, \"0\")\n",
    ")\n",
    "\n",
    "print(f\"Kolumny: {len(recs_flat.columns)}\")\n",
    "recs_flat.select(\"user_key\", \"r_01\", \"r_02\", \"r_03\", \"s_01\", \"s_02\", \"s_03\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. Eksport rekomendacji do HBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja katalogu HBase dla rekomendacji\n",
    "recs_columns = {\n",
    "    \"user_key\": {\"cf\": \"rowkey\", \"col\": \"user_key\", \"type\": \"string\"},\n",
    "    \"user_id\":  {\"cf\": \"meta\", \"col\": \"user_id\", \"type\": \"int\"}\n",
    "}\n",
    "\n",
    "# Dodaj kolumny r_01..r_20 (movie_ids) do CF \"recs\"\n",
    "for i in range(1, TOP_K + 1):\n",
    "    recs_columns[f\"r_{i:02d}\"] = {\"cf\": \"recs\", \"col\": f\"r_{i:02d}\", \"type\": \"int\"}\n",
    "\n",
    "# Dodaj kolumny s_01..s_20 (scores) do CF \"scores\"\n",
    "for i in range(1, TOP_K + 1):\n",
    "    recs_columns[f\"s_{i:02d}\"] = {\"cf\": \"scores\", \"col\": f\"s_{i:02d}\", \"type\": \"float\"}\n",
    "\n",
    "recs_catalog = json.dumps({\n",
    "    \"table\": {\"namespace\": \"default\", \"name\": \"user_recommendations\"},\n",
    "    \"rowkey\": \"user_key\",\n",
    "    \"columns\": recs_columns\n",
    "})\n",
    "\n",
    "print(f\"Katalog ma {len(recs_columns)} kolumn\")\n",
    "print(json.dumps(json.loads(recs_catalog), indent=2)[:500] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapis rekomendacji do HBase\n",
    "start = time.time()\n",
    "\n",
    "recs_flat.write \\\n",
    "    .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .options(catalog=recs_catalog) \\\n",
    "    .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "    .option(\"newTable\", \"5\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "write_time = time.time() - start\n",
    "print(f\"Rekomendacje zapisane do HBase w {write_time:.1f}s ({num_users} uzytkownikow)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja - odczytaj rekomendacje z HBase\n",
    "recs_from_hbase = spark.read \\\n",
    "    .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .options(catalog=recs_catalog) \\\n",
    "    .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "print(f\"Rekomendacje w HBase: {recs_from_hbase.count()} uzytkownikow\")\n",
    "\n",
    "# Pokaz rekomendacje dla user 42\n",
    "user_42 = recs_from_hbase.filter(col(\"user_key\") == \"000042\")\n",
    "user_42.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pokaz rekomendacje z tytulami filmow\n",
    "user_42_recs = user_42.select(\n",
    "    *[col(f\"r_{i:02d}\").alias(\"movie_id\") for i in range(1, 6)]\n",
    ")\n",
    "\n",
    "# Rozpakuj do wierszy\n",
    "rec_rows = []\n",
    "u42 = user_42.collect()[0]\n",
    "for i in range(1, TOP_K + 1):\n",
    "    movie_id = u42[f\"r_{i:02d}\"]\n",
    "    score = u42[f\"s_{i:02d}\"]\n",
    "    rec_rows.append((i, movie_id, float(score)))\n",
    "\n",
    "rec_df = spark.createDataFrame(rec_rows, [\"rank\", \"movie_id\", \"predicted_score\"])\n",
    "rec_df.join(movies, \"movie_id\") \\\n",
    "    .select(\"rank\", \"title\", \"genres\", \"predicted_score\") \\\n",
    "    .orderBy(\"rank\") \\\n",
    "    .show(TOP_K, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Bulk Load vs Individual Puts - porownanie wydajnosci\n",
    "\n",
    "Dwa sposoby zapisu danych do HBase:\n",
    "- **Individual Puts**: kazdy wiersz osobno przez RegionServer (wolne)\n",
    "- **Bulk Load**: generuj HFiles bezposrednio i zaladuj do RegionServer (szybkie)\n",
    "\n",
    "```\n",
    "Individual Puts:                    Bulk Load:\n",
    "                                    \n",
    "Client ──► RegionServer             Spark ──► HFiles (HDFS)\n",
    "  │              │                               │\n",
    "  │  WAL write   │                  HBase ◄──── Load HFiles\n",
    "  │  MemStore    │                  (completebulkload)\n",
    "  │  flush       │                               \n",
    "  ▼              ▼                  Pomija WAL i MemStore!\n",
    "  HFile          HFile              Bezposrednio HFiles.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Individual Puts (standardowy zapis przez SHC)\n",
    "sample_10k = recs_flat.limit(10000).cache()\n",
    "sample_10k.count()  # force cache\n",
    "\n",
    "# Metoda 1: Standardowy zapis (individual puts pod spodem)\n",
    "puts_catalog = json.dumps({\n",
    "    \"table\": {\"namespace\": \"default\", \"name\": \"recs_puts_test\"},\n",
    "    \"rowkey\": \"user_key\",\n",
    "    \"columns\": recs_columns\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "sample_10k.write \\\n",
    "    .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .options(catalog=puts_catalog) \\\n",
    "    .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "    .option(\"newTable\", \"5\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "puts_time = time.time() - start\n",
    "print(f\"Individual Puts (10k rows): {puts_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metoda 2: Bulk Load - generuj HFiles i zaladuj\n",
    "# Wymaga posortowania po row key i zapisu jako HFile\n",
    "\n",
    "bulk_catalog = json.dumps({\n",
    "    \"table\": {\"namespace\": \"default\", \"name\": \"recs_bulk_test\"},\n",
    "    \"rowkey\": \"user_key\",\n",
    "    \"columns\": recs_columns\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Bulk load wymaga posortowania po row key\n",
    "sample_sorted = sample_10k.orderBy(\"user_key\")\n",
    "\n",
    "sample_sorted.write \\\n",
    "    .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .options(catalog=bulk_catalog) \\\n",
    "    .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "    .option(\"newTable\", \"5\") \\\n",
    "    .option(\"hbase.spark.bulkload.enable\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "bulk_time = time.time() - start\n",
    "print(f\"Bulk Load (10k rows): {bulk_time:.1f}s\")\n",
    "print(f\"\\nSpeedup: {puts_time / bulk_time:.1f}x szybciej z Bulk Load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000002",
   "metadata": {},
   "source": [
    "### Kiedy uzywac Bulk Load?\n",
    "\n",
    "| Scenariusz | Metoda | Dlaczego |\n",
    "|------------|--------|----------|\n",
    "| Inicjalny zaladowanie danych | Bulk Load | Miliony wierszy, jednorazowo |\n",
    "| Batch update rekomendacji | Bulk Load | Duza ilosc, co kilka godzin |\n",
    "| Pojedyncze oceny uzytkownikow | Individual Put | Male dane, real-time |\n",
    "| Streaming updates | Individual Put | Niski latency wymagany |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. TTL - automatyczne wygasanie rekomendacji\n",
    "\n",
    "HBase umozliwia ustawienie **TTL (Time To Live)** na column family. Po uplywie TTL dane sa automatycznie usuwane podczas compaction.\n",
    "\n",
    "Przydatne dla rekomendacji - stare rekomendacje tracą aktualnosc!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Tworzenie tabeli z TTL w HBase Shell\n",
    "# TTL = 86400 (24 godziny) - rekomendacje wygasaja po 1 dniu\n",
    "# TTL = 604800 (7 dni) - rekomendacje wygasaja po tygodniu\n",
    "\n",
    "# hbase shell <<EOF\n",
    "# create 'user_recommendations_ttl',\n",
    "#   {NAME => 'recs', VERSIONS => 3, TTL => 604800, COMPRESSION => 'SNAPPY'},\n",
    "#   {NAME => 'scores', VERSIONS => 3, TTL => 604800, COMPRESSION => 'SNAPPY'},\n",
    "#   {NAME => 'meta', VERSIONS => 1, TTL => 2592000}\n",
    "# EOF\n",
    "\n",
    "# Sprawdz TTL:\n",
    "# hbase shell -n <<< \"describe 'user_recommendations_ttl'\"\n",
    "\n",
    "# Zmiana TTL na istniejacej tabeli:\n",
    "# hbase shell <<EOF\n",
    "# disable 'user_recommendations_ttl'\n",
    "# alter 'user_recommendations_ttl', {NAME => 'recs', TTL => 86400}\n",
    "# enable 'user_recommendations_ttl'\n",
    "# EOF\n",
    "\n",
    "echo \"TTL configuration reference (uncomment to run)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symulacja TTL - zapisz rekomendacje z roznym timestamp\n",
    "# W produkcji HBase automatycznie przypisuje timestamp\n",
    "\n",
    "# Dodaj kolumne z timestamp generowania rekomendacji\n",
    "recs_with_ts = recs_flat.limit(100).withColumn(\n",
    "    \"generated_at\", current_timestamp().cast(\"string\")\n",
    ").withColumn(\n",
    "    \"expires_at\",\n",
    "    date_add(current_timestamp(), 7).cast(\"string\")  # wygasa za 7 dni\n",
    ")\n",
    "\n",
    "print(\"Rekomendacje z timestamp i expiry:\")\n",
    "recs_with_ts.select(\"user_key\", \"generated_at\", \"expires_at\", \"r_01\", \"r_02\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Wersjonowanie - historia rekomendacji\n",
    "\n",
    "HBase przechowuje wiele wersji (timestamps) tej samej komorki. Mozemy uzyc tego do sledzenia jak rekomendacje zmienialy sie w czasie.\n",
    "\n",
    "```\n",
    "Row: 000042\n",
    "  recs:r_01\n",
    "    @t=1700000000  movie_id=318   (Shawshank Redemption)\n",
    "    @t=1699000000  movie_id=296   (Pulp Fiction)\n",
    "    @t=1698000000  movie_id=2571  (Matrix)\n",
    "  recs:r_02  \n",
    "    @t=1700000000  movie_id=858   (Godfather)\n",
    "    @t=1699000000  movie_id=318   (Shawshank Redemption)\n",
    "    ...\n",
    "    \n",
    "GET z VERSIONS => 3  →  zwraca 3 ostatnie wersje rekomendacji\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Odczyt wersji w HBase Shell\n",
    "\n",
    "# Odczyt najnowszej wersji (domyslnie):\n",
    "# hbase shell -n <<< \"get 'user_recommendations', '000042', {COLUMN => 'recs:r_01'}\"\n",
    "\n",
    "# Odczyt 3 ostatnich wersji:\n",
    "# hbase shell -n <<< \"get 'user_recommendations', '000042', {COLUMN => 'recs:r_01', VERSIONS => 3}\"\n",
    "\n",
    "# Odczyt wersji z zakresu czasowego:\n",
    "# hbase shell <<EOF\n",
    "# get 'user_recommendations', '000042', {COLUMN => 'recs:r_01', TIMERANGE => [1699000000000, 1700000000000]}\n",
    "# EOF\n",
    "\n",
    "echo \"Versioning commands reference (uncomment to run)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symulacja wersjonowania - dwa rozne modele ALS\n",
    "# Model 1: rank=10, regParam=0.1\n",
    "# Model 2: rank=50, regParam=0.05\n",
    "\n",
    "# Trenuj drugi model z innymi parametrami\n",
    "als_v2 = ALS(\n",
    "    maxIter=15,\n",
    "    regParam=0.05,\n",
    "    rank=50,\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"movie_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "model_v2 = als_v2.fit(training)\n",
    "recs_v2 = model_v2.recommendForAllUsers(TOP_K)\n",
    "\n",
    "# Porownaj rekomendacje v1 vs v2 dla user 42\n",
    "recs_v1_42 = all_recs.filter(col(\"user_id\") == 42) \\\n",
    "    .select(explode(\"recommendations\").alias(\"rec\")) \\\n",
    "    .select(col(\"rec.movie_id\").alias(\"movie_id_v1\"))\n",
    "\n",
    "recs_v2_42 = recs_v2.filter(col(\"user_id\") == 42) \\\n",
    "    .select(explode(\"recommendations\").alias(\"rec\")) \\\n",
    "    .select(col(\"rec.movie_id\").alias(\"movie_id_v2\"))\n",
    "\n",
    "# Pokaz roznice\n",
    "v1_ids = set(r.movie_id_v1 for r in recs_v1_42.collect())\n",
    "v2_ids = set(r.movie_id_v2 for r in recs_v2_42.collect())\n",
    "overlap = v1_ids & v2_ids\n",
    "\n",
    "print(f\"Model v1 (rank=20): {sorted(v1_ids)}\")\n",
    "print(f\"Model v2 (rank=50): {sorted(v2_ids)}\")\n",
    "print(f\"Wspolne filmy: {len(overlap)} / {TOP_K} ({len(overlap)/TOP_K*100:.0f}%)\")\n",
    "print(f\"Nowe w v2: {v2_ids - v1_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000002",
   "metadata": {},
   "source": [
    "W produkcji kazdy zapis do tej samej komorki automatycznie tworzy nowa wersje z aktualnym timestampem. Poprzednie wersje sa dostepne az do osiagniecia limitu VERSIONS lub TTL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## 9. Scan z filtrami - segmentacja uzytkownikow\n",
    "\n",
    "HBase oferuje rozne filtry do przeszukiwania danych. Przydatne do analizy rekomendacji per segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# HBase Scan z filtrami\n",
    "\n",
    "# PrefixFilter - rekomendacje dla userow 000100-000199\n",
    "# hbase shell <<EOF\n",
    "# scan 'user_recommendations', {\n",
    "#   STARTROW => '000100',\n",
    "#   STOPROW => '000200',\n",
    "#   COLUMNS => ['recs:r_01', 'recs:r_02', 'recs:r_03'],\n",
    "#   LIMIT => 10\n",
    "# }\n",
    "# EOF\n",
    "\n",
    "# SingleColumnValueFilter - userzy ktorym polecono film 318\n",
    "# hbase shell <<EOF\n",
    "# scan 'user_recommendations', {\n",
    "#   FILTER => \"SingleColumnValueFilter('recs', 'r_01', =, 'binary:318')\",\n",
    "#   LIMIT => 20\n",
    "# }\n",
    "# EOF\n",
    "\n",
    "# ColumnPrefixFilter - tylko kolumny rekomendacji (bez scores)\n",
    "# hbase shell <<EOF\n",
    "# scan 'user_recommendations', {\n",
    "#   FILTER => \"ColumnPrefixFilter('r_')\",\n",
    "#   LIMIT => 5\n",
    "# }\n",
    "# EOF\n",
    "\n",
    "# MultipleColumnPrefixFilter - rekomendacje i meta\n",
    "# hbase shell <<EOF\n",
    "# scan 'user_recommendations', {\n",
    "#   FILTER => \"MultipleColumnPrefixFilter('r_', 's_')\",\n",
    "#   STARTROW => '000042',\n",
    "#   STOPROW => '000043'\n",
    "# }\n",
    "# EOF\n",
    "\n",
    "echo \"HBase filter commands reference (uncomment to run)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentacja przez Spark - \"power users\" z duza liczba ocen\n",
    "user_activity = ratings.groupBy(\"user_id\").agg(\n",
    "    count(\"*\").alias(\"num_ratings\"),\n",
    "    avg(\"rating\").alias(\"avg_rating\")\n",
    ")\n",
    "\n",
    "# Power users: > 500 ocen\n",
    "power_users = user_activity.filter(col(\"num_ratings\") > 500)\n",
    "print(f\"Power users (>500 ocen): {power_users.count()}\")\n",
    "\n",
    "# Polacz z rekomendacjami z HBase\n",
    "power_user_recs = recs_from_hbase.join(\n",
    "    power_users.withColumn(\"user_key\", lpad(col(\"user_id\").cast(\"string\"), 6, \"0\")),\n",
    "    \"user_key\"\n",
    ")\n",
    "\n",
    "print(f\"Rekomendacje dla power users: {power_user_recs.count()}\")\n",
    "power_user_recs.select(\"user_key\", \"num_ratings\", \"avg_rating\", \"r_01\", \"r_02\", \"r_03\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10000001",
   "metadata": {},
   "source": [
    "## 10. Tabela podobnych filmow w HBase\n",
    "\n",
    "Oprócz rekomendacji per uzytkownik, czesto potrzebujemy tez **podobnych filmow** (item-item similarity). HBase jest idealny do tego - szybki GET po movie_id.\n",
    "\n",
    "```\n",
    "Tabela: movie_similarity\n",
    "┌──────────┬─────────────────────────────────────────────────┐\n",
    "│ Row Key  │ Column Family: similar                          │\n",
    "│(movie_id)├──────────┬──────────┬──────────┬──────┬────────┤\n",
    "│          │sim:m_01  │sim:m_02  │sim:m_03  │ ...  │sim:m_10│\n",
    "├──────────┼──────────┼──────────┼──────────┼──────┼────────┤\n",
    "│ 000001   │ 3114     │ 2355     │ 588      │ ...  │ 1265   │\n",
    "│(Toy Story)│(Toy St2) │(Bug Life)│(Aladdin) │      │(G Day) │\n",
    "├──────────┼──────────┼──────────┼──────────┼──────┼────────┤\n",
    "│ 000318   │ 858      │ 527      │ 1193     │ ...  │ 4226   │\n",
    "│(Shawsh.) │(Godfth.) │(Schindl.)│(Matrix)  │      │(Memento)│\n",
    "└──────────┴──────────┴──────────┴──────────┴──────┴────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oblicz item-item similarity z wektorow latentnych ALS\n",
    "import numpy as np\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "item_factors = model.itemFactors\n",
    "print(f\"Item factors: {item_factors.count()} filmow x rank={model.rank}\")\n",
    "\n",
    "# Dla kazdego filmu znajdz 10 najbardzej podobnych\n",
    "# Uzyj cross-join + cosine similarity\n",
    "# UWAGA: dla duzej liczby filmow to jest kosztowne - w produkcji uzyj LSH\n",
    "\n",
    "# Weź top 1000 najpopularniejszych filmow (zeby nie eksplodowac)\n",
    "popular_movies = ratings.groupBy(\"movie_id\").count() \\\n",
    "    .orderBy(desc(\"count\")).limit(1000) \\\n",
    "    .select(\"movie_id\")\n",
    "\n",
    "popular_factors = item_factors.join(\n",
    "    popular_movies, item_factors[\"id\"] == popular_movies[\"movie_id\"]\n",
    ").select(col(\"id\").alias(\"movie_id\"), \"features\")\n",
    "\n",
    "print(f\"Popularne filmy z wektorami: {popular_factors.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity miedzy wszystkimi parami (top 1000 filmow)\n",
    "@udf(DoubleType())\n",
    "def cosine_sim(v1, v2):\n",
    "    a = np.array(v1)\n",
    "    b = np.array(v2)\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))\n",
    "\n",
    "# Self-join dla par filmow\n",
    "pairs = popular_factors.alias(\"a\").crossJoin(popular_factors.alias(\"b\")) \\\n",
    "    .filter(col(\"a.movie_id\") < col(\"b.movie_id\")) \\\n",
    "    .withColumn(\"similarity\", cosine_sim(col(\"a.features\"), col(\"b.features\"))) \\\n",
    "    .select(\n",
    "        col(\"a.movie_id\").alias(\"movie_id_a\"),\n",
    "        col(\"b.movie_id\").alias(\"movie_id_b\"),\n",
    "        \"similarity\"\n",
    "    )\n",
    "\n",
    "pairs.cache()\n",
    "print(f\"Par filmow: {pairs.count()}\")\n",
    "pairs.orderBy(desc(\"similarity\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 podobnych filmow per film\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "SIM_K = 10\n",
    "\n",
    "# Symetryczne pary (a->b i b->a)\n",
    "all_pairs = pairs.union(\n",
    "    pairs.select(\n",
    "        col(\"movie_id_b\").alias(\"movie_id_a\"),\n",
    "        col(\"movie_id_a\").alias(\"movie_id_b\"),\n",
    "        \"similarity\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Top K per film\n",
    "w = Window.partitionBy(\"movie_id_a\").orderBy(desc(\"similarity\"))\n",
    "top_similar = all_pairs.withColumn(\"rank\", row_number().over(w)) \\\n",
    "    .filter(col(\"rank\") <= SIM_K)\n",
    "\n",
    "# Pivot do formatu plaskiego: movie_id, m_01, m_02, ..., m_10\n",
    "sim_flat = top_similar.groupBy(\"movie_id_a\").pivot(\"rank\", list(range(1, SIM_K + 1))) \\\n",
    "    .agg(first(\"movie_id_b\"))\n",
    "\n",
    "# Rename kolumn\n",
    "for i in range(1, SIM_K + 1):\n",
    "    sim_flat = sim_flat.withColumnRenamed(str(i), f\"m_{i:02d}\")\n",
    "\n",
    "sim_flat = sim_flat.withColumnRenamed(\"movie_id_a\", \"movie_id\") \\\n",
    "    .withColumn(\"movie_key\", lpad(col(\"movie_id\").cast(\"string\"), 6, \"0\"))\n",
    "\n",
    "print(f\"Tabela podobnych filmow: {sim_flat.count()} filmow\")\n",
    "sim_flat.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz tabele podobnych filmow do HBase\n",
    "sim_columns = {\n",
    "    \"movie_key\": {\"cf\": \"rowkey\", \"col\": \"movie_key\", \"type\": \"string\"},\n",
    "    \"movie_id\":  {\"cf\": \"info\", \"col\": \"movie_id\", \"type\": \"int\"}\n",
    "}\n",
    "for i in range(1, SIM_K + 1):\n",
    "    sim_columns[f\"m_{i:02d}\"] = {\"cf\": \"similar\", \"col\": f\"m_{i:02d}\", \"type\": \"int\"}\n",
    "\n",
    "sim_catalog = json.dumps({\n",
    "    \"table\": {\"namespace\": \"default\", \"name\": \"movie_similarity\"},\n",
    "    \"rowkey\": \"movie_key\",\n",
    "    \"columns\": sim_columns\n",
    "})\n",
    "\n",
    "start = time.time()\n",
    "sim_flat.write \\\n",
    "    .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .options(catalog=sim_catalog) \\\n",
    "    .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "    .option(\"newTable\", \"5\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "sim_time = time.time() - start\n",
    "print(f\"Tabela movie_similarity zapisana w {sim_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja - filmy podobne do Toy Story (movie_id=1)\n",
    "sim_from_hbase = spark.read \\\n",
    "    .format(\"org.apache.hadoop.hbase.spark\") \\\n",
    "    .options(catalog=sim_catalog) \\\n",
    "    .option(\"hbase.spark.use.hbasecontext\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "toy_story_sim = sim_from_hbase.filter(col(\"movie_key\") == \"000001\")\n",
    "if toy_story_sim.count() > 0:\n",
    "    ts = toy_story_sim.collect()[0]\n",
    "    print(\"Filmy podobne do Toy Story (1995):\")\n",
    "    for i in range(1, SIM_K + 1):\n",
    "        mid = ts[f\"m_{i:02d}\"]\n",
    "        if mid:\n",
    "            title = movies.filter(col(\"movie_id\") == mid).select(\"title\").collect()\n",
    "            title_str = title[0][\"title\"] if title else \"unknown\"\n",
    "            print(f\"  {i:2d}. movie_id={mid} - {title_str}\")\n",
    "else:\n",
    "    print(\"Toy Story nie jest w top 1000 popularnych filmow lub HBase niedostepny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11000001",
   "metadata": {},
   "source": [
    "## 11. Spark reads z HBase - real-time feature enrichment\n",
    "\n",
    "W produkcji Spark moze czytac z HBase w celu wzbogacenia danych (feature enrichment) - np. dolaczenie profilu uzytkownika do streamu zdarzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symulacja: strumien nowych ocen + enrichment z HBase\n",
    "\n",
    "# \"Nowe\" oceny (symulacja - ostatnie 1000 ocen)\n",
    "new_ratings = ratings.orderBy(desc(\"rating_timestamp\")).limit(1000)\n",
    "\n",
    "# Dolacz rekomendacje z HBase - czy user ocenil film ktory mu polecilismy?\n",
    "enriched = new_ratings.withColumn(\n",
    "    \"user_key\", lpad(col(\"user_id\").cast(\"string\"), 6, \"0\")\n",
    ").join(\n",
    "    recs_from_hbase.select(\"user_key\", \"r_01\", \"r_02\", \"r_03\", \"r_04\", \"r_05\"),\n",
    "    \"user_key\",\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# Sprawdz czy oceniony film byl w top 5 rekomendacji\n",
    "enriched = enriched.withColumn(\n",
    "    \"was_recommended\",\n",
    "    col(\"movie_id\").isin(\n",
    "        col(\"r_01\"), col(\"r_02\"), col(\"r_03\"), col(\"r_04\"), col(\"r_05\")\n",
    "    )\n",
    ")\n",
    "\n",
    "hit_rate = enriched.filter(col(\"was_recommended\") == True).count() / enriched.count() * 100\n",
    "print(f\"Hit rate (film w top 5 recs): {hit_rate:.2f}%\")\n",
    "\n",
    "enriched.filter(col(\"was_recommended\") == True) \\\n",
    "    .select(\"user_id\", \"movie_id\", \"rating\", \"r_01\", \"r_02\", \"r_03\") \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12000001",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "\n",
    "Zaimplementuj sprawdzanie swiezosci rekomendacji uzywajac timestampow.\n",
    "\n",
    "1. Zapisz rekomendacje do HBase z kolumna `meta:generated_at` (timestamp generowania)\n",
    "2. Odczytaj rekomendacje z HBase\n",
    "3. Dla kazdego uzytkownika sprawdz czy rekomendacje sa \"swieze\" (< 7 dni)\n",
    "4. Oblicz statystyki:\n",
    "   - Ile procent uzytkownikow ma swieze rekomendacje?\n",
    "   - Ile procent ma przestarzale rekomendacje (> 7 dni)?\n",
    "   - Ile procent nie ma rekomendacji wcale?\n",
    "5. Dla uzytkownikow z przestarzalymi rekomendacjami - wygeneruj nowe i zapisz do HBase\n",
    "\n",
    "Wskazowka: Symuluj rozne daty generowania, np. losowo 1-14 dni wstecz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiazanie:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13000001",
   "metadata": {},
   "source": [
    "## Zadanie koncowe\n",
    "\n",
    "Zbuduj pelny pipeline: train ALS, export do HBase, symulacja API reads.\n",
    "\n",
    "### Kroki:\n",
    "\n",
    "1. **Trening modelu ALS** na pelnych danych MovieLens\n",
    "   - Uzyj optymalnych parametrow (rank=20, regParam=0.1, maxIter=15)\n",
    "   - Zmierz RMSE na zbiorze testowym\n",
    "\n",
    "2. **Generowanie i eksport rekomendacji:**\n",
    "   - Top 20 rekomendacji per uzytkownik → tabela `pipeline_user_recs`\n",
    "   - Top 10 podobnych filmow per film (top 500) → tabela `pipeline_movie_sim`\n",
    "   - Profil uzytkownika (avg_rating, num_ratings, top_genre) → tabela `pipeline_user_profiles`\n",
    "   - Uzyj bulk load dla duzych tabel\n",
    "\n",
    "3. **Symulacja API reads** (GET dla 100 losowych uzytkownikow):\n",
    "   - Pobierz rekomendacje z HBase\n",
    "   - Dolacz tytuly filmow\n",
    "   - Zmierz sredni czas odpowiedzi\n",
    "   - Porownaj z PostgreSQL (Spark JDBC)\n",
    "\n",
    "4. **Raport:**\n",
    "   - Czas treningu modelu\n",
    "   - Czas generowania rekomendacji\n",
    "   - Czas zapisu do HBase (puts vs bulk load)\n",
    "   - Sredni czas odczytu z HBase vs PostgreSQL\n",
    "   - RMSE modelu\n",
    "   - Przykladowe rekomendacje dla 3 uzytkownikow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiazanie:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.unpersist()\n",
    "movies.unpersist()\n",
    "training.unpersist()\n",
    "all_recs.unpersist()\n",
    "pairs.unpersist()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
