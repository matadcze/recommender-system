{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 23 - NiFi Data Flows\n",
    "\n",
    "Apache NiFi - platforma do automatyzacji przepływu danych między systemami.\n",
    "\n",
    "**Tematy:**\n",
    "- Architektura NiFi: FlowFile, Processor, Connection, Process Group\n",
    "- NiFi REST API - zarządzanie z poziomu Pythona\n",
    "- Tworzenie procesorów: GetFile, PutFile, RouteOnAttribute\n",
    "- Tworzenie flow: CSV -> transformacja -> Parquet/HDFS\n",
    "- NiFi Expression Language\n",
    "- Data provenance i lineage\n",
    "- Monitoring: bulletins, back pressure, stats via API\n",
    "- Zadanie koncowe: flow do ingestion danych MovieLens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "## 1. Architektura Apache NiFi\n",
    "\n",
    "```\n",
    "                        ┌─────────────────────────────────────┐\n",
    "                        │           NiFi Cluster              │\n",
    "                        │                                     │\n",
    "  ┌──────────┐          │  ┌───────────┐    ┌───────────┐    │         ┌──────────┐\n",
    "  │  Source   │──FlowFile─>│ Processor │───>│ Processor │────│─────────│  Sink    │\n",
    "  │(CSV/API)  │          │  │ (GetFile) │    │(Transform)│    │         │(HDFS/DB) │\n",
    "  └──────────┘          │  └───────────┘    └───────────┘    │         └──────────┘\n",
    "                        │       │                  │          │\n",
    "                        │       └──Connection──────┘          │\n",
    "                        │      (queue + back pressure)        │\n",
    "                        └─────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Kluczowe koncepty:\n",
    "\n",
    "| Koncept | Opis |\n",
    "|---------|------|\n",
    "| **FlowFile** | Jednostka danych w NiFi - content (bajty) + attributes (metadane key-value) |\n",
    "| **Processor** | Komponent przetwarzajacy FlowFile (GetFile, PutHDFS, RouteOnAttribute...) |\n",
    "| **Connection** | Kolejka FIFO miedzy procesorami z back pressure |\n",
    "| **Process Group** | Kontener grupujacy procesory (jak folder) |\n",
    "| **Controller Service** | Wspoldzielona usluga (np. DBCPConnectionPool, SSLContext) |\n",
    "| **Provenance** | Pelna historia kazdego FlowFile - skad, dokad, co sie zmienilo |\n",
    "\n",
    "### Dlaczego NiFi?\n",
    "- **Wizualny interfejs** - projektowanie flow drag & drop\n",
    "- **Back pressure** - automatyczne spowalnianie zrodla gdy sink nie nadaza\n",
    "- **Guaranteed delivery** - dane nie gina (Write-Ahead Log)\n",
    "- **Data provenance** - pelny lineage kazdego bajta\n",
    "- **300+ procesorow** out of the box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2000001",
   "metadata": {},
   "source": [
    "## 2. Setup - polaczenie z NiFi REST API\n",
    "\n",
    "NiFi udostepnia pelne REST API do zarzadzania flow programowo.\n",
    "Dokumentacja: `https://nifi:8443/nifi-api` lub `http://nifi:8080/nifi-api`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import urllib3\n",
    "\n",
    "# Wylacz ostrzezenia SSL dla self-signed cert\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Konfiguracja NiFi API\n",
    "# Dostosuj URL w zaleznosci od konfiguracji (HTTPS z auth lub HTTP bez)\n",
    "NIFI_API = \"https://nifi:8443/nifi-api\"\n",
    "# Alternatywnie: NIFI_API = \"http://nifi:8080/nifi-api\"\n",
    "\n",
    "# Sesja HTTP z domyslnymi ustawieniami\n",
    "session = requests.Session()\n",
    "session.verify = False  # self-signed cert\n",
    "\n",
    "# Jezeli NiFi wymaga autentykacji (HTTPS), uzyskaj token:\n",
    "def get_nifi_token(username=\"admin\", password=\"admin123456789\"):\n",
    "    \"\"\"Uzyskaj token dostepu do NiFi API.\"\"\"\n",
    "    try:\n",
    "        resp = session.post(\n",
    "            f\"{NIFI_API}/access/token\",\n",
    "            data={\"username\": username, \"password\": password},\n",
    "            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "        )\n",
    "        if resp.status_code == 201:\n",
    "            token = resp.text\n",
    "            session.headers.update({\"Authorization\": f\"Bearer {token}\"})\n",
    "            print(f\"Token uzyskany pomyslnie (dlugosc: {len(token)})\")\n",
    "            return token\n",
    "        else:\n",
    "            print(f\"Auth nie wymagany lub blad: {resp.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Proba polaczenia bez auth: {e}\")\n",
    "        return None\n",
    "\n",
    "# Probuj uzyskac token (opcjonalne)\n",
    "token = get_nifi_token()\n",
    "\n",
    "# Test polaczenia\n",
    "resp = session.get(f\"{NIFI_API}/system-diagnostics\")\n",
    "print(f\"\\nStatus polaczenia: {resp.status_code}\")\n",
    "if resp.status_code == 200:\n",
    "    diag = resp.json()\n",
    "    heap = diag[\"systemDiagnostics\"][\"aggregateSnapshot\"][\"heapUtilization\"]\n",
    "    print(f\"NiFi Heap: {heap}\")\n",
    "    print(\"Polaczenie z NiFi OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions do pracy z NiFi API\n",
    "\n",
    "def nifi_get(path):\n",
    "    \"\"\"GET request do NiFi API.\"\"\"\n",
    "    resp = session.get(f\"{NIFI_API}{path}\")\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def nifi_post(path, data):\n",
    "    \"\"\"POST request do NiFi API.\"\"\"\n",
    "    resp = session.post(\n",
    "        f\"{NIFI_API}{path}\",\n",
    "        json=data,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def nifi_put(path, data):\n",
    "    \"\"\"PUT request do NiFi API.\"\"\"\n",
    "    resp = session.put(\n",
    "        f\"{NIFI_API}{path}\",\n",
    "        json=data,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def nifi_delete(path, params=None):\n",
    "    \"\"\"DELETE request do NiFi API.\"\"\"\n",
    "    resp = session.delete(f\"{NIFI_API}{path}\", params=params)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "# Pobierz root process group ID\n",
    "root_flow = nifi_get(\"/flow/process-groups/root\")\n",
    "ROOT_PG_ID = root_flow[\"processGroupFlow\"][\"id\"]\n",
    "print(f\"Root Process Group ID: {ROOT_PG_ID}\")\n",
    "\n",
    "# Wyswietl istniejace procesory\n",
    "processors = root_flow[\"processGroupFlow\"][\"flow\"][\"processors\"]\n",
    "print(f\"\\nIstniejace procesory ({len(processors)}):\")\n",
    "for p in processors:\n",
    "    name = p[\"component\"][\"name\"]\n",
    "    ptype = p[\"component\"][\"type\"].split(\".\")[-1]\n",
    "    state = p[\"component\"][\"state\"]\n",
    "    print(f\"  {name:<30} [{ptype}] state={state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3000001",
   "metadata": {},
   "source": [
    "## 3. Tworzenie Process Group\n",
    "\n",
    "Process Group to kontener na procesory - jak folder w systemie plikow.\n",
    "Pozwala organizowac flow w logiczne bloki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stworz Process Group \"MovieLens Ingestion\"\n",
    "pg_body = {\n",
    "    \"revision\": {\"version\": 0},\n",
    "    \"component\": {\n",
    "        \"name\": \"MovieLens Ingestion\",\n",
    "        \"position\": {\"x\": 100, \"y\": 100}\n",
    "    }\n",
    "}\n",
    "\n",
    "pg_resp = nifi_post(f\"/process-groups/{ROOT_PG_ID}/process-groups\", pg_body)\n",
    "PG_ID = pg_resp[\"id\"]\n",
    "print(f\"Process Group utworzony: {PG_ID}\")\n",
    "print(f\"Nazwa: {pg_resp['component']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4000001",
   "metadata": {},
   "source": [
    "## 4. Tworzenie procesorow\n",
    "\n",
    "Procesory to podstawowe jednostki przetwarzania w NiFi.\n",
    "\n",
    "### Najwazniejsze typy procesorow:\n",
    "\n",
    "| Procesor | Kategoria | Opis |\n",
    "|----------|-----------|------|\n",
    "| **GetFile** | Input | Czyta pliki z lokalnego systemu plikow |\n",
    "| **PutFile** | Output | Zapisuje FlowFile do lokalnego FS |\n",
    "| **PutHDFS** | Output | Zapisuje FlowFile do HDFS |\n",
    "| **RouteOnAttribute** | Routing | Kieruje FlowFile na podstawie atrybutow |\n",
    "| **UpdateAttribute** | Transform | Modyfikuje atrybuty FlowFile |\n",
    "| **ConvertRecord** | Transform | Konwersja formatow (CSV->JSON->Avro) |\n",
    "| **ExecuteSQL** | Database | Wykonuje zapytanie SQL |\n",
    "| **PutDatabaseRecord** | Database | Wstawia rekordy do bazy |\n",
    "| **PublishKafka** | Messaging | Wysyla do tematu Kafka |\n",
    "| **InvokeHTTP** | External | Wywoluje HTTP endpoint |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_processor(pg_id, proc_type, name, position, config=None, auto_terminate=None):\n",
    "    \"\"\"Tworzy procesor w danym Process Group.\n",
    "    \n",
    "    Args:\n",
    "        pg_id: ID Process Group\n",
    "        proc_type: pelen typ procesora (np. org.apache.nifi.processors.standard.GetFile)\n",
    "        name: nazwa wyswietlana\n",
    "        position: dict z x, y\n",
    "        config: dict z propertiami procesora\n",
    "        auto_terminate: lista relacji do auto-terminate\n",
    "    \"\"\"\n",
    "    body = {\n",
    "        \"revision\": {\"version\": 0},\n",
    "        \"component\": {\n",
    "            \"type\": proc_type,\n",
    "            \"name\": name,\n",
    "            \"position\": position,\n",
    "            \"config\": {}\n",
    "        }\n",
    "    }\n",
    "    if config:\n",
    "        body[\"component\"][\"config\"][\"properties\"] = config\n",
    "    if auto_terminate:\n",
    "        body[\"component\"][\"config\"][\"autoTerminatedRelationships\"] = auto_terminate\n",
    "    \n",
    "    result = nifi_post(f\"/process-groups/{pg_id}/processors\", body)\n",
    "    print(f\"Procesor '{name}' utworzony: {result['id']}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- Procesor 1: GetFile - czyta CSV z dysku ---\n",
    "get_file = create_processor(\n",
    "    PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.GetFile\",\n",
    "    \"Read MovieLens CSV\",\n",
    "    {\"x\": 100, \"y\": 100},\n",
    "    config={\n",
    "        \"Input Directory\": \"/data/raw/movielens\",\n",
    "        \"File Filter\": \"rating\\\\.csv\",\n",
    "        \"Keep Source File\": \"true\",       # nie usuwaj oryginalu\n",
    "        \"Batch Size\": \"1\"                  # 1 plik na raz\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Procesor 2: UpdateAttribute - dodaj metadane ---\n",
    "update_attr = create_processor(\n",
    "    PG_ID,\n",
    "    \"org.apache.nifi.processors.attributes.UpdateAttribute\",\n",
    "    \"Add Metadata\",\n",
    "    {\"x\": 100, \"y\": 300},\n",
    "    config={\n",
    "        \"dataset\": \"movielens\",\n",
    "        \"ingestion.timestamp\": \"${now():format('yyyy-MM-dd HH:mm:ss')}\",\n",
    "        \"source.type\": \"csv\",\n",
    "        \"schema.name\": \"ratings\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Procesor 3: RouteOnAttribute - routing na podstawie rozmiaru ---\n",
    "route = create_processor(\n",
    "    PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.RouteOnAttribute\",\n",
    "    \"Route by Size\",\n",
    "    {\"x\": 100, \"y\": 500},\n",
    "    config={\n",
    "        \"Routing Strategy\": \"Route to Property name\",\n",
    "        \"large_file\": \"${fileSize:gt(10485760)}\",   # > 10MB\n",
    "        \"small_file\": \"${fileSize:le(10485760)}\"     # <= 10MB\n",
    "    },\n",
    "    auto_terminate=[\"unmatched\"]\n",
    ")\n",
    "\n",
    "# --- Procesor 4: PutFile - zapisz wynik ---\n",
    "put_file = create_processor(\n",
    "    PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.PutFile\",\n",
    "    \"Write to Staging\",\n",
    "    {\"x\": 100, \"y\": 700},\n",
    "    config={\n",
    "        \"Directory\": \"/data/staging/movielens/${schema.name}\",\n",
    "        \"Conflict Resolution Strategy\": \"replace\"\n",
    "    },\n",
    "    auto_terminate=[\"success\", \"failure\"]\n",
    ")\n",
    "\n",
    "print(\"\\nWszystkie procesory utworzone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie polaczen (Connection) miedzy procesorami\n",
    "\n",
    "def create_connection(pg_id, source_id, dest_id, relationships, name=\"\"):\n",
    "    \"\"\"Tworzy polaczenie miedzy dwoma procesorami.\"\"\"\n",
    "    body = {\n",
    "        \"revision\": {\"version\": 0},\n",
    "        \"component\": {\n",
    "            \"name\": name,\n",
    "            \"source\": {\"id\": source_id, \"type\": \"PROCESSOR\", \"groupId\": pg_id},\n",
    "            \"destination\": {\"id\": dest_id, \"type\": \"PROCESSOR\", \"groupId\": pg_id},\n",
    "            \"selectedRelationships\": relationships,\n",
    "            \"backPressureObjectThreshold\": 10000,\n",
    "            \"backPressureDataSizeThreshold\": \"1 GB\",\n",
    "            \"flowFileExpiration\": \"0 sec\"   # nigdy nie wygasaj\n",
    "        }\n",
    "    }\n",
    "    result = nifi_post(f\"/process-groups/{pg_id}/connections\", body)\n",
    "    print(f\"Connection '{name}': {source_id[:8]}... -> {dest_id[:8]}... [{', '.join(relationships)}]\")\n",
    "    return result\n",
    "\n",
    "# GetFile -> UpdateAttribute\n",
    "conn1 = create_connection(\n",
    "    PG_ID,\n",
    "    get_file[\"id\"], update_attr[\"id\"],\n",
    "    [\"success\"],\n",
    "    \"raw csv\"\n",
    ")\n",
    "\n",
    "# UpdateAttribute -> RouteOnAttribute\n",
    "conn2 = create_connection(\n",
    "    PG_ID,\n",
    "    update_attr[\"id\"], route[\"id\"],\n",
    "    [\"success\"],\n",
    "    \"with metadata\"\n",
    ")\n",
    "\n",
    "# RouteOnAttribute (large_file) -> PutFile\n",
    "conn3 = create_connection(\n",
    "    PG_ID,\n",
    "    route[\"id\"], put_file[\"id\"],\n",
    "    [\"large_file\"],\n",
    "    \"large files\"\n",
    ")\n",
    "\n",
    "# RouteOnAttribute (small_file) -> PutFile\n",
    "conn4 = create_connection(\n",
    "    PG_ID,\n",
    "    route[\"id\"], put_file[\"id\"],\n",
    "    [\"small_file\"],\n",
    "    \"small files\"\n",
    ")\n",
    "\n",
    "print(\"\\nFlow polaczony!\")\n",
    "print(\"Schemat: GetFile -> UpdateAttribute -> RouteOnAttribute -> PutFile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000001",
   "metadata": {},
   "source": [
    "## 5. NiFi Expression Language\n",
    "\n",
    "NiFi Expression Language (EL) pozwala dynamicznie odwolywac sie do atrybutow FlowFile.\n",
    "\n",
    "### Podstawowa skladnia:\n",
    "\n",
    "```\n",
    "${attribute_name}                          # wartosc atrybutu\n",
    "${filename:substringBefore('.')}           # operacje na stringach\n",
    "${fileSize:gt(1048576)}                    # porownanie (> 1MB)\n",
    "${now():format('yyyy-MM-dd')}              # aktualna data\n",
    "${literal('hello'):append(' world')}       # literal + append\n",
    "```\n",
    "\n",
    "### Najczesciej uzywane funkcje:\n",
    "\n",
    "| Funkcja | Opis | Przyklad |\n",
    "|---------|------|----------|\n",
    "| `substringBefore(sep)` | Tekst przed separatorem | `${filename:substringBefore('.')}` -> `rating` |\n",
    "| `substringAfter(sep)` | Tekst po separatorze | `${filename:substringAfter('.')}` -> `csv` |\n",
    "| `replace(old, new)` | Zamiana tekstu | `${filename:replace('.csv', '.parquet')}` |\n",
    "| `toUpper()` / `toLower()` | Zmiana wielkosci | `${dataset:toUpper()}` -> `MOVIELENS` |\n",
    "| `format(pattern)` | Formatowanie daty | `${now():format('yyyyMMdd')}` -> `20260209` |\n",
    "| `gt(val)` / `lt(val)` | Porownania | `${fileSize:gt(1000000)}` -> `true` |\n",
    "| `isEmpty()` | Czy pusty | `${attr:isEmpty()}` -> `true/false` |\n",
    "| `ifElse(t, f)` | Warunkowe | `${x:gt(5):ifElse('big','small')}` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyklad: UpdateAttribute z NiFi Expression Language\n",
    "# Tworzymy procesor ktory dynamicznie generuje sciezki\n",
    "\n",
    "el_processor = create_processor(\n",
    "    PG_ID,\n",
    "    \"org.apache.nifi.processors.attributes.UpdateAttribute\",\n",
    "    \"Dynamic Path Generator\",\n",
    "    {\"x\": 400, \"y\": 100},\n",
    "    config={\n",
    "        # Dynamiczna sciezka na HDFS z data partycjonowania\n",
    "        \"hdfs.output.path\": \"/data/movielens/bronze/${now():format('yyyy/MM/dd')}\",\n",
    "        \n",
    "        # Nazwa pliku z timestampem\n",
    "        \"output.filename\": \"${filename:substringBefore('.')}_${now():format('yyyyMMdd_HHmmss')}.${filename:substringAfter('.')}\",\n",
    "        \n",
    "        # Rozmiar w czytelnym formacie\n",
    "        \"file.size.human\": \"${fileSize:div(1048576)} MB\",\n",
    "        \n",
    "        # Typ pliku\n",
    "        \"file.extension\": \"${filename:substringAfterLast('.')}\",\n",
    "        \n",
    "        # Warunkowy atrybut\n",
    "        \"priority\": \"${fileSize:gt(52428800):ifElse('high', 'normal')}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nPrzykladowe Expression Language wyrazenia:\")\n",
    "print(\"  hdfs.output.path  = /data/movielens/bronze/2026/02/09\")\n",
    "print(\"  output.filename   = rating_20260209_143022.csv\")\n",
    "print(\"  file.size.human   = 245 MB\")\n",
    "print(\"  file.extension    = csv\")\n",
    "print(\"  priority          = high (jesli > 50MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6000001",
   "metadata": {},
   "source": [
    "## 6. Flow: CSV -> HDFS z transformacja\n",
    "\n",
    "Budujemy bardziej zaawansowany flow ktory:\n",
    "1. Czyta pliki CSV z katalogu\n",
    "2. Waliduje schemat\n",
    "3. Konwertuje na format Avro/JSON\n",
    "4. Zapisuje na HDFS z partycjonowaniem po dacie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzymy nowy Process Group dla HDFS flow\n",
    "hdfs_pg = nifi_post(f\"/process-groups/{ROOT_PG_ID}/process-groups\", {\n",
    "    \"revision\": {\"version\": 0},\n",
    "    \"component\": {\n",
    "        \"name\": \"CSV to HDFS Pipeline\",\n",
    "        \"position\": {\"x\": 500, \"y\": 100}\n",
    "    }\n",
    "})\n",
    "HDFS_PG_ID = hdfs_pg[\"id\"]\n",
    "print(f\"HDFS Process Group: {HDFS_PG_ID}\")\n",
    "\n",
    "# 1. GetFile - zrodlo CSV\n",
    "hdfs_getfile = create_processor(\n",
    "    HDFS_PG_ID,\n",
    "    \"org.apache.nifi.processors.standard.GetFile\",\n",
    "    \"Ingest CSV\",\n",
    "    {\"x\": 100, \"y\": 100},\n",
    "    config={\n",
    "        \"Input Directory\": \"/data/raw/movielens\",\n",
    "        \"File Filter\": \".*\\\\.csv\",\n",
    "        \"Keep Source File\": \"true\",\n",
    "        \"Batch Size\": \"10\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. UpdateAttribute - dodaj metadane i sciezke HDFS\n",
    "hdfs_metadata = create_processor(\n",
    "    HDFS_PG_ID,\n",
    "    \"org.apache.nifi.processors.attributes.UpdateAttribute\",\n",
    "    \"Set HDFS Path\",\n",
    "    {\"x\": 100, \"y\": 300},\n",
    "    config={\n",
    "        \"hdfs.directory\": \"/data/movielens/bronze/${now():format('yyyy-MM-dd')}\",\n",
    "        \"dataset.name\": \"movielens\",\n",
    "        \"ingestion.id\": \"${UUID()}\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. PutHDFS - zapis do HDFS\n",
    "hdfs_put = create_processor(\n",
    "    HDFS_PG_ID,\n",
    "    \"org.apache.nifi.processors.hadoop.PutHDFS\",\n",
    "    \"Write to HDFS\",\n",
    "    {\"x\": 100, \"y\": 500},\n",
    "    config={\n",
    "        \"Hadoop Configuration Resources\": \"/etc/hadoop/core-site.xml,/etc/hadoop/hdfs-site.xml\",\n",
    "        \"Directory\": \"${hdfs.directory}\",\n",
    "        \"Conflict Resolution Strategy\": \"replace\"\n",
    "    },\n",
    "    auto_terminate=[\"success\", \"failure\"]\n",
    ")\n",
    "\n",
    "# Polaczenia\n",
    "create_connection(HDFS_PG_ID, hdfs_getfile[\"id\"], hdfs_metadata[\"id\"], [\"success\"], \"raw csv\")\n",
    "create_connection(HDFS_PG_ID, hdfs_metadata[\"id\"], hdfs_put[\"id\"], [\"success\"], \"to HDFS\")\n",
    "\n",
    "print(\"\\nFlow CSV -> HDFS gotowy!\")\n",
    "print(\"Schemat: GetFile -> UpdateAttribute -> PutHDFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7000001",
   "metadata": {},
   "source": [
    "## 7. Uruchamianie i zatrzymywanie flow\n",
    "\n",
    "Procesory w NiFi maja stany:\n",
    "- **STOPPED** - nie przetwarza (domyslny po utworzeniu)\n",
    "- **RUNNING** - aktywnie przetwarza FlowFile\n",
    "- **DISABLED** - wylaczony (nie mozna uruchomic)\n",
    "- **INVALID** - blad konfiguracji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_processor(processor_id):\n",
    "    \"\"\"Uruchom procesor.\"\"\"\n",
    "    proc = nifi_get(f\"/processors/{processor_id}\")\n",
    "    version = proc[\"revision\"][\"version\"]\n",
    "    return nifi_put(f\"/processors/{processor_id}/run-status\", {\n",
    "        \"revision\": {\"version\": version},\n",
    "        \"state\": \"RUNNING\"\n",
    "    })\n",
    "\n",
    "def stop_processor(processor_id):\n",
    "    \"\"\"Zatrzymaj procesor.\"\"\"\n",
    "    proc = nifi_get(f\"/processors/{processor_id}\")\n",
    "    version = proc[\"revision\"][\"version\"]\n",
    "    return nifi_put(f\"/processors/{processor_id}/run-status\", {\n",
    "        \"revision\": {\"version\": version},\n",
    "        \"state\": \"STOPPED\"\n",
    "    })\n",
    "\n",
    "def start_process_group(pg_id):\n",
    "    \"\"\"Uruchom wszystkie procesory w Process Group.\"\"\"\n",
    "    return nifi_put(f\"/flow/process-groups/{pg_id}\", {\n",
    "        \"id\": pg_id,\n",
    "        \"state\": \"RUNNING\"\n",
    "    })\n",
    "\n",
    "def stop_process_group(pg_id):\n",
    "    \"\"\"Zatrzymaj wszystkie procesory w Process Group.\"\"\"\n",
    "    return nifi_put(f\"/flow/process-groups/{pg_id}\", {\n",
    "        \"id\": pg_id,\n",
    "        \"state\": \"STOPPED\"\n",
    "    })\n",
    "\n",
    "# Uruchom caly flow MovieLens Ingestion\n",
    "print(\"Uruchamiam Process Group...\")\n",
    "start_process_group(PG_ID)\n",
    "print(\"Flow uruchomiony!\")\n",
    "\n",
    "# Poczekaj chwile i sprawdz status\n",
    "time.sleep(5)\n",
    "\n",
    "pg_status = nifi_get(f\"/flow/process-groups/{PG_ID}/status\")\n",
    "stats = pg_status[\"processGroupStatus\"][\"aggregateSnapshot\"]\n",
    "print(f\"\\nStatystyki flow:\")\n",
    "print(f\"  FlowFiles In:     {stats.get('flowFilesIn', 0)}\")\n",
    "print(f\"  FlowFiles Out:    {stats.get('flowFilesOut', 0)}\")\n",
    "print(f\"  Bytes Read:       {stats.get('bytesRead', 0)}\")\n",
    "print(f\"  Bytes Written:    {stats.get('bytesWritten', 0)}\")\n",
    "print(f\"  Queued:           {stats.get('flowFilesQueued', 0)} files\")\n",
    "\n",
    "# Zatrzymaj flow\n",
    "stop_process_group(PG_ID)\n",
    "print(\"\\nFlow zatrzymany.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8000001",
   "metadata": {},
   "source": [
    "## 8. Data Provenance - sledzenie danych\n",
    "\n",
    "NiFi rejestruje pelna historie kazdego FlowFile:\n",
    "- **CREATE** - FlowFile stworzony (np. przez GetFile)\n",
    "- **RECEIVE** - FlowFile otrzymany z zewnatrz\n",
    "- **SEND** - FlowFile wyslany na zewnatrz\n",
    "- **ATTRIBUTES_MODIFIED** - zmiana atrybutow\n",
    "- **CONTENT_MODIFIED** - zmiana zawartosci\n",
    "- **ROUTE** - FlowFile przekierowany\n",
    "- **DROP** - FlowFile usuniety\n",
    "\n",
    "To jest kluczowa przewaga NiFi nad innymi narzedziami ETL - pelny **data lineage**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provenance Query - historia FlowFile\n",
    "def query_provenance(processor_id=None, max_results=100):\n",
    "    \"\"\"Zapytaj o provenance events.\"\"\"\n",
    "    query = {\n",
    "        \"provenance\": {\n",
    "            \"request\": {\n",
    "                \"maxResults\": max_results,\n",
    "                \"summarize\": False,\n",
    "                \"searchTerms\": {}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if processor_id:\n",
    "        query[\"provenance\"][\"request\"][\"searchTerms\"][\"ProcessorID\"] = {\n",
    "            \"value\": processor_id\n",
    "        }\n",
    "    \n",
    "    # Submit query\n",
    "    result = nifi_post(\"/provenance\", query)\n",
    "    query_id = result[\"provenance\"][\"id\"]\n",
    "    \n",
    "    # Poll for results\n",
    "    for _ in range(10):\n",
    "        time.sleep(1)\n",
    "        result = nifi_get(f\"/provenance/{query_id}\")\n",
    "        if result[\"provenance\"][\"finished\"]:\n",
    "            break\n",
    "    \n",
    "    events = result[\"provenance\"][\"results\"][\"provenanceEvents\"]\n",
    "    \n",
    "    # Cleanup\n",
    "    nifi_delete(f\"/provenance/{query_id}\")\n",
    "    \n",
    "    return events\n",
    "\n",
    "# Pobierz provenance events\n",
    "events = query_provenance(max_results=20)\n",
    "print(f\"Provenance events ({len(events)}):\")\n",
    "print(f\"{'Czas':<25} {'Typ':<25} {'Procesor':<25} {'Plik'}\")\n",
    "print(\"-\" * 100)\n",
    "for e in events[:20]:\n",
    "    ts = e.get(\"eventTime\", \"?\")\n",
    "    etype = e.get(\"eventType\", \"?\")\n",
    "    comp = e.get(\"componentName\", \"?\")[:24]\n",
    "    fname = e.get(\"attributes\", {}).get(\"filename\", \"?\")\n",
    "    print(f\"{ts:<25} {etype:<25} {comp:<25} {fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9000001",
   "metadata": {},
   "source": [
    "## 9. Monitoring: bulletins, back pressure, statystyki\n",
    "\n",
    "### Back Pressure\n",
    "Kazda Connection (kolejka) ma dwa limity:\n",
    "- **Object Threshold**: max liczba FlowFile w kolejce (domyslnie 10,000)\n",
    "- **Data Size Threshold**: max rozmiar danych w kolejce (domyslnie 1 GB)\n",
    "\n",
    "Gdy limit zostanie osiagniety, procesor zrodlowy jest **wstrzymywany** automatycznie.\n",
    "To zapobiega OOM i zapewnia stabilnosc pipeline.\n",
    "\n",
    "### Bulletins\n",
    "Komunikaty o bledach i ostrzezeniach - widoczne w UI i przez API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Monitoring NiFi via API ===\n",
    "\n",
    "# 1. System Diagnostics\n",
    "diag = nifi_get(\"/system-diagnostics\")\n",
    "snap = diag[\"systemDiagnostics\"][\"aggregateSnapshot\"]\n",
    "print(\"=== System Diagnostics ===\")\n",
    "print(f\"  Heap Used:        {snap['usedHeap']}\")\n",
    "print(f\"  Heap Available:   {snap['freeHeap']}\")\n",
    "print(f\"  Heap Utilization: {snap['heapUtilization']}\")\n",
    "print(f\"  Processors:       {snap['totalThreads']} threads\")\n",
    "print(f\"  Content Repo:     {snap['contentRepositoryStorageUsage'][0]['utilization']}\")\n",
    "print(f\"  FlowFile Repo:    {snap['flowFileRepositoryStorageUsage']['utilization']}\")\n",
    "\n",
    "# 2. Process Group status (metryki calego flow)\n",
    "print(\"\\n=== Root Process Group Status ===\")\n",
    "root_status = nifi_get(f\"/flow/process-groups/root/status\")\n",
    "rs = root_status[\"processGroupStatus\"][\"aggregateSnapshot\"]\n",
    "print(f\"  Active Threads:    {rs.get('activeThreadCount', 0)}\")\n",
    "print(f\"  FlowFiles Queued:  {rs.get('flowFilesQueued', 0)}\")\n",
    "print(f\"  Bytes Queued:      {rs.get('bytesQueued', 0)}\")\n",
    "print(f\"  Running:           {rs.get('runningCount', 0)} processors\")\n",
    "print(f\"  Stopped:           {rs.get('stoppedCount', 0)} processors\")\n",
    "print(f\"  Invalid:           {rs.get('invalidCount', 0)} processors\")\n",
    "\n",
    "# 3. Bulletins (errors/warnings)\n",
    "print(\"\\n=== Bulletins (ostatnie komunikaty) ===\")\n",
    "bulletins = nifi_get(\"/flow/bulletin-board\")\n",
    "bulletin_list = bulletins.get(\"bulletinBoard\", {}).get(\"bulletins\", [])\n",
    "if bulletin_list:\n",
    "    for b in bulletin_list[:10]:\n",
    "        bb = b.get(\"bulletin\", {})\n",
    "        print(f\"  [{bb.get('level', '?')}] {bb.get('sourceName', '?')}: {bb.get('message', '?')[:80]}\")\n",
    "else:\n",
    "    print(\"  Brak bulletinow - wszystko dziala poprawnie.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Szczegolowe metryki per procesor\n",
    "\n",
    "def get_processor_stats(pg_id):\n",
    "    \"\"\"Pobierz statystyki wszystkich procesorow w Process Group.\"\"\"\n",
    "    flow = nifi_get(f\"/flow/process-groups/{pg_id}\")\n",
    "    processors = flow[\"processGroupFlow\"][\"flow\"][\"processors\"]\n",
    "    \n",
    "    print(f\"{'Procesor':<30} {'Status':<10} {'In':<8} {'Out':<8} {'Read':<12} {'Written':<12} {'Tasks'}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for p in processors:\n",
    "        comp = p[\"component\"]\n",
    "        status = p.get(\"status\", {}).get(\"aggregateSnapshot\", {})\n",
    "        print(\n",
    "            f\"{comp['name']:<30} \"\n",
    "            f\"{comp['state']:<10} \"\n",
    "            f\"{status.get('flowFilesIn', 0):<8} \"\n",
    "            f\"{status.get('flowFilesOut', 0):<8} \"\n",
    "            f\"{status.get('bytesRead', 0):<12} \"\n",
    "            f\"{status.get('bytesWritten', 0):<12} \"\n",
    "            f\"{status.get('taskCount', 0)}\"\n",
    "        )\n",
    "\n",
    "# 5. Connection queue stats (back pressure monitoring)\n",
    "def get_connection_stats(pg_id):\n",
    "    \"\"\"Pobierz statystyki kolejek w Process Group.\"\"\"\n",
    "    flow = nifi_get(f\"/flow/process-groups/{pg_id}\")\n",
    "    connections = flow[\"processGroupFlow\"][\"flow\"][\"connections\"]\n",
    "    \n",
    "    print(f\"\\n{'Connection':<25} {'Queued Files':<15} {'Queued Size':<15} {'Back Pressure'}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for c in connections:\n",
    "        name = c[\"component\"].get(\"name\", \"unnamed\")[:24]\n",
    "        status = c.get(\"status\", {}).get(\"aggregateSnapshot\", {})\n",
    "        queued_count = status.get(\"flowFilesQueued\", 0)\n",
    "        queued_size = status.get(\"bytesQueued\", 0)\n",
    "        bp_pct = status.get(\"percentUseCount\", \"0%\")\n",
    "        print(f\"{name:<25} {queued_count:<15} {queued_size:<15} {bp_pct}\")\n",
    "\n",
    "print(\"=== Statystyki MovieLens Ingestion ===\")\n",
    "get_processor_stats(PG_ID)\n",
    "get_connection_stats(PG_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10000001",
   "metadata": {},
   "source": [
    "## 10. Czyszczenie zasobow\n",
    "\n",
    "Przed usunieciem Process Group nalezy:\n",
    "1. Zatrzymac wszystkie procesory\n",
    "2. Oproznic kolejki (connections)\n",
    "3. Usunac polaczenia\n",
    "4. Usunac procesory\n",
    "5. Usunac Process Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_process_group(pg_id):\n",
    "    \"\"\"Usun Process Group i wszystkie jej elementy.\"\"\"\n",
    "    # 1. Stop all processors\n",
    "    stop_process_group(pg_id)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 2. Get all connections and processors\n",
    "    flow = nifi_get(f\"/flow/process-groups/{pg_id}\")\n",
    "    connections = flow[\"processGroupFlow\"][\"flow\"][\"connections\"]\n",
    "    processors = flow[\"processGroupFlow\"][\"flow\"][\"processors\"]\n",
    "    \n",
    "    # 3. Drop queues and delete connections\n",
    "    for c in connections:\n",
    "        conn_id = c[\"id\"]\n",
    "        # Drop queue\n",
    "        try:\n",
    "            nifi_post(f\"/flowfile-queues/{conn_id}/drop-requests\", {})\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            pass\n",
    "        # Delete connection\n",
    "        version = c[\"revision\"][\"version\"]\n",
    "        nifi_delete(f\"/connections/{conn_id}\", params={\"version\": version})\n",
    "    \n",
    "    # 4. Delete processors\n",
    "    for p in processors:\n",
    "        proc_id = p[\"id\"]\n",
    "        version = p[\"revision\"][\"version\"]\n",
    "        nifi_delete(f\"/processors/{proc_id}\", params={\"version\": version})\n",
    "    \n",
    "    # 5. Delete process group\n",
    "    pg = nifi_get(f\"/process-groups/{pg_id}\")\n",
    "    version = pg[\"revision\"][\"version\"]\n",
    "    nifi_delete(f\"/process-groups/{pg_id}\", params={\"version\": version})\n",
    "    \n",
    "    print(f\"Process Group {pg_id} usuniety.\")\n",
    "\n",
    "# Odkomentuj aby posprzatac:\n",
    "# cleanup_process_group(PG_ID)\n",
    "# cleanup_process_group(HDFS_PG_ID)\n",
    "print(\"Odkomentuj powyzsze linie aby usunac utworzone flow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11000001",
   "metadata": {},
   "source": [
    "## Zadanie koncowe\n",
    "\n",
    "Stworz kompletny flow do ingestion danych MovieLens:\n",
    "\n",
    "1. **GetFile** - czytaj pliki CSV z `/data/raw/movielens/`\n",
    "2. **UpdateAttribute** - dodaj metadane: dataset, timestamp, schema name\n",
    "3. **RouteOnAttribute** - rozdziel pliki: `rating.csv` vs `movie.csv` vs inne\n",
    "4. **PutHDFS** (x2) - osobne sciezki HDFS dla ratings i movies:\n",
    "   - `/data/movielens/bronze/ratings/YYYY-MM-DD/`\n",
    "   - `/data/movielens/bronze/movies/YYYY-MM-DD/`\n",
    "5. Uruchom flow i zweryfikuj ze dane sa na HDFS\n",
    "6. Sprawdz provenance - ile eventow? Jakie typy?\n",
    "7. Sprawdz statystyki - ile bajtow przetworzone?\n",
    "\n",
    "**Bonus:** Dodaj procesor LogAttribute miedzy krokami zeby logowac atrybuty FlowFile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twoje rozwiazanie:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
